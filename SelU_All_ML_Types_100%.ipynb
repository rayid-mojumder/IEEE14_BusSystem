{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_loading_visualizing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the required library: matplotlib, tensorflow, numpy, scipy, sklearn, pickle\n",
    "from utils import *\n",
    "\n",
    "# Machine precision (to avoid division by zero)\n",
    "EPS = np.finfo(float).eps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.00000000e+01+0.j -1.00000000e+03+0.j -5.00000000e+01+0.j ...\n",
      "  -1.01802508e+00+0.j -1.00000000e+00+0.j -5.13578797e+00+0.j]\n",
      " [-5.00000000e+01+0.j -1.00000000e+03+0.j -5.00000000e+01+0.j ...\n",
      "  -1.01802508e+00+0.j -1.00000000e+00+0.j -5.13578797e+00+0.j]\n",
      " [-5.00000000e+01+0.j -1.00000000e+03+0.j -5.00000000e+01+0.j ...\n",
      "  -1.01802508e+00+0.j -1.00000000e+00+0.j -5.13578797e+00+0.j]\n",
      " ...\n",
      " [-5.00000000e+01+0.j -1.00000000e+03+0.j -3.02410064e+01+0.j ...\n",
      "  -5.13578797e+00+0.j  0.00000000e+00+0.j -1.94250194e-01+0.j]\n",
      " [-5.00000000e+01+0.j -1.00000000e+03+0.j -5.00000000e+01+0.j ...\n",
      "  -1.01802508e+00+0.j -1.00000000e+00+0.j -5.13578797e+00+0.j]\n",
      " [-5.00000000e+01+0.j -1.00000000e+03+0.j -5.00000000e+01+0.j ...\n",
      "  -1.01802508e+00+0.j -1.00000000e+00+0.j -5.13578797e+00+0.j]]\n",
      "We have 19815 number of scenarios, each with 49 eigenvalues\n"
     ]
    }
   ],
   "source": [
    "#load raw data from the _raw_data directory\n",
    "eigs19branches = np.load(\"_raw_data/19Branches_all_eigs.npy\")\n",
    "print(eigs19branches)\n",
    "\n",
    "print(f\"We have {eigs19branches.shape[0]} number of scenarios, \\\n",
    "each with {eigs19branches.shape[1]} eigenvalues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataset to panda frame\n",
    "#to select dataset size\n",
    "#df = pd.DataFrame(eigs19branches)\n",
    "#print(df)\n",
    "#75% Dataset\n",
    "#eigs19branches=df.head(14862)\n",
    "#eigs19branches=eigs19branches.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Organize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column labels, using Scenario number\n",
    "col_labels = []\n",
    "for i in range(1, eigs19branches.shape[0] + 1):\n",
    "    col_labels.append(\"Scenario {}\".format(i))\n",
    "\n",
    "#col_labels = [\"Scenario {}\".format(i) for i in range(1, eigs19branches.shape[0] + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario 1</th>\n",
       "      <th>Scenario 2</th>\n",
       "      <th>Scenario 3</th>\n",
       "      <th>Scenario 4</th>\n",
       "      <th>Scenario 5</th>\n",
       "      <th>Scenario 6</th>\n",
       "      <th>Scenario 7</th>\n",
       "      <th>Scenario 8</th>\n",
       "      <th>Scenario 9</th>\n",
       "      <th>Scenario 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Scenario 19806</th>\n",
       "      <th>Scenario 19807</th>\n",
       "      <th>Scenario 19808</th>\n",
       "      <th>Scenario 19809</th>\n",
       "      <th>Scenario 19810</th>\n",
       "      <th>Scenario 19811</th>\n",
       "      <th>Scenario 19812</th>\n",
       "      <th>Scenario 19813</th>\n",
       "      <th>Scenario 19814</th>\n",
       "      <th>Scenario 19815</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 19815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Scenario 1, Scenario 2, Scenario 3, Scenario 4, Scenario 5, Scenario 6, Scenario 7, Scenario 8, Scenario 9, Scenario 10, Scenario 11, Scenario 12, Scenario 13, Scenario 14, Scenario 15, Scenario 16, Scenario 17, Scenario 18, Scenario 19, Scenario 20, Scenario 21, Scenario 22, Scenario 23, Scenario 24, Scenario 25, Scenario 26, Scenario 27, Scenario 28, Scenario 29, Scenario 30, Scenario 31, Scenario 32, Scenario 33, Scenario 34, Scenario 35, Scenario 36, Scenario 37, Scenario 38, Scenario 39, Scenario 40, Scenario 41, Scenario 42, Scenario 43, Scenario 44, Scenario 45, Scenario 46, Scenario 47, Scenario 48, Scenario 49, Scenario 50, Scenario 51, Scenario 52, Scenario 53, Scenario 54, Scenario 55, Scenario 56, Scenario 57, Scenario 58, Scenario 59, Scenario 60, Scenario 61, Scenario 62, Scenario 63, Scenario 64, Scenario 65, Scenario 66, Scenario 67, Scenario 68, Scenario 69, Scenario 70, Scenario 71, Scenario 72, Scenario 73, Scenario 74, Scenario 75, Scenario 76, Scenario 77, Scenario 78, Scenario 79, Scenario 80, Scenario 81, Scenario 82, Scenario 83, Scenario 84, Scenario 85, Scenario 86, Scenario 87, Scenario 88, Scenario 89, Scenario 90, Scenario 91, Scenario 92, Scenario 93, Scenario 94, Scenario 95, Scenario 96, Scenario 97, Scenario 98, Scenario 99, Scenario 100, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 19815 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Pandas Eigenvalue Dataframe (df)\n",
    "df_eigenvalues = pd.DataFrame([], columns = col_labels)\n",
    "#Show dataframe head\n",
    "df_eigenvalues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Scenario 1                  Scenario 2  \\\n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "7  -4.996949e+01+0.000000e+00j -4.541522e+01+8.844841e+00j   \n",
      "8  -4.518070e+01+8.875545e+00j -4.541522e+01-8.844841e+00j   \n",
      "9  -4.518070e+01-8.875545e+00j -4.997318e+01+0.000000e+00j   \n",
      "10 -3.360567e+01+0.000000e+00j -4.993132e+01+0.000000e+00j   \n",
      "11 -3.200945e+01+0.000000e+00j -3.380793e+01+0.000000e+00j   \n",
      "12 -2.818329e+01+0.000000e+00j -3.270792e+01+0.000000e+00j   \n",
      "13 -4.991036e+01+0.000000e+00j -2.950612e+01+0.000000e+00j   \n",
      "14 -2.208684e+01+0.000000e+00j -2.656016e+01+0.000000e+00j   \n",
      "15 -2.108769e+00+1.251623e+01j -2.368393e+01+0.000000e+00j   \n",
      "16 -2.108769e+00-1.251623e+01j -2.108382e+00+1.472277e+01j   \n",
      "17 -4.144137e-01+9.808973e+00j -2.108382e+00-1.472277e+01j   \n",
      "18 -4.144137e-01-9.808973e+00j -2.527727e+00+1.325679e+01j   \n",
      "19 -2.230345e+00+9.367706e+00j -2.527727e+00-1.325679e+01j   \n",
      "20 -2.230345e+00-9.367706e+00j -2.650810e+00+1.201203e+01j   \n",
      "21  1.243422e+00+7.567015e+00j -2.650810e+00-1.201203e+01j   \n",
      "22  1.243422e+00-7.567015e+00j  1.446583e+00+7.173574e+00j   \n",
      "23 -1.858957e+01+0.000000e+00j  1.446583e+00-7.173574e+00j   \n",
      "24 -1.745205e+01+0.000000e+00j -2.626455e+00+9.116671e+00j   \n",
      "25 -1.417009e+01+0.000000e+00j -2.626455e+00-9.116671e+00j   \n",
      "26 -8.736220e+00+0.000000e+00j -1.841289e+01+0.000000e+00j   \n",
      "27 -8.176703e+00+0.000000e+00j -1.723120e+01+0.000000e+00j   \n",
      "28 -1.723253e+01+0.000000e+00j -1.484610e+01+0.000000e+00j   \n",
      "29 -4.761905e+00+0.000000e+00j -1.166816e+01+0.000000e+00j   \n",
      "30 -6.568366e-01+2.386633e+00j -8.676042e+00+0.000000e+00j   \n",
      "31 -6.568366e-01-2.386633e+00j -8.200111e+00+0.000000e+00j   \n",
      "32 -5.913697e-01+6.038079e-01j -1.153053e+00+1.371947e+00j   \n",
      "33 -5.913697e-01-6.038079e-01j -1.153053e+00-1.371947e+00j   \n",
      "34 -1.099220e+00+0.000000e+00j  1.565162e-07+0.000000e+00j   \n",
      "35 -1.007849e+00+2.937368e-03j -6.219983e-01+5.349144e-01j   \n",
      "36 -1.007849e+00-2.937368e-03j -6.219983e-01-5.349144e-01j   \n",
      "37 -5.930979e-01+0.000000e+00j -1.874133e-01+0.000000e+00j   \n",
      "38 -7.064281e-01+0.000000e+00j -5.307748e-01+0.000000e+00j   \n",
      "39 -1.896261e-01+0.000000e+00j -1.068050e+00+0.000000e+00j   \n",
      "40 -1.131440e-07+0.000000e+00j -9.431683e-01+0.000000e+00j   \n",
      "41 -2.992171e-10+0.000000e+00j -9.533766e-01+0.000000e+00j   \n",
      "42 -1.976285e-01+0.000000e+00j -1.001449e+00+0.000000e+00j   \n",
      "43 -6.666667e-01+0.000000e+00j -1.004696e+00+3.437373e-03j   \n",
      "44 -1.003828e+00+0.000000e+00j -1.004696e+00-3.437373e-03j   \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
      "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
      "\n",
      "                    Scenario 3                  Scenario 4  \\\n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "7  -4.805986e+01+8.005420e+00j -4.751593e+01+8.246593e+00j   \n",
      "8  -4.805986e+01-8.005420e+00j -4.751593e+01-8.246593e+00j   \n",
      "9  -4.997368e+01+0.000000e+00j -4.997299e+01+0.000000e+00j   \n",
      "10 -4.993512e+01+0.000000e+00j -4.992874e+01+0.000000e+00j   \n",
      "11 -3.813048e+01+0.000000e+00j -3.752501e+01+0.000000e+00j   \n",
      "12 -3.334199e+01+0.000000e+00j -3.330350e+01+0.000000e+00j   \n",
      "13 -3.043924e+01+0.000000e+00j -2.969931e+01+0.000000e+00j   \n",
      "14 -2.760969e+01+0.000000e+00j -2.683373e+01+0.000000e+00j   \n",
      "15 -2.447333e+01+0.000000e+00j -2.215740e+01+0.000000e+00j   \n",
      "16 -2.616708e+00+1.503744e+01j -2.151875e+00+1.449906e+01j   \n",
      "17 -2.616708e+00-1.503744e+01j -2.151875e+00-1.449906e+01j   \n",
      "18 -2.641145e+00+1.299707e+01j -2.593388e+00+1.269962e+01j   \n",
      "19 -2.641145e+00-1.299707e+01j -2.593388e+00-1.269962e+01j   \n",
      "20  1.855439e+00+8.586063e+00j  1.724677e+00+8.494078e+00j   \n",
      "21  1.855439e+00-8.586063e+00j  1.724677e+00-8.494078e+00j   \n",
      "22 -2.389494e+00+1.166413e+01j -7.394821e-01+9.220989e+00j   \n",
      "23 -2.389494e+00-1.166413e+01j -7.394821e-01-9.220989e+00j   \n",
      "24 -4.344945e+00+9.467973e+00j -4.057242e+00+9.497336e+00j   \n",
      "25 -4.344945e+00-9.467973e+00j -4.057242e+00-9.497336e+00j   \n",
      "26 -1.887281e+01+0.000000e+00j -1.884939e+01+0.000000e+00j   \n",
      "27 -1.731042e+01+0.000000e+00j -1.778642e+01+0.000000e+00j   \n",
      "28 -1.621494e+01+0.000000e+00j -1.584647e+01+0.000000e+00j   \n",
      "29 -1.351082e+01+0.000000e+00j -1.322920e+01+0.000000e+00j   \n",
      "30 -8.610389e+00+0.000000e+00j -8.616701e+00+0.000000e+00j   \n",
      "31 -8.343411e+00+0.000000e+00j -8.321725e+00+0.000000e+00j   \n",
      "32 -1.187271e+00+1.260550e+00j -1.070050e+00+1.444852e+00j   \n",
      "33 -1.187271e+00-1.260550e+00j -1.070050e+00-1.444852e+00j   \n",
      "34  5.459872e-07+0.000000e+00j -2.843306e-07+0.000000e+00j   \n",
      "35 -1.863090e-01+0.000000e+00j -1.861114e-01+0.000000e+00j   \n",
      "36 -6.469483e-01+6.245284e-01j -6.402568e-01+6.373359e-01j   \n",
      "37 -6.469483e-01-6.245284e-01j -6.402568e-01-6.373359e-01j   \n",
      "38 -8.373278e-01+0.000000e+00j -8.349445e-01+0.000000e+00j   \n",
      "39 -1.047039e+00+0.000000e+00j -1.070309e+00+0.000000e+00j   \n",
      "40 -1.017900e+00+0.000000e+00j -9.540169e-01+0.000000e+00j   \n",
      "41 -9.845790e-01+0.000000e+00j -1.005259e+00+1.257624e-02j   \n",
      "42 -9.938086e-01+0.000000e+00j -1.005259e+00-1.257624e-02j   \n",
      "43 -9.985471e-01+0.000000e+00j -1.017366e+00+0.000000e+00j   \n",
      "44 -1.005094e+00+0.000000e+00j -1.005067e+00+0.000000e+00j   \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
      "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
      "\n",
      "                    Scenario 5                  Scenario 6  \\\n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "2  -5.000000e+01+0.000000e+00j -3.024101e+01+0.000000e+00j   \n",
      "3  -1.000000e+03+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "4  -9.999998e+02+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "6  -1.000000e+03+0.000000e+00j -4.997225e+01+0.000000e+00j   \n",
      "7  -4.799658e+01+8.065178e+00j -3.178208e+01+0.000000e+00j   \n",
      "8  -4.799658e+01-8.065178e+00j -2.712724e+01+0.000000e+00j   \n",
      "9  -4.997448e+01+0.000000e+00j -2.235854e+00+1.397127e+01j   \n",
      "10 -4.993494e+01+0.000000e+00j -2.235854e+00-1.397127e+01j   \n",
      "11 -3.799214e+01+0.000000e+00j -1.827441e+01+0.000000e+00j   \n",
      "12 -3.324219e+01+0.000000e+00j -1.234670e+01+0.000000e+00j   \n",
      "13 -2.994588e+01+0.000000e+00j -8.689411e+00+0.000000e+00j   \n",
      "14 -2.580198e+01+0.000000e+00j -7.607472e+00+0.000000e+00j   \n",
      "15 -2.259375e+01+0.000000e+00j -5.856738e-01+8.419188e-01j   \n",
      "16 -2.879272e+00+1.323777e+01j -5.856738e-01-8.419188e-01j   \n",
      "17 -2.879272e+00-1.323777e+01j -1.004172e+00+0.000000e+00j   \n",
      "18 -2.632725e+00+1.276021e+01j -1.197868e-06+0.000000e+00j   \n",
      "19 -2.632725e+00-1.276021e+01j -3.078263e-01+0.000000e+00j   \n",
      "20  1.736030e+00+8.562365e+00j -1.529042e-01+0.000000e+00j   \n",
      "21  1.736030e+00-8.562365e+00j -9.999998e+02+0.000000e+00j   \n",
      "22 -1.475727e+00+1.127523e+01j -4.363366e+01+8.743671e+00j   \n",
      "23 -1.475727e+00-1.127523e+01j -4.363366e+01-8.743671e+00j   \n",
      "24 -4.217833e+00+9.408689e+00j -5.552430e-01+1.061762e+01j   \n",
      "25 -4.217833e+00-9.408689e+00j -5.552430e-01-1.061762e+01j   \n",
      "26 -1.829118e+01+0.000000e+00j -2.621537e+01+0.000000e+00j   \n",
      "27 -1.604073e+01+0.000000e+00j -2.202259e+01+0.000000e+00j   \n",
      "28 -1.428456e+01+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "29 -1.264601e+01+0.000000e+00j -4.993822e+01+0.000000e+00j   \n",
      "30 -8.525122e+00+0.000000e+00j -2.640101e+01+0.000000e+00j   \n",
      "31 -8.318721e+00+0.000000e+00j -1.941362e+01+0.000000e+00j   \n",
      "32 -1.187472e+00+1.275286e+00j -1.013037e+00+0.000000e+00j   \n",
      "33 -1.187472e+00-1.275286e+00j -1.221090e+00+1.173747e+00j   \n",
      "34  3.241041e-07+0.000000e+00j -1.221090e+00-1.173747e+00j   \n",
      "35 -1.863265e-01+0.000000e+00j -9.852675e-01+0.000000e+00j   \n",
      "36 -6.462651e-01+6.202891e-01j -1.976285e-01+0.000000e+00j   \n",
      "37 -6.462651e-01-6.202891e-01j -1.976284e-01+0.000000e+00j   \n",
      "38 -7.886194e-01+0.000000e+00j  1.037752e-11+0.000000e+00j   \n",
      "39 -8.380542e-01+0.000000e+00j -5.563854e-08+0.000000e+00j   \n",
      "40 -1.047545e+00+0.000000e+00j -1.103030e+00+0.000000e+00j   \n",
      "41 -9.693773e-01+0.000000e+00j -1.001605e+00+0.000000e+00j   \n",
      "42 -1.021163e+00+0.000000e+00j -1.102653e+00+0.000000e+00j   \n",
      "43 -1.001953e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "44 -1.005202e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "46 -1.018025e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
      "47 -1.000000e+00+0.000000e+00j  0.000000e+00+0.000000e+00j   \n",
      "48 -5.135788e+00+0.000000e+00j -1.942502e-01+0.000000e+00j   \n",
      "\n",
      "                    Scenario 7                  Scenario 8  \\\n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "7  -4.752763e+01+8.254978e+00j -4.801786e+01+8.072489e+00j   \n",
      "8  -4.752763e+01-8.254978e+00j -4.801786e+01-8.072489e+00j   \n",
      "9  -4.997396e+01+0.000000e+00j -4.997339e+01+0.000000e+00j   \n",
      "10 -4.993566e+01+0.000000e+00j -3.802116e+01+0.000000e+00j   \n",
      "11 -3.751461e+01+0.000000e+00j -3.327804e+01+0.000000e+00j   \n",
      "12 -3.335564e+01+0.000000e+00j -2.978243e+01+0.000000e+00j   \n",
      "13 -2.995962e+01+0.000000e+00j -4.991036e+01+0.000000e+00j   \n",
      "14 -2.637017e+01+0.000000e+00j -2.540316e+01+0.000000e+00j   \n",
      "15 -2.348029e+01+0.000000e+00j -2.009746e+00+1.318564e+01j   \n",
      "16 -2.492177e+00+1.323114e+01j -2.009746e+00-1.318564e+01j   \n",
      "17 -2.492177e+00-1.323114e+01j -2.229466e+00+1.238629e+01j   \n",
      "18 -2.252503e+00+1.294271e+01j -2.229466e+00-1.238629e+01j   \n",
      "19 -2.252503e+00-1.294271e+01j  1.607665e+00+8.816554e+00j   \n",
      "20  1.807165e+00+8.528168e+00j  1.607665e+00-8.816554e+00j   \n",
      "21  1.807165e+00-8.528168e+00j -4.107595e+00+9.665444e+00j   \n",
      "22 -1.661407e+00+1.149697e+01j -4.107595e+00-9.665444e+00j   \n",
      "23 -1.661407e+00-1.149697e+01j -1.871028e+01+0.000000e+00j   \n",
      "24 -4.079542e+00+9.425006e+00j -1.593057e+01+0.000000e+00j   \n",
      "25 -4.079542e+00-9.425006e+00j -1.676595e+01+0.000000e+00j   \n",
      "26 -1.900681e+01+0.000000e+00j -8.581714e+00+0.000000e+00j   \n",
      "27 -1.706448e+01+0.000000e+00j -8.370385e+00+0.000000e+00j   \n",
      "28 -1.642414e+01+0.000000e+00j -1.723253e+01+0.000000e+00j   \n",
      "29 -1.356588e+01+0.000000e+00j -4.761905e+00+0.000000e+00j   \n",
      "30 -8.623900e+00+0.000000e+00j -6.568366e-01+2.386633e+00j   \n",
      "31 -8.374520e+00+0.000000e+00j -6.568366e-01-2.386633e+00j   \n",
      "32 -1.193483e+00+1.237424e+00j -6.410903e-01+6.320596e-01j   \n",
      "33 -1.193483e+00-1.237424e+00j -6.410903e-01-6.320596e-01j   \n",
      "34 -8.309433e-07+0.000000e+00j -1.860008e-01+0.000000e+00j   \n",
      "35 -1.861870e-01+0.000000e+00j  2.017632e-07+0.000000e+00j   \n",
      "36 -6.497290e-01+6.147039e-01j -2.694031e-09+0.000000e+00j   \n",
      "37 -6.497290e-01-6.147039e-01j -1.976285e-01+0.000000e+00j   \n",
      "38 -8.266339e-01+1.559102e-02j -8.348231e-01+0.000000e+00j   \n",
      "39 -8.266339e-01-1.559102e-02j -9.347787e-01+0.000000e+00j   \n",
      "40 -1.082653e+00+0.000000e+00j -1.051737e+00+0.000000e+00j   \n",
      "41 -1.018074e+00+0.000000e+00j -1.019676e+00+0.000000e+00j   \n",
      "42 -9.889775e-01+0.000000e+00j -1.005087e+00+0.000000e+00j   \n",
      "43 -9.994966e-01+0.000000e+00j -6.666667e-01+0.000000e+00j   \n",
      "44 -1.005117e+00+0.000000e+00j -1.003828e+00+0.000000e+00j   \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
      "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
      "\n",
      "                    Scenario 9                 Scenario 10  ...  \\\n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j  ...   \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  ...   \n",
      "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j  ...   \n",
      "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  ...   \n",
      "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j  ...   \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  ...   \n",
      "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  ...   \n",
      "7  -4.533189e+01+8.850499e+00j -4.539563e+01+8.832355e+00j  ...   \n",
      "8  -4.533189e+01-8.850499e+00j -4.539563e+01-8.832355e+00j  ...   \n",
      "9  -4.997024e+01+0.000000e+00j -4.996982e+01+0.000000e+00j  ...   \n",
      "10 -4.993630e+01+0.000000e+00j -4.992928e+01+0.000000e+00j  ...   \n",
      "11 -3.366293e+01+0.000000e+00j -3.384288e+01+0.000000e+00j  ...   \n",
      "12 -3.057122e+01+0.000000e+00j -3.125660e+01+0.000000e+00j  ...   \n",
      "13 -2.924791e+01+0.000000e+00j -2.932420e+01+0.000000e+00j  ...   \n",
      "14 -2.757200e+01+0.000000e+00j -2.558983e+01+0.000000e+00j  ...   \n",
      "15 -2.401152e+01+0.000000e+00j -2.268199e+01+0.000000e+00j  ...   \n",
      "16 -2.465046e+00+1.509705e+01j -2.218815e+00+1.476103e+01j  ...   \n",
      "17 -2.465046e+00-1.509705e+01j -2.218815e+00-1.476103e+01j  ...   \n",
      "18 -2.156797e+00+1.261819e+01j -1.436001e+00+1.155366e+01j  ...   \n",
      "19 -2.156797e+00-1.261819e+01j -1.436001e+00-1.155366e+01j  ...   \n",
      "20 -1.234252e+00+1.124197e+01j  1.570066e+00+7.419796e+00j  ...   \n",
      "21 -1.234252e+00-1.124197e+01j  1.570066e+00-7.419796e+00j  ...   \n",
      "22 -2.489253e+00+9.213810e+00j -1.052524e+00+1.013176e+01j  ...   \n",
      "23 -2.489253e+00-9.213810e+00j -1.052524e+00-1.013176e+01j  ...   \n",
      "24  1.420194e+00+7.007686e+00j -2.662958e+00+9.341817e+00j  ...   \n",
      "25  1.420194e+00-7.007686e+00j -2.662958e+00-9.341817e+00j  ...   \n",
      "26 -1.804594e+01+0.000000e+00j -1.767913e+01+0.000000e+00j  ...   \n",
      "27 -1.710448e+01+0.000000e+00j -1.653969e+01+0.000000e+00j  ...   \n",
      "28 -1.512107e+01+0.000000e+00j -1.502977e+01+0.000000e+00j  ...   \n",
      "29 -1.338120e+01+0.000000e+00j -1.330008e+01+0.000000e+00j  ...   \n",
      "30 -8.595434e+00+0.000000e+00j -8.656181e+00+0.000000e+00j  ...   \n",
      "31 -8.402456e+00+0.000000e+00j -8.458188e+00+0.000000e+00j  ...   \n",
      "32 -1.182951e+00+1.177165e+00j -1.045364e+00+1.493667e+00j  ...   \n",
      "33 -1.182951e+00-1.177165e+00j -1.045364e+00-1.493667e+00j  ...   \n",
      "34  6.167129e-08+0.000000e+00j -3.328709e-08+0.000000e+00j  ...   \n",
      "35 -5.784021e-01+5.295259e-01j -5.969817e-01+5.521807e-01j  ...   \n",
      "36 -5.784021e-01-5.295259e-01j -5.969817e-01-5.521807e-01j  ...   \n",
      "37 -1.889523e-01+0.000000e+00j -1.890055e-01+0.000000e+00j  ...   \n",
      "38 -4.460000e-01+0.000000e+00j -5.339992e-01+0.000000e+00j  ...   \n",
      "39 -1.080996e+00+0.000000e+00j -9.283582e-01+0.000000e+00j  ...   \n",
      "40 -9.670052e-01+0.000000e+00j -1.055312e+00+0.000000e+00j  ...   \n",
      "41 -9.949231e-01+1.029646e-02j -1.004626e+00+1.252746e-02j  ...   \n",
      "42 -9.949231e-01-1.029646e-02j -1.004626e+00-1.252746e-02j  ...   \n",
      "43 -1.002957e+00+3.828176e-03j -1.005960e+00+4.346537e-03j  ...   \n",
      "44 -1.002957e+00-3.828176e-03j -1.005960e+00-4.346537e-03j  ...   \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j  ...   \n",
      "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j  ...   \n",
      "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j  ...   \n",
      "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j  ...   \n",
      "\n",
      "                Scenario 19806              Scenario 19807  \\\n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "7  -4.542038e+01+8.825096e+00j -4.822295e+01+7.980638e+00j   \n",
      "8  -4.542038e+01-8.825096e+00j -4.822295e+01-7.980638e+00j   \n",
      "9  -4.997313e+01+0.000000e+00j -4.997451e+01+0.000000e+00j   \n",
      "10 -4.993613e+01+0.000000e+00j -4.993149e+01+0.000000e+00j   \n",
      "11 -3.387425e+01+0.000000e+00j -3.821279e+01+0.000000e+00j   \n",
      "12 -3.292159e+01+0.000000e+00j -3.326046e+01+0.000000e+00j   \n",
      "13 -2.975168e+01+0.000000e+00j -3.046664e+01+0.000000e+00j   \n",
      "14 -2.758380e+01+0.000000e+00j -2.639723e+01+0.000000e+00j   \n",
      "15 -2.418545e+01+0.000000e+00j -2.383425e+01+0.000000e+00j   \n",
      "16 -2.756337e+00+1.566992e+01j -2.278552e+00+1.435003e+01j   \n",
      "17 -2.756337e+00-1.566992e+01j -2.278552e+00-1.435003e+01j   \n",
      "18 -2.528354e+00+1.368498e+01j -2.718879e+00+1.293008e+01j   \n",
      "19 -2.528354e+00-1.368498e+01j -2.718879e+00-1.293008e+01j   \n",
      "20 -2.536526e+00+1.235479e+01j  1.751303e+00+8.581940e+00j   \n",
      "21 -2.536526e+00-1.235479e+01j  1.751303e+00-8.581940e+00j   \n",
      "22  1.612711e+00+7.371855e+00j -2.575343e+00+1.166916e+01j   \n",
      "23  1.612711e+00-7.371855e+00j -2.575343e+00-1.166916e+01j   \n",
      "24 -2.762606e+00+9.174267e+00j -4.323739e+00+9.414391e+00j   \n",
      "25 -2.762606e+00-9.174267e+00j -4.323739e+00-9.414391e+00j   \n",
      "26 -1.910348e+01+0.000000e+00j -1.846759e+01+0.000000e+00j   \n",
      "27 -1.726415e+01+0.000000e+00j -1.666443e+01+0.000000e+00j   \n",
      "28 -1.531079e+01+0.000000e+00j -1.580108e+01+0.000000e+00j   \n",
      "29 -1.314455e+01+0.000000e+00j -1.196060e+01+0.000000e+00j   \n",
      "30 -8.747668e+00+0.000000e+00j -8.552050e+00+0.000000e+00j   \n",
      "31 -8.238276e+00+0.000000e+00j -8.340093e+00+0.000000e+00j   \n",
      "32 -1.175585e+00+1.195252e+00j -1.145635e+00+1.399106e+00j   \n",
      "33 -1.175585e+00-1.195252e+00j -1.145635e+00-1.399106e+00j   \n",
      "34  5.846560e-08+0.000000e+00j  1.150629e-07+0.000000e+00j   \n",
      "35 -6.196099e-01+5.327362e-01j -1.863949e-01+0.000000e+00j   \n",
      "36 -6.196099e-01-5.327362e-01j -6.488509e-01+6.147873e-01j   \n",
      "37 -1.875530e-01+0.000000e+00j -6.488509e-01-6.147873e-01j   \n",
      "38 -5.560640e-01+0.000000e+00j -8.390606e-01+0.000000e+00j   \n",
      "39 -1.073951e+00+0.000000e+00j -9.561312e-01+0.000000e+00j   \n",
      "40 -9.471056e-01+0.000000e+00j -9.531611e-01+0.000000e+00j   \n",
      "41 -1.005071e+00+3.872870e-03j -1.037703e+00+0.000000e+00j   \n",
      "42 -1.005071e+00-3.872870e-03j -1.019935e+00+0.000000e+00j   \n",
      "43 -9.917700e-01+8.128550e-03j -1.002047e+00+0.000000e+00j   \n",
      "44 -9.917700e-01-8.128550e-03j -1.005169e+00+0.000000e+00j   \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
      "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
      "\n",
      "                Scenario 19808              Scenario 19809  \\\n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "7  -4.736156e+01+8.330170e+00j -4.538858e+01+8.831742e+00j   \n",
      "8  -4.736156e+01-8.330170e+00j -4.538858e+01-8.831742e+00j   \n",
      "9  -4.996958e+01+0.000000e+00j -4.997312e+01+0.000000e+00j   \n",
      "10 -4.993465e+01+0.000000e+00j -4.993489e+01+0.000000e+00j   \n",
      "11 -3.722767e+01+0.000000e+00j -3.380255e+01+0.000000e+00j   \n",
      "12 -3.206271e+01+0.000000e+00j -3.267566e+01+0.000000e+00j   \n",
      "13 -2.997472e+01+0.000000e+00j -2.940520e+01+0.000000e+00j   \n",
      "14 -2.706112e+01+0.000000e+00j -2.695835e+01+0.000000e+00j   \n",
      "15 -2.424684e+01+0.000000e+00j -2.398446e+01+0.000000e+00j   \n",
      "16 -2.300784e+00+1.459796e+01j -2.438194e+00+1.515575e+01j   \n",
      "17 -2.300784e+00-1.459796e+01j -2.438194e+00-1.515575e+01j   \n",
      "18 -2.262176e+00+1.234287e+01j -2.492242e+00+1.334721e+01j   \n",
      "19 -2.262176e+00-1.234287e+01j -2.492242e+00-1.334721e+01j   \n",
      "20  1.697650e+00+8.302389e+00j -2.261376e+00+1.217793e+01j   \n",
      "21  1.697650e+00-8.302389e+00j -2.261376e+00-1.217793e+01j   \n",
      "22 -1.641208e+00+1.083610e+01j  1.554996e+00+7.250948e+00j   \n",
      "23 -1.641208e+00-1.083610e+01j  1.554996e+00-7.250948e+00j   \n",
      "24 -3.531841e+00+9.796642e+00j -2.684192e+00+9.158974e+00j   \n",
      "25 -3.531841e+00-9.796642e+00j -2.684192e+00-9.158974e+00j   \n",
      "26 -1.829214e+01+0.000000e+00j -1.848164e+01+0.000000e+00j   \n",
      "27 -1.717154e+01+0.000000e+00j -1.690433e+01+0.000000e+00j   \n",
      "28 -1.633797e+01+0.000000e+00j -1.512386e+01+0.000000e+00j   \n",
      "29 -1.417895e+01+0.000000e+00j -1.337684e+01+0.000000e+00j   \n",
      "30 -8.617552e+00+0.000000e+00j -8.688605e+00+0.000000e+00j   \n",
      "31 -8.511134e+00+0.000000e+00j -8.218134e+00+0.000000e+00j   \n",
      "32 -1.186471e+00+1.258818e+00j -1.165897e+00+1.228425e+00j   \n",
      "33 -1.186471e+00-1.258818e+00j -1.165897e+00-1.228425e+00j   \n",
      "34 -1.801596e-07+0.000000e+00j  4.189514e-07+0.000000e+00j   \n",
      "35 -1.863550e-01+0.000000e+00j -6.180762e-01+5.264913e-01j   \n",
      "36 -6.140420e-01+5.871639e-01j -6.180762e-01-5.264913e-01j   \n",
      "37 -6.140420e-01-5.871639e-01j -1.875584e-01+0.000000e+00j   \n",
      "38 -8.109252e-01+0.000000e+00j -5.183409e-01+0.000000e+00j   \n",
      "39 -9.546331e-01+0.000000e+00j -9.348125e-01+0.000000e+00j   \n",
      "40 -1.052300e+00+0.000000e+00j -1.068181e+00+0.000000e+00j   \n",
      "41 -1.018458e+00+0.000000e+00j -9.969529e-01+1.126147e-02j   \n",
      "42 -1.002710e+00+6.527041e-03j -9.969529e-01-1.126147e-02j   \n",
      "43 -1.002710e+00-6.527041e-03j -1.005138e+00+4.192293e-03j   \n",
      "44 -1.006315e+00+0.000000e+00j -1.005138e+00-4.192293e-03j   \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
      "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
      "\n",
      "                Scenario 19810              Scenario 19811  \\\n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
      "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
      "7  -4.801776e+01+8.075638e+00j -4.821297e+01+7.952461e+00j   \n",
      "8  -4.801776e+01-8.075638e+00j -4.821297e+01-7.952461e+00j   \n",
      "9  -4.997023e+01+0.000000e+00j -4.997466e+01+0.000000e+00j   \n",
      "10 -3.808626e+01+0.000000e+00j -4.993227e+01+0.000000e+00j   \n",
      "11 -3.284128e+01+0.000000e+00j -3.828509e+01+0.000000e+00j   \n",
      "12 -2.950077e+01+0.000000e+00j -3.345916e+01+0.000000e+00j   \n",
      "13 -2.597344e+01+0.000000e+00j -3.051328e+01+0.000000e+00j   \n",
      "14 -4.991036e+01+0.000000e+00j -2.638857e+01+0.000000e+00j   \n",
      "15 -2.051855e+00+1.390397e+01j -2.390588e+01+0.000000e+00j   \n",
      "16 -2.051855e+00-1.390397e+01j -2.482523e+00+1.466977e+01j   \n",
      "17 -2.323699e+00+1.196302e+01j -2.482523e+00-1.466977e+01j   \n",
      "18 -2.323699e+00-1.196302e+01j -2.918771e+00+1.323338e+01j   \n",
      "19  1.510684e+00+8.674597e+00j -2.918771e+00-1.323338e+01j   \n",
      "20  1.510684e+00-8.674597e+00j  1.780811e+00+8.643807e+00j   \n",
      "21 -3.910239e+00+9.539650e+00j  1.780811e+00-8.643807e+00j   \n",
      "22 -3.910239e+00-9.539650e+00j -2.840242e+00+1.191385e+01j   \n",
      "23 -1.845406e+01+0.000000e+00j -2.840242e+00-1.191385e+01j   \n",
      "24 -1.497426e+01+0.000000e+00j -4.448062e+00+9.463512e+00j   \n",
      "25 -1.708382e+01+0.000000e+00j -4.448062e+00-9.463512e+00j   \n",
      "26 -8.592392e+00+0.000000e+00j -1.858398e+01+0.000000e+00j   \n",
      "27 -8.196063e+00+0.000000e+00j -1.641643e+01+0.000000e+00j   \n",
      "28 -1.723253e+01+0.000000e+00j -1.550439e+01+0.000000e+00j   \n",
      "29 -4.761905e+00+0.000000e+00j -1.151969e+01+0.000000e+00j   \n",
      "30 -6.568366e-01+2.386633e+00j -8.562531e+00+0.000000e+00j   \n",
      "31 -6.568366e-01-2.386633e+00j -8.285036e+00+0.000000e+00j   \n",
      "32 -6.207826e-01+7.017344e-01j -1.145323e+00+1.375404e+00j   \n",
      "33 -6.207826e-01-7.017344e-01j -1.145323e+00-1.375404e+00j   \n",
      "34 -1.858857e-01+0.000000e+00j -4.930683e-07+0.000000e+00j   \n",
      "35 -8.330554e-01+0.000000e+00j -1.863613e-01+0.000000e+00j   \n",
      "36 -1.051661e+00+0.000000e+00j -6.464288e-01+6.166708e-01j   \n",
      "37 -9.552251e-01+0.000000e+00j -6.464288e-01-6.166708e-01j   \n",
      "38 -1.019924e+00+0.000000e+00j -8.426926e-01+0.000000e+00j   \n",
      "39 -6.666667e-01+0.000000e+00j -9.413932e-01+0.000000e+00j   \n",
      "40 -1.004816e+00+0.000000e+00j -9.503138e-01+0.000000e+00j   \n",
      "41 -1.976285e-01+0.000000e+00j -1.033439e+00+0.000000e+00j   \n",
      "42  1.410956e-07+0.000000e+00j -1.019426e+00+0.000000e+00j   \n",
      "43 -8.976516e-10+0.000000e+00j -1.001584e+00+0.000000e+00j   \n",
      "44 -1.003828e+00+0.000000e+00j -1.005205e+00+0.000000e+00j   \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
      "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
      "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
      "\n",
      "                Scenario 19812        Scenario 19813  \\\n",
      "0  -5.000000e+01+0.000000e+00j  -50.000000+0.000000j   \n",
      "1  -1.000000e+03+0.000000e+00j  -1000.00000+0.00000j   \n",
      "2  -5.000000e+01+0.000000e+00j  -30.241006+0.000000j   \n",
      "3  -1.000000e+03+0.000000e+00j  -50.000000+0.000000j   \n",
      "4  -9.999998e+02+0.000000e+00j  -1000.00000+0.00000j   \n",
      "5  -1.000000e+03+0.000000e+00j -999.999999+0.000000j   \n",
      "6  -1.000000e+03+0.000000e+00j -999.999998+0.000000j   \n",
      "7  -4.809936e+01+7.995917e+00j -999.999753+0.000000j   \n",
      "8  -4.809936e+01-7.995917e+00j  -49.972977+0.000000j   \n",
      "9  -4.997022e+01+0.000000e+00j  -49.941029+0.000000j   \n",
      "10 -4.993494e+01+0.000000e+00j  -43.633658+8.743671j   \n",
      "11 -3.814960e+01+0.000000e+00j  -43.633658-8.743671j   \n",
      "12 -3.288440e+01+0.000000e+00j  -31.030179+0.000000j   \n",
      "13 -2.956625e+01+0.000000e+00j  -28.005535+0.000000j   \n",
      "14 -2.745832e+01+0.000000e+00j  -24.956000+0.000000j   \n",
      "15 -2.417355e+01+0.000000e+00j  -23.662313+0.000000j   \n",
      "16 -2.650365e+00+1.479759e+01j  -2.519163+17.724214j   \n",
      "17 -2.650365e+00-1.479759e+01j  -2.519163-17.724214j   \n",
      "18 -2.783152e+00+1.312907e+01j  -2.581083+15.769474j   \n",
      "19 -2.783152e+00-1.312907e+01j  -2.581083-15.769474j   \n",
      "20  1.764014e+00+8.584571e+00j  -1.885774+14.519988j   \n",
      "21  1.764014e+00-8.584571e+00j  -1.885774-14.519988j   \n",
      "22 -1.997536e+00+1.084007e+01j  -18.985405+0.000000j   \n",
      "23 -1.997536e+00-1.084007e+01j  -17.564882+0.000000j   \n",
      "24 -4.300016e+00+9.523139e+00j  -15.508366+0.000000j   \n",
      "25 -4.300016e+00-9.523139e+00j  -12.406587+0.000000j   \n",
      "26 -1.820156e+01+0.000000e+00j -8.8778430+0.0000000j   \n",
      "27 -1.671510e+01+0.000000e+00j -7.8090830+0.0000000j   \n",
      "28 -1.490134e+01+0.000000e+00j  -0.555243+10.617616j   \n",
      "29 -1.316534e+01+0.000000e+00j  -0.555243-10.617616j   \n",
      "30 -8.558530e+00+0.000000e+00j -1.0225340+1.3294760j   \n",
      "31 -8.146958e+00+0.000000e+00j -1.0225340-1.3294760j   \n",
      "32 -1.185117e+00+1.279897e+00j -0.5289180+0.7937210j   \n",
      "33 -1.185117e+00-1.279897e+00j -0.5289180-0.7937210j   \n",
      "34 -5.654428e-08+0.000000e+00j  0.0000030+0.0000000j   \n",
      "35 -1.860579e-01+0.000000e+00j -0.1405750+0.0000000j   \n",
      "36 -6.195383e-01+7.050075e-01j -0.2530160+0.0000000j   \n",
      "37 -6.195383e-01-7.050075e-01j -0.5546420+0.0000000j   \n",
      "38 -8.326205e-01+0.000000e+00j -1.0912310+0.0000000j   \n",
      "39 -1.043240e+00+0.000000e+00j -1.0119590+0.0221850j   \n",
      "40 -9.741198e-01+0.000000e+00j -1.0119590-0.0221850j   \n",
      "41 -9.780176e-01+0.000000e+00j -1.0012030+0.0000000j   \n",
      "42 -1.019152e+00+0.000000e+00j -1.0130370+0.0000000j   \n",
      "43 -1.001234e+00+0.000000e+00j -1.0000000+0.0000000j   \n",
      "44 -1.004816e+00+0.000000e+00j -1.0180250+0.0000000j   \n",
      "45 -1.000000e+00+0.000000e+00j -1.0000000+0.0000000j   \n",
      "46 -1.018025e+00+0.000000e+00j -5.1357880+0.0000000j   \n",
      "47 -1.000000e+00+0.000000e+00j  0.0000000+0.0000000j   \n",
      "48 -5.135788e+00+0.000000e+00j -0.1942500+0.0000000j   \n",
      "\n",
      "                Scenario 19814              Scenario 19815  \n",
      "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j  \n",
      "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  \n",
      "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j  \n",
      "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  \n",
      "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j  \n",
      "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  \n",
      "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  \n",
      "7  -4.515031e+01+8.862549e+00j -4.757701e+01+8.232589e+00j  \n",
      "8  -4.515031e+01-8.862549e+00j -4.757701e+01-8.232589e+00j  \n",
      "9  -4.997286e+01+0.000000e+00j -4.997013e+01+0.000000e+00j  \n",
      "10 -4.993628e+01+0.000000e+00j -4.993509e+01+0.000000e+00j  \n",
      "11 -3.266495e+01+0.000000e+00j -3.756699e+01+0.000000e+00j  \n",
      "12 -3.343166e+01+0.000000e+00j -3.290894e+01+0.000000e+00j  \n",
      "13 -2.949043e+01+0.000000e+00j -2.967164e+01+0.000000e+00j  \n",
      "14 -2.650026e+01+0.000000e+00j -2.736157e+01+0.000000e+00j  \n",
      "15 -2.409646e+01+0.000000e+00j -2.430230e+01+0.000000e+00j  \n",
      "16 -2.246570e+00+1.477584e+01j -2.547493e+00+1.479194e+01j  \n",
      "17 -2.246570e+00-1.477584e+01j -2.547493e+00-1.479194e+01j  \n",
      "18 -2.266112e+00+1.333104e+01j -2.529688e+00+1.290237e+01j  \n",
      "19 -2.266112e+00-1.333104e+01j -2.529688e+00-1.290237e+01j  \n",
      "20 -2.227150e+00+1.269666e+01j  1.830223e+00+8.494314e+00j  \n",
      "21 -2.227150e+00-1.269666e+01j  1.830223e+00-8.494314e+00j  \n",
      "22 -2.233387e+00+9.414941e+00j -1.807129e+00+1.074757e+01j  \n",
      "23 -2.233387e+00-9.414941e+00j -1.807129e+00-1.074757e+01j  \n",
      "24  1.381332e+00+7.100172e+00j -4.094805e+00+9.482176e+00j  \n",
      "25  1.381332e+00-7.100172e+00j -4.094805e+00-9.482176e+00j  \n",
      "26 -1.946046e+01+0.000000e+00j -1.883194e+01+0.000000e+00j  \n",
      "27 -1.670538e+01+0.000000e+00j -1.658529e+01+0.000000e+00j  \n",
      "28 -1.570627e+01+0.000000e+00j -1.525657e+01+0.000000e+00j  \n",
      "29 -1.374166e+01+0.000000e+00j -1.389188e+01+0.000000e+00j  \n",
      "30 -8.790636e+00+0.000000e+00j -8.637091e+00+0.000000e+00j  \n",
      "31 -8.331044e+00+0.000000e+00j -8.227269e+00+0.000000e+00j  \n",
      "32 -1.135757e+00+1.191575e+00j -1.191162e+00+1.258817e+00j  \n",
      "33 -1.135757e+00-1.191575e+00j -1.191162e+00-1.258817e+00j  \n",
      "34 -1.825353e-08+0.000000e+00j -5.004686e-07+0.000000e+00j  \n",
      "35 -6.041052e-01+5.131972e-01j -1.861383e-01+0.000000e+00j  \n",
      "36 -6.041052e-01-5.131972e-01j -6.210831e-01+6.956186e-01j  \n",
      "37 -1.890564e-01+0.000000e+00j -6.210831e-01-6.956186e-01j  \n",
      "38 -5.296216e-01+0.000000e+00j -8.270570e-01+0.000000e+00j  \n",
      "39 -1.049173e+00+0.000000e+00j -9.713591e-01+0.000000e+00j  \n",
      "40 -9.703435e-01+0.000000e+00j -1.039622e+00+0.000000e+00j  \n",
      "41 -1.004684e+00+1.637869e-02j -1.016630e+00+0.000000e+00j  \n",
      "42 -1.004684e+00-1.637869e-02j -9.986679e-01+1.402810e-03j  \n",
      "43 -9.931485e-01+0.000000e+00j -9.986679e-01-1.402810e-03j  \n",
      "44 -1.007060e+00+0.000000e+00j -1.004906e+00+0.000000e+00j  \n",
      "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j  \n",
      "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j  \n",
      "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j  \n",
      "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j  \n",
      "\n",
      "[49 rows x 19815 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create proper 49 rows of eigen values using scenario 1 to 19815\n",
    "for sc in range(1, eigs19branches.shape[0] + 1):\n",
    "    df_eigenvalues[\"Scenario {}\".format(sc)] = eigs19branches[sc - 1][:]\n",
    "#print(df_eigenvalues)\n",
    "\n",
    "#Store the dataframe as a pickle file 'df_eigenvalues.pkl'\n",
    "with open(\"_preproc_data/df_eigenvalues.pkl\", 'wb') as f:\n",
    "    pickle.dump(df_eigenvalues, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario 1</th>\n",
       "      <th>Scenario 2</th>\n",
       "      <th>Scenario 3</th>\n",
       "      <th>Scenario 4</th>\n",
       "      <th>Scenario 5</th>\n",
       "      <th>Scenario 6</th>\n",
       "      <th>Scenario 7</th>\n",
       "      <th>Scenario 8</th>\n",
       "      <th>Scenario 9</th>\n",
       "      <th>Scenario 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Scenario 19806</th>\n",
       "      <th>Scenario 19807</th>\n",
       "      <th>Scenario 19808</th>\n",
       "      <th>Scenario 19809</th>\n",
       "      <th>Scenario 19810</th>\n",
       "      <th>Scenario 19811</th>\n",
       "      <th>Scenario 19812</th>\n",
       "      <th>Scenario 19813</th>\n",
       "      <th>Scenario 19814</th>\n",
       "      <th>Scenario 19815</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-50.000000+0.000000j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1000.00000+0.00000j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-3.024101e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-30.241006+0.000000j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-5.000000e+01+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-50.000000+0.000000j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-1000.00000+0.00000j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-999.999999+0.000000j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-4.997225e+01+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-999.999998+0.000000j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-4.996949e+01+0.000000e+00j</td>\n",
       "      <td>-4.541522e+01+8.844841e+00j</td>\n",
       "      <td>-4.805986e+01+8.005420e+00j</td>\n",
       "      <td>-4.751593e+01+8.246593e+00j</td>\n",
       "      <td>-4.799658e+01+8.065178e+00j</td>\n",
       "      <td>-3.178208e+01+0.000000e+00j</td>\n",
       "      <td>-4.752763e+01+8.254978e+00j</td>\n",
       "      <td>-4.801786e+01+8.072489e+00j</td>\n",
       "      <td>-4.533189e+01+8.850499e+00j</td>\n",
       "      <td>-4.539563e+01+8.832355e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.542038e+01+8.825096e+00j</td>\n",
       "      <td>-4.822295e+01+7.980638e+00j</td>\n",
       "      <td>-4.736156e+01+8.330170e+00j</td>\n",
       "      <td>-4.538858e+01+8.831742e+00j</td>\n",
       "      <td>-4.801776e+01+8.075638e+00j</td>\n",
       "      <td>-4.821297e+01+7.952461e+00j</td>\n",
       "      <td>-4.809936e+01+7.995917e+00j</td>\n",
       "      <td>-999.999753+0.000000j</td>\n",
       "      <td>-4.515031e+01+8.862549e+00j</td>\n",
       "      <td>-4.757701e+01+8.232589e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-4.518070e+01+8.875545e+00j</td>\n",
       "      <td>-4.541522e+01-8.844841e+00j</td>\n",
       "      <td>-4.805986e+01-8.005420e+00j</td>\n",
       "      <td>-4.751593e+01-8.246593e+00j</td>\n",
       "      <td>-4.799658e+01-8.065178e+00j</td>\n",
       "      <td>-2.712724e+01+0.000000e+00j</td>\n",
       "      <td>-4.752763e+01-8.254978e+00j</td>\n",
       "      <td>-4.801786e+01-8.072489e+00j</td>\n",
       "      <td>-4.533189e+01-8.850499e+00j</td>\n",
       "      <td>-4.539563e+01-8.832355e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.542038e+01-8.825096e+00j</td>\n",
       "      <td>-4.822295e+01-7.980638e+00j</td>\n",
       "      <td>-4.736156e+01-8.330170e+00j</td>\n",
       "      <td>-4.538858e+01-8.831742e+00j</td>\n",
       "      <td>-4.801776e+01-8.075638e+00j</td>\n",
       "      <td>-4.821297e+01-7.952461e+00j</td>\n",
       "      <td>-4.809936e+01-7.995917e+00j</td>\n",
       "      <td>-49.972977+0.000000j</td>\n",
       "      <td>-4.515031e+01-8.862549e+00j</td>\n",
       "      <td>-4.757701e+01-8.232589e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-4.518070e+01-8.875545e+00j</td>\n",
       "      <td>-4.997318e+01+0.000000e+00j</td>\n",
       "      <td>-4.997368e+01+0.000000e+00j</td>\n",
       "      <td>-4.997299e+01+0.000000e+00j</td>\n",
       "      <td>-4.997448e+01+0.000000e+00j</td>\n",
       "      <td>-2.235854e+00+1.397127e+01j</td>\n",
       "      <td>-4.997396e+01+0.000000e+00j</td>\n",
       "      <td>-4.997339e+01+0.000000e+00j</td>\n",
       "      <td>-4.997024e+01+0.000000e+00j</td>\n",
       "      <td>-4.996982e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.997313e+01+0.000000e+00j</td>\n",
       "      <td>-4.997451e+01+0.000000e+00j</td>\n",
       "      <td>-4.996958e+01+0.000000e+00j</td>\n",
       "      <td>-4.997312e+01+0.000000e+00j</td>\n",
       "      <td>-4.997023e+01+0.000000e+00j</td>\n",
       "      <td>-4.997466e+01+0.000000e+00j</td>\n",
       "      <td>-4.997022e+01+0.000000e+00j</td>\n",
       "      <td>-49.941029+0.000000j</td>\n",
       "      <td>-4.997286e+01+0.000000e+00j</td>\n",
       "      <td>-4.997013e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-3.360567e+01+0.000000e+00j</td>\n",
       "      <td>-4.993132e+01+0.000000e+00j</td>\n",
       "      <td>-4.993512e+01+0.000000e+00j</td>\n",
       "      <td>-4.992874e+01+0.000000e+00j</td>\n",
       "      <td>-4.993494e+01+0.000000e+00j</td>\n",
       "      <td>-2.235854e+00-1.397127e+01j</td>\n",
       "      <td>-4.993566e+01+0.000000e+00j</td>\n",
       "      <td>-3.802116e+01+0.000000e+00j</td>\n",
       "      <td>-4.993630e+01+0.000000e+00j</td>\n",
       "      <td>-4.992928e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.993613e+01+0.000000e+00j</td>\n",
       "      <td>-4.993149e+01+0.000000e+00j</td>\n",
       "      <td>-4.993465e+01+0.000000e+00j</td>\n",
       "      <td>-4.993489e+01+0.000000e+00j</td>\n",
       "      <td>-3.808626e+01+0.000000e+00j</td>\n",
       "      <td>-4.993227e+01+0.000000e+00j</td>\n",
       "      <td>-4.993494e+01+0.000000e+00j</td>\n",
       "      <td>-43.633658+8.743671j</td>\n",
       "      <td>-4.993628e+01+0.000000e+00j</td>\n",
       "      <td>-4.993509e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3.200945e+01+0.000000e+00j</td>\n",
       "      <td>-3.380793e+01+0.000000e+00j</td>\n",
       "      <td>-3.813048e+01+0.000000e+00j</td>\n",
       "      <td>-3.752501e+01+0.000000e+00j</td>\n",
       "      <td>-3.799214e+01+0.000000e+00j</td>\n",
       "      <td>-1.827441e+01+0.000000e+00j</td>\n",
       "      <td>-3.751461e+01+0.000000e+00j</td>\n",
       "      <td>-3.327804e+01+0.000000e+00j</td>\n",
       "      <td>-3.366293e+01+0.000000e+00j</td>\n",
       "      <td>-3.384288e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.387425e+01+0.000000e+00j</td>\n",
       "      <td>-3.821279e+01+0.000000e+00j</td>\n",
       "      <td>-3.722767e+01+0.000000e+00j</td>\n",
       "      <td>-3.380255e+01+0.000000e+00j</td>\n",
       "      <td>-3.284128e+01+0.000000e+00j</td>\n",
       "      <td>-3.828509e+01+0.000000e+00j</td>\n",
       "      <td>-3.814960e+01+0.000000e+00j</td>\n",
       "      <td>-43.633658-8.743671j</td>\n",
       "      <td>-3.266495e+01+0.000000e+00j</td>\n",
       "      <td>-3.756699e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-2.818329e+01+0.000000e+00j</td>\n",
       "      <td>-3.270792e+01+0.000000e+00j</td>\n",
       "      <td>-3.334199e+01+0.000000e+00j</td>\n",
       "      <td>-3.330350e+01+0.000000e+00j</td>\n",
       "      <td>-3.324219e+01+0.000000e+00j</td>\n",
       "      <td>-1.234670e+01+0.000000e+00j</td>\n",
       "      <td>-3.335564e+01+0.000000e+00j</td>\n",
       "      <td>-2.978243e+01+0.000000e+00j</td>\n",
       "      <td>-3.057122e+01+0.000000e+00j</td>\n",
       "      <td>-3.125660e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.292159e+01+0.000000e+00j</td>\n",
       "      <td>-3.326046e+01+0.000000e+00j</td>\n",
       "      <td>-3.206271e+01+0.000000e+00j</td>\n",
       "      <td>-3.267566e+01+0.000000e+00j</td>\n",
       "      <td>-2.950077e+01+0.000000e+00j</td>\n",
       "      <td>-3.345916e+01+0.000000e+00j</td>\n",
       "      <td>-3.288440e+01+0.000000e+00j</td>\n",
       "      <td>-31.030179+0.000000j</td>\n",
       "      <td>-3.343166e+01+0.000000e+00j</td>\n",
       "      <td>-3.290894e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-4.991036e+01+0.000000e+00j</td>\n",
       "      <td>-2.950612e+01+0.000000e+00j</td>\n",
       "      <td>-3.043924e+01+0.000000e+00j</td>\n",
       "      <td>-2.969931e+01+0.000000e+00j</td>\n",
       "      <td>-2.994588e+01+0.000000e+00j</td>\n",
       "      <td>-8.689411e+00+0.000000e+00j</td>\n",
       "      <td>-2.995962e+01+0.000000e+00j</td>\n",
       "      <td>-4.991036e+01+0.000000e+00j</td>\n",
       "      <td>-2.924791e+01+0.000000e+00j</td>\n",
       "      <td>-2.932420e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.975168e+01+0.000000e+00j</td>\n",
       "      <td>-3.046664e+01+0.000000e+00j</td>\n",
       "      <td>-2.997472e+01+0.000000e+00j</td>\n",
       "      <td>-2.940520e+01+0.000000e+00j</td>\n",
       "      <td>-2.597344e+01+0.000000e+00j</td>\n",
       "      <td>-3.051328e+01+0.000000e+00j</td>\n",
       "      <td>-2.956625e+01+0.000000e+00j</td>\n",
       "      <td>-28.005535+0.000000j</td>\n",
       "      <td>-2.949043e+01+0.000000e+00j</td>\n",
       "      <td>-2.967164e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-2.208684e+01+0.000000e+00j</td>\n",
       "      <td>-2.656016e+01+0.000000e+00j</td>\n",
       "      <td>-2.760969e+01+0.000000e+00j</td>\n",
       "      <td>-2.683373e+01+0.000000e+00j</td>\n",
       "      <td>-2.580198e+01+0.000000e+00j</td>\n",
       "      <td>-7.607472e+00+0.000000e+00j</td>\n",
       "      <td>-2.637017e+01+0.000000e+00j</td>\n",
       "      <td>-2.540316e+01+0.000000e+00j</td>\n",
       "      <td>-2.757200e+01+0.000000e+00j</td>\n",
       "      <td>-2.558983e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.758380e+01+0.000000e+00j</td>\n",
       "      <td>-2.639723e+01+0.000000e+00j</td>\n",
       "      <td>-2.706112e+01+0.000000e+00j</td>\n",
       "      <td>-2.695835e+01+0.000000e+00j</td>\n",
       "      <td>-4.991036e+01+0.000000e+00j</td>\n",
       "      <td>-2.638857e+01+0.000000e+00j</td>\n",
       "      <td>-2.745832e+01+0.000000e+00j</td>\n",
       "      <td>-24.956000+0.000000j</td>\n",
       "      <td>-2.650026e+01+0.000000e+00j</td>\n",
       "      <td>-2.736157e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.108769e+00+1.251623e+01j</td>\n",
       "      <td>-2.368393e+01+0.000000e+00j</td>\n",
       "      <td>-2.447333e+01+0.000000e+00j</td>\n",
       "      <td>-2.215740e+01+0.000000e+00j</td>\n",
       "      <td>-2.259375e+01+0.000000e+00j</td>\n",
       "      <td>-5.856738e-01+8.419188e-01j</td>\n",
       "      <td>-2.348029e+01+0.000000e+00j</td>\n",
       "      <td>-2.009746e+00+1.318564e+01j</td>\n",
       "      <td>-2.401152e+01+0.000000e+00j</td>\n",
       "      <td>-2.268199e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.418545e+01+0.000000e+00j</td>\n",
       "      <td>-2.383425e+01+0.000000e+00j</td>\n",
       "      <td>-2.424684e+01+0.000000e+00j</td>\n",
       "      <td>-2.398446e+01+0.000000e+00j</td>\n",
       "      <td>-2.051855e+00+1.390397e+01j</td>\n",
       "      <td>-2.390588e+01+0.000000e+00j</td>\n",
       "      <td>-2.417355e+01+0.000000e+00j</td>\n",
       "      <td>-23.662313+0.000000j</td>\n",
       "      <td>-2.409646e+01+0.000000e+00j</td>\n",
       "      <td>-2.430230e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-2.108769e+00-1.251623e+01j</td>\n",
       "      <td>-2.108382e+00+1.472277e+01j</td>\n",
       "      <td>-2.616708e+00+1.503744e+01j</td>\n",
       "      <td>-2.151875e+00+1.449906e+01j</td>\n",
       "      <td>-2.879272e+00+1.323777e+01j</td>\n",
       "      <td>-5.856738e-01-8.419188e-01j</td>\n",
       "      <td>-2.492177e+00+1.323114e+01j</td>\n",
       "      <td>-2.009746e+00-1.318564e+01j</td>\n",
       "      <td>-2.465046e+00+1.509705e+01j</td>\n",
       "      <td>-2.218815e+00+1.476103e+01j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.756337e+00+1.566992e+01j</td>\n",
       "      <td>-2.278552e+00+1.435003e+01j</td>\n",
       "      <td>-2.300784e+00+1.459796e+01j</td>\n",
       "      <td>-2.438194e+00+1.515575e+01j</td>\n",
       "      <td>-2.051855e+00-1.390397e+01j</td>\n",
       "      <td>-2.482523e+00+1.466977e+01j</td>\n",
       "      <td>-2.650365e+00+1.479759e+01j</td>\n",
       "      <td>-2.519163+17.724214j</td>\n",
       "      <td>-2.246570e+00+1.477584e+01j</td>\n",
       "      <td>-2.547493e+00+1.479194e+01j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-4.144137e-01+9.808973e+00j</td>\n",
       "      <td>-2.108382e+00-1.472277e+01j</td>\n",
       "      <td>-2.616708e+00-1.503744e+01j</td>\n",
       "      <td>-2.151875e+00-1.449906e+01j</td>\n",
       "      <td>-2.879272e+00-1.323777e+01j</td>\n",
       "      <td>-1.004172e+00+0.000000e+00j</td>\n",
       "      <td>-2.492177e+00-1.323114e+01j</td>\n",
       "      <td>-2.229466e+00+1.238629e+01j</td>\n",
       "      <td>-2.465046e+00-1.509705e+01j</td>\n",
       "      <td>-2.218815e+00-1.476103e+01j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.756337e+00-1.566992e+01j</td>\n",
       "      <td>-2.278552e+00-1.435003e+01j</td>\n",
       "      <td>-2.300784e+00-1.459796e+01j</td>\n",
       "      <td>-2.438194e+00-1.515575e+01j</td>\n",
       "      <td>-2.323699e+00+1.196302e+01j</td>\n",
       "      <td>-2.482523e+00-1.466977e+01j</td>\n",
       "      <td>-2.650365e+00-1.479759e+01j</td>\n",
       "      <td>-2.519163-17.724214j</td>\n",
       "      <td>-2.246570e+00-1.477584e+01j</td>\n",
       "      <td>-2.547493e+00-1.479194e+01j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-4.144137e-01-9.808973e+00j</td>\n",
       "      <td>-2.527727e+00+1.325679e+01j</td>\n",
       "      <td>-2.641145e+00+1.299707e+01j</td>\n",
       "      <td>-2.593388e+00+1.269962e+01j</td>\n",
       "      <td>-2.632725e+00+1.276021e+01j</td>\n",
       "      <td>-1.197868e-06+0.000000e+00j</td>\n",
       "      <td>-2.252503e+00+1.294271e+01j</td>\n",
       "      <td>-2.229466e+00-1.238629e+01j</td>\n",
       "      <td>-2.156797e+00+1.261819e+01j</td>\n",
       "      <td>-1.436001e+00+1.155366e+01j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.528354e+00+1.368498e+01j</td>\n",
       "      <td>-2.718879e+00+1.293008e+01j</td>\n",
       "      <td>-2.262176e+00+1.234287e+01j</td>\n",
       "      <td>-2.492242e+00+1.334721e+01j</td>\n",
       "      <td>-2.323699e+00-1.196302e+01j</td>\n",
       "      <td>-2.918771e+00+1.323338e+01j</td>\n",
       "      <td>-2.783152e+00+1.312907e+01j</td>\n",
       "      <td>-2.581083+15.769474j</td>\n",
       "      <td>-2.266112e+00+1.333104e+01j</td>\n",
       "      <td>-2.529688e+00+1.290237e+01j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-2.230345e+00+9.367706e+00j</td>\n",
       "      <td>-2.527727e+00-1.325679e+01j</td>\n",
       "      <td>-2.641145e+00-1.299707e+01j</td>\n",
       "      <td>-2.593388e+00-1.269962e+01j</td>\n",
       "      <td>-2.632725e+00-1.276021e+01j</td>\n",
       "      <td>-3.078263e-01+0.000000e+00j</td>\n",
       "      <td>-2.252503e+00-1.294271e+01j</td>\n",
       "      <td>1.607665e+00+8.816554e+00j</td>\n",
       "      <td>-2.156797e+00-1.261819e+01j</td>\n",
       "      <td>-1.436001e+00-1.155366e+01j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.528354e+00-1.368498e+01j</td>\n",
       "      <td>-2.718879e+00-1.293008e+01j</td>\n",
       "      <td>-2.262176e+00-1.234287e+01j</td>\n",
       "      <td>-2.492242e+00-1.334721e+01j</td>\n",
       "      <td>1.510684e+00+8.674597e+00j</td>\n",
       "      <td>-2.918771e+00-1.323338e+01j</td>\n",
       "      <td>-2.783152e+00-1.312907e+01j</td>\n",
       "      <td>-2.581083-15.769474j</td>\n",
       "      <td>-2.266112e+00-1.333104e+01j</td>\n",
       "      <td>-2.529688e+00-1.290237e+01j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-2.230345e+00-9.367706e+00j</td>\n",
       "      <td>-2.650810e+00+1.201203e+01j</td>\n",
       "      <td>1.855439e+00+8.586063e+00j</td>\n",
       "      <td>1.724677e+00+8.494078e+00j</td>\n",
       "      <td>1.736030e+00+8.562365e+00j</td>\n",
       "      <td>-1.529042e-01+0.000000e+00j</td>\n",
       "      <td>1.807165e+00+8.528168e+00j</td>\n",
       "      <td>1.607665e+00-8.816554e+00j</td>\n",
       "      <td>-1.234252e+00+1.124197e+01j</td>\n",
       "      <td>1.570066e+00+7.419796e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.536526e+00+1.235479e+01j</td>\n",
       "      <td>1.751303e+00+8.581940e+00j</td>\n",
       "      <td>1.697650e+00+8.302389e+00j</td>\n",
       "      <td>-2.261376e+00+1.217793e+01j</td>\n",
       "      <td>1.510684e+00-8.674597e+00j</td>\n",
       "      <td>1.780811e+00+8.643807e+00j</td>\n",
       "      <td>1.764014e+00+8.584571e+00j</td>\n",
       "      <td>-1.885774+14.519988j</td>\n",
       "      <td>-2.227150e+00+1.269666e+01j</td>\n",
       "      <td>1.830223e+00+8.494314e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.243422e+00+7.567015e+00j</td>\n",
       "      <td>-2.650810e+00-1.201203e+01j</td>\n",
       "      <td>1.855439e+00-8.586063e+00j</td>\n",
       "      <td>1.724677e+00-8.494078e+00j</td>\n",
       "      <td>1.736030e+00-8.562365e+00j</td>\n",
       "      <td>-9.999998e+02+0.000000e+00j</td>\n",
       "      <td>1.807165e+00-8.528168e+00j</td>\n",
       "      <td>-4.107595e+00+9.665444e+00j</td>\n",
       "      <td>-1.234252e+00-1.124197e+01j</td>\n",
       "      <td>1.570066e+00-7.419796e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.536526e+00-1.235479e+01j</td>\n",
       "      <td>1.751303e+00-8.581940e+00j</td>\n",
       "      <td>1.697650e+00-8.302389e+00j</td>\n",
       "      <td>-2.261376e+00-1.217793e+01j</td>\n",
       "      <td>-3.910239e+00+9.539650e+00j</td>\n",
       "      <td>1.780811e+00-8.643807e+00j</td>\n",
       "      <td>1.764014e+00-8.584571e+00j</td>\n",
       "      <td>-1.885774-14.519988j</td>\n",
       "      <td>-2.227150e+00-1.269666e+01j</td>\n",
       "      <td>1.830223e+00-8.494314e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.243422e+00-7.567015e+00j</td>\n",
       "      <td>1.446583e+00+7.173574e+00j</td>\n",
       "      <td>-2.389494e+00+1.166413e+01j</td>\n",
       "      <td>-7.394821e-01+9.220989e+00j</td>\n",
       "      <td>-1.475727e+00+1.127523e+01j</td>\n",
       "      <td>-4.363366e+01+8.743671e+00j</td>\n",
       "      <td>-1.661407e+00+1.149697e+01j</td>\n",
       "      <td>-4.107595e+00-9.665444e+00j</td>\n",
       "      <td>-2.489253e+00+9.213810e+00j</td>\n",
       "      <td>-1.052524e+00+1.013176e+01j</td>\n",
       "      <td>...</td>\n",
       "      <td>1.612711e+00+7.371855e+00j</td>\n",
       "      <td>-2.575343e+00+1.166916e+01j</td>\n",
       "      <td>-1.641208e+00+1.083610e+01j</td>\n",
       "      <td>1.554996e+00+7.250948e+00j</td>\n",
       "      <td>-3.910239e+00-9.539650e+00j</td>\n",
       "      <td>-2.840242e+00+1.191385e+01j</td>\n",
       "      <td>-1.997536e+00+1.084007e+01j</td>\n",
       "      <td>-18.985405+0.000000j</td>\n",
       "      <td>-2.233387e+00+9.414941e+00j</td>\n",
       "      <td>-1.807129e+00+1.074757e+01j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.858957e+01+0.000000e+00j</td>\n",
       "      <td>1.446583e+00-7.173574e+00j</td>\n",
       "      <td>-2.389494e+00-1.166413e+01j</td>\n",
       "      <td>-7.394821e-01-9.220989e+00j</td>\n",
       "      <td>-1.475727e+00-1.127523e+01j</td>\n",
       "      <td>-4.363366e+01-8.743671e+00j</td>\n",
       "      <td>-1.661407e+00-1.149697e+01j</td>\n",
       "      <td>-1.871028e+01+0.000000e+00j</td>\n",
       "      <td>-2.489253e+00-9.213810e+00j</td>\n",
       "      <td>-1.052524e+00-1.013176e+01j</td>\n",
       "      <td>...</td>\n",
       "      <td>1.612711e+00-7.371855e+00j</td>\n",
       "      <td>-2.575343e+00-1.166916e+01j</td>\n",
       "      <td>-1.641208e+00-1.083610e+01j</td>\n",
       "      <td>1.554996e+00-7.250948e+00j</td>\n",
       "      <td>-1.845406e+01+0.000000e+00j</td>\n",
       "      <td>-2.840242e+00-1.191385e+01j</td>\n",
       "      <td>-1.997536e+00-1.084007e+01j</td>\n",
       "      <td>-17.564882+0.000000j</td>\n",
       "      <td>-2.233387e+00-9.414941e+00j</td>\n",
       "      <td>-1.807129e+00-1.074757e+01j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.745205e+01+0.000000e+00j</td>\n",
       "      <td>-2.626455e+00+9.116671e+00j</td>\n",
       "      <td>-4.344945e+00+9.467973e+00j</td>\n",
       "      <td>-4.057242e+00+9.497336e+00j</td>\n",
       "      <td>-4.217833e+00+9.408689e+00j</td>\n",
       "      <td>-5.552430e-01+1.061762e+01j</td>\n",
       "      <td>-4.079542e+00+9.425006e+00j</td>\n",
       "      <td>-1.593057e+01+0.000000e+00j</td>\n",
       "      <td>1.420194e+00+7.007686e+00j</td>\n",
       "      <td>-2.662958e+00+9.341817e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.762606e+00+9.174267e+00j</td>\n",
       "      <td>-4.323739e+00+9.414391e+00j</td>\n",
       "      <td>-3.531841e+00+9.796642e+00j</td>\n",
       "      <td>-2.684192e+00+9.158974e+00j</td>\n",
       "      <td>-1.497426e+01+0.000000e+00j</td>\n",
       "      <td>-4.448062e+00+9.463512e+00j</td>\n",
       "      <td>-4.300016e+00+9.523139e+00j</td>\n",
       "      <td>-15.508366+0.000000j</td>\n",
       "      <td>1.381332e+00+7.100172e+00j</td>\n",
       "      <td>-4.094805e+00+9.482176e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.417009e+01+0.000000e+00j</td>\n",
       "      <td>-2.626455e+00-9.116671e+00j</td>\n",
       "      <td>-4.344945e+00-9.467973e+00j</td>\n",
       "      <td>-4.057242e+00-9.497336e+00j</td>\n",
       "      <td>-4.217833e+00-9.408689e+00j</td>\n",
       "      <td>-5.552430e-01-1.061762e+01j</td>\n",
       "      <td>-4.079542e+00-9.425006e+00j</td>\n",
       "      <td>-1.676595e+01+0.000000e+00j</td>\n",
       "      <td>1.420194e+00-7.007686e+00j</td>\n",
       "      <td>-2.662958e+00-9.341817e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.762606e+00-9.174267e+00j</td>\n",
       "      <td>-4.323739e+00-9.414391e+00j</td>\n",
       "      <td>-3.531841e+00-9.796642e+00j</td>\n",
       "      <td>-2.684192e+00-9.158974e+00j</td>\n",
       "      <td>-1.708382e+01+0.000000e+00j</td>\n",
       "      <td>-4.448062e+00-9.463512e+00j</td>\n",
       "      <td>-4.300016e+00-9.523139e+00j</td>\n",
       "      <td>-12.406587+0.000000j</td>\n",
       "      <td>1.381332e+00-7.100172e+00j</td>\n",
       "      <td>-4.094805e+00-9.482176e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-8.736220e+00+0.000000e+00j</td>\n",
       "      <td>-1.841289e+01+0.000000e+00j</td>\n",
       "      <td>-1.887281e+01+0.000000e+00j</td>\n",
       "      <td>-1.884939e+01+0.000000e+00j</td>\n",
       "      <td>-1.829118e+01+0.000000e+00j</td>\n",
       "      <td>-2.621537e+01+0.000000e+00j</td>\n",
       "      <td>-1.900681e+01+0.000000e+00j</td>\n",
       "      <td>-8.581714e+00+0.000000e+00j</td>\n",
       "      <td>-1.804594e+01+0.000000e+00j</td>\n",
       "      <td>-1.767913e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.910348e+01+0.000000e+00j</td>\n",
       "      <td>-1.846759e+01+0.000000e+00j</td>\n",
       "      <td>-1.829214e+01+0.000000e+00j</td>\n",
       "      <td>-1.848164e+01+0.000000e+00j</td>\n",
       "      <td>-8.592392e+00+0.000000e+00j</td>\n",
       "      <td>-1.858398e+01+0.000000e+00j</td>\n",
       "      <td>-1.820156e+01+0.000000e+00j</td>\n",
       "      <td>-8.8778430+0.0000000j</td>\n",
       "      <td>-1.946046e+01+0.000000e+00j</td>\n",
       "      <td>-1.883194e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-8.176703e+00+0.000000e+00j</td>\n",
       "      <td>-1.723120e+01+0.000000e+00j</td>\n",
       "      <td>-1.731042e+01+0.000000e+00j</td>\n",
       "      <td>-1.778642e+01+0.000000e+00j</td>\n",
       "      <td>-1.604073e+01+0.000000e+00j</td>\n",
       "      <td>-2.202259e+01+0.000000e+00j</td>\n",
       "      <td>-1.706448e+01+0.000000e+00j</td>\n",
       "      <td>-8.370385e+00+0.000000e+00j</td>\n",
       "      <td>-1.710448e+01+0.000000e+00j</td>\n",
       "      <td>-1.653969e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.726415e+01+0.000000e+00j</td>\n",
       "      <td>-1.666443e+01+0.000000e+00j</td>\n",
       "      <td>-1.717154e+01+0.000000e+00j</td>\n",
       "      <td>-1.690433e+01+0.000000e+00j</td>\n",
       "      <td>-8.196063e+00+0.000000e+00j</td>\n",
       "      <td>-1.641643e+01+0.000000e+00j</td>\n",
       "      <td>-1.671510e+01+0.000000e+00j</td>\n",
       "      <td>-7.8090830+0.0000000j</td>\n",
       "      <td>-1.670538e+01+0.000000e+00j</td>\n",
       "      <td>-1.658529e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.723253e+01+0.000000e+00j</td>\n",
       "      <td>-1.484610e+01+0.000000e+00j</td>\n",
       "      <td>-1.621494e+01+0.000000e+00j</td>\n",
       "      <td>-1.584647e+01+0.000000e+00j</td>\n",
       "      <td>-1.428456e+01+0.000000e+00j</td>\n",
       "      <td>-1.000000e+03+0.000000e+00j</td>\n",
       "      <td>-1.642414e+01+0.000000e+00j</td>\n",
       "      <td>-1.723253e+01+0.000000e+00j</td>\n",
       "      <td>-1.512107e+01+0.000000e+00j</td>\n",
       "      <td>-1.502977e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.531079e+01+0.000000e+00j</td>\n",
       "      <td>-1.580108e+01+0.000000e+00j</td>\n",
       "      <td>-1.633797e+01+0.000000e+00j</td>\n",
       "      <td>-1.512386e+01+0.000000e+00j</td>\n",
       "      <td>-1.723253e+01+0.000000e+00j</td>\n",
       "      <td>-1.550439e+01+0.000000e+00j</td>\n",
       "      <td>-1.490134e+01+0.000000e+00j</td>\n",
       "      <td>-0.555243+10.617616j</td>\n",
       "      <td>-1.570627e+01+0.000000e+00j</td>\n",
       "      <td>-1.525657e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-4.761905e+00+0.000000e+00j</td>\n",
       "      <td>-1.166816e+01+0.000000e+00j</td>\n",
       "      <td>-1.351082e+01+0.000000e+00j</td>\n",
       "      <td>-1.322920e+01+0.000000e+00j</td>\n",
       "      <td>-1.264601e+01+0.000000e+00j</td>\n",
       "      <td>-4.993822e+01+0.000000e+00j</td>\n",
       "      <td>-1.356588e+01+0.000000e+00j</td>\n",
       "      <td>-4.761905e+00+0.000000e+00j</td>\n",
       "      <td>-1.338120e+01+0.000000e+00j</td>\n",
       "      <td>-1.330008e+01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.314455e+01+0.000000e+00j</td>\n",
       "      <td>-1.196060e+01+0.000000e+00j</td>\n",
       "      <td>-1.417895e+01+0.000000e+00j</td>\n",
       "      <td>-1.337684e+01+0.000000e+00j</td>\n",
       "      <td>-4.761905e+00+0.000000e+00j</td>\n",
       "      <td>-1.151969e+01+0.000000e+00j</td>\n",
       "      <td>-1.316534e+01+0.000000e+00j</td>\n",
       "      <td>-0.555243-10.617616j</td>\n",
       "      <td>-1.374166e+01+0.000000e+00j</td>\n",
       "      <td>-1.389188e+01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-6.568366e-01+2.386633e+00j</td>\n",
       "      <td>-8.676042e+00+0.000000e+00j</td>\n",
       "      <td>-8.610389e+00+0.000000e+00j</td>\n",
       "      <td>-8.616701e+00+0.000000e+00j</td>\n",
       "      <td>-8.525122e+00+0.000000e+00j</td>\n",
       "      <td>-2.640101e+01+0.000000e+00j</td>\n",
       "      <td>-8.623900e+00+0.000000e+00j</td>\n",
       "      <td>-6.568366e-01+2.386633e+00j</td>\n",
       "      <td>-8.595434e+00+0.000000e+00j</td>\n",
       "      <td>-8.656181e+00+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.747668e+00+0.000000e+00j</td>\n",
       "      <td>-8.552050e+00+0.000000e+00j</td>\n",
       "      <td>-8.617552e+00+0.000000e+00j</td>\n",
       "      <td>-8.688605e+00+0.000000e+00j</td>\n",
       "      <td>-6.568366e-01+2.386633e+00j</td>\n",
       "      <td>-8.562531e+00+0.000000e+00j</td>\n",
       "      <td>-8.558530e+00+0.000000e+00j</td>\n",
       "      <td>-1.0225340+1.3294760j</td>\n",
       "      <td>-8.790636e+00+0.000000e+00j</td>\n",
       "      <td>-8.637091e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-6.568366e-01-2.386633e+00j</td>\n",
       "      <td>-8.200111e+00+0.000000e+00j</td>\n",
       "      <td>-8.343411e+00+0.000000e+00j</td>\n",
       "      <td>-8.321725e+00+0.000000e+00j</td>\n",
       "      <td>-8.318721e+00+0.000000e+00j</td>\n",
       "      <td>-1.941362e+01+0.000000e+00j</td>\n",
       "      <td>-8.374520e+00+0.000000e+00j</td>\n",
       "      <td>-6.568366e-01-2.386633e+00j</td>\n",
       "      <td>-8.402456e+00+0.000000e+00j</td>\n",
       "      <td>-8.458188e+00+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.238276e+00+0.000000e+00j</td>\n",
       "      <td>-8.340093e+00+0.000000e+00j</td>\n",
       "      <td>-8.511134e+00+0.000000e+00j</td>\n",
       "      <td>-8.218134e+00+0.000000e+00j</td>\n",
       "      <td>-6.568366e-01-2.386633e+00j</td>\n",
       "      <td>-8.285036e+00+0.000000e+00j</td>\n",
       "      <td>-8.146958e+00+0.000000e+00j</td>\n",
       "      <td>-1.0225340-1.3294760j</td>\n",
       "      <td>-8.331044e+00+0.000000e+00j</td>\n",
       "      <td>-8.227269e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-5.913697e-01+6.038079e-01j</td>\n",
       "      <td>-1.153053e+00+1.371947e+00j</td>\n",
       "      <td>-1.187271e+00+1.260550e+00j</td>\n",
       "      <td>-1.070050e+00+1.444852e+00j</td>\n",
       "      <td>-1.187472e+00+1.275286e+00j</td>\n",
       "      <td>-1.013037e+00+0.000000e+00j</td>\n",
       "      <td>-1.193483e+00+1.237424e+00j</td>\n",
       "      <td>-6.410903e-01+6.320596e-01j</td>\n",
       "      <td>-1.182951e+00+1.177165e+00j</td>\n",
       "      <td>-1.045364e+00+1.493667e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.175585e+00+1.195252e+00j</td>\n",
       "      <td>-1.145635e+00+1.399106e+00j</td>\n",
       "      <td>-1.186471e+00+1.258818e+00j</td>\n",
       "      <td>-1.165897e+00+1.228425e+00j</td>\n",
       "      <td>-6.207826e-01+7.017344e-01j</td>\n",
       "      <td>-1.145323e+00+1.375404e+00j</td>\n",
       "      <td>-1.185117e+00+1.279897e+00j</td>\n",
       "      <td>-0.5289180+0.7937210j</td>\n",
       "      <td>-1.135757e+00+1.191575e+00j</td>\n",
       "      <td>-1.191162e+00+1.258817e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-5.913697e-01-6.038079e-01j</td>\n",
       "      <td>-1.153053e+00-1.371947e+00j</td>\n",
       "      <td>-1.187271e+00-1.260550e+00j</td>\n",
       "      <td>-1.070050e+00-1.444852e+00j</td>\n",
       "      <td>-1.187472e+00-1.275286e+00j</td>\n",
       "      <td>-1.221090e+00+1.173747e+00j</td>\n",
       "      <td>-1.193483e+00-1.237424e+00j</td>\n",
       "      <td>-6.410903e-01-6.320596e-01j</td>\n",
       "      <td>-1.182951e+00-1.177165e+00j</td>\n",
       "      <td>-1.045364e+00-1.493667e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.175585e+00-1.195252e+00j</td>\n",
       "      <td>-1.145635e+00-1.399106e+00j</td>\n",
       "      <td>-1.186471e+00-1.258818e+00j</td>\n",
       "      <td>-1.165897e+00-1.228425e+00j</td>\n",
       "      <td>-6.207826e-01-7.017344e-01j</td>\n",
       "      <td>-1.145323e+00-1.375404e+00j</td>\n",
       "      <td>-1.185117e+00-1.279897e+00j</td>\n",
       "      <td>-0.5289180-0.7937210j</td>\n",
       "      <td>-1.135757e+00-1.191575e+00j</td>\n",
       "      <td>-1.191162e+00-1.258817e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-1.099220e+00+0.000000e+00j</td>\n",
       "      <td>1.565162e-07+0.000000e+00j</td>\n",
       "      <td>5.459872e-07+0.000000e+00j</td>\n",
       "      <td>-2.843306e-07+0.000000e+00j</td>\n",
       "      <td>3.241041e-07+0.000000e+00j</td>\n",
       "      <td>-1.221090e+00-1.173747e+00j</td>\n",
       "      <td>-8.309433e-07+0.000000e+00j</td>\n",
       "      <td>-1.860008e-01+0.000000e+00j</td>\n",
       "      <td>6.167129e-08+0.000000e+00j</td>\n",
       "      <td>-3.328709e-08+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>5.846560e-08+0.000000e+00j</td>\n",
       "      <td>1.150629e-07+0.000000e+00j</td>\n",
       "      <td>-1.801596e-07+0.000000e+00j</td>\n",
       "      <td>4.189514e-07+0.000000e+00j</td>\n",
       "      <td>-1.858857e-01+0.000000e+00j</td>\n",
       "      <td>-4.930683e-07+0.000000e+00j</td>\n",
       "      <td>-5.654428e-08+0.000000e+00j</td>\n",
       "      <td>0.0000030+0.0000000j</td>\n",
       "      <td>-1.825353e-08+0.000000e+00j</td>\n",
       "      <td>-5.004686e-07+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1.007849e+00+2.937368e-03j</td>\n",
       "      <td>-6.219983e-01+5.349144e-01j</td>\n",
       "      <td>-1.863090e-01+0.000000e+00j</td>\n",
       "      <td>-1.861114e-01+0.000000e+00j</td>\n",
       "      <td>-1.863265e-01+0.000000e+00j</td>\n",
       "      <td>-9.852675e-01+0.000000e+00j</td>\n",
       "      <td>-1.861870e-01+0.000000e+00j</td>\n",
       "      <td>2.017632e-07+0.000000e+00j</td>\n",
       "      <td>-5.784021e-01+5.295259e-01j</td>\n",
       "      <td>-5.969817e-01+5.521807e-01j</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.196099e-01+5.327362e-01j</td>\n",
       "      <td>-1.863949e-01+0.000000e+00j</td>\n",
       "      <td>-1.863550e-01+0.000000e+00j</td>\n",
       "      <td>-6.180762e-01+5.264913e-01j</td>\n",
       "      <td>-8.330554e-01+0.000000e+00j</td>\n",
       "      <td>-1.863613e-01+0.000000e+00j</td>\n",
       "      <td>-1.860579e-01+0.000000e+00j</td>\n",
       "      <td>-0.1405750+0.0000000j</td>\n",
       "      <td>-6.041052e-01+5.131972e-01j</td>\n",
       "      <td>-1.861383e-01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.007849e+00-2.937368e-03j</td>\n",
       "      <td>-6.219983e-01-5.349144e-01j</td>\n",
       "      <td>-6.469483e-01+6.245284e-01j</td>\n",
       "      <td>-6.402568e-01+6.373359e-01j</td>\n",
       "      <td>-6.462651e-01+6.202891e-01j</td>\n",
       "      <td>-1.976285e-01+0.000000e+00j</td>\n",
       "      <td>-6.497290e-01+6.147039e-01j</td>\n",
       "      <td>-2.694031e-09+0.000000e+00j</td>\n",
       "      <td>-5.784021e-01-5.295259e-01j</td>\n",
       "      <td>-5.969817e-01-5.521807e-01j</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.196099e-01-5.327362e-01j</td>\n",
       "      <td>-6.488509e-01+6.147873e-01j</td>\n",
       "      <td>-6.140420e-01+5.871639e-01j</td>\n",
       "      <td>-6.180762e-01-5.264913e-01j</td>\n",
       "      <td>-1.051661e+00+0.000000e+00j</td>\n",
       "      <td>-6.464288e-01+6.166708e-01j</td>\n",
       "      <td>-6.195383e-01+7.050075e-01j</td>\n",
       "      <td>-0.2530160+0.0000000j</td>\n",
       "      <td>-6.041052e-01-5.131972e-01j</td>\n",
       "      <td>-6.210831e-01+6.956186e-01j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-5.930979e-01+0.000000e+00j</td>\n",
       "      <td>-1.874133e-01+0.000000e+00j</td>\n",
       "      <td>-6.469483e-01-6.245284e-01j</td>\n",
       "      <td>-6.402568e-01-6.373359e-01j</td>\n",
       "      <td>-6.462651e-01-6.202891e-01j</td>\n",
       "      <td>-1.976284e-01+0.000000e+00j</td>\n",
       "      <td>-6.497290e-01-6.147039e-01j</td>\n",
       "      <td>-1.976285e-01+0.000000e+00j</td>\n",
       "      <td>-1.889523e-01+0.000000e+00j</td>\n",
       "      <td>-1.890055e-01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.875530e-01+0.000000e+00j</td>\n",
       "      <td>-6.488509e-01-6.147873e-01j</td>\n",
       "      <td>-6.140420e-01-5.871639e-01j</td>\n",
       "      <td>-1.875584e-01+0.000000e+00j</td>\n",
       "      <td>-9.552251e-01+0.000000e+00j</td>\n",
       "      <td>-6.464288e-01-6.166708e-01j</td>\n",
       "      <td>-6.195383e-01-7.050075e-01j</td>\n",
       "      <td>-0.5546420+0.0000000j</td>\n",
       "      <td>-1.890564e-01+0.000000e+00j</td>\n",
       "      <td>-6.210831e-01-6.956186e-01j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-7.064281e-01+0.000000e+00j</td>\n",
       "      <td>-5.307748e-01+0.000000e+00j</td>\n",
       "      <td>-8.373278e-01+0.000000e+00j</td>\n",
       "      <td>-8.349445e-01+0.000000e+00j</td>\n",
       "      <td>-7.886194e-01+0.000000e+00j</td>\n",
       "      <td>1.037752e-11+0.000000e+00j</td>\n",
       "      <td>-8.266339e-01+1.559102e-02j</td>\n",
       "      <td>-8.348231e-01+0.000000e+00j</td>\n",
       "      <td>-4.460000e-01+0.000000e+00j</td>\n",
       "      <td>-5.339992e-01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.560640e-01+0.000000e+00j</td>\n",
       "      <td>-8.390606e-01+0.000000e+00j</td>\n",
       "      <td>-8.109252e-01+0.000000e+00j</td>\n",
       "      <td>-5.183409e-01+0.000000e+00j</td>\n",
       "      <td>-1.019924e+00+0.000000e+00j</td>\n",
       "      <td>-8.426926e-01+0.000000e+00j</td>\n",
       "      <td>-8.326205e-01+0.000000e+00j</td>\n",
       "      <td>-1.0912310+0.0000000j</td>\n",
       "      <td>-5.296216e-01+0.000000e+00j</td>\n",
       "      <td>-8.270570e-01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1.896261e-01+0.000000e+00j</td>\n",
       "      <td>-1.068050e+00+0.000000e+00j</td>\n",
       "      <td>-1.047039e+00+0.000000e+00j</td>\n",
       "      <td>-1.070309e+00+0.000000e+00j</td>\n",
       "      <td>-8.380542e-01+0.000000e+00j</td>\n",
       "      <td>-5.563854e-08+0.000000e+00j</td>\n",
       "      <td>-8.266339e-01-1.559102e-02j</td>\n",
       "      <td>-9.347787e-01+0.000000e+00j</td>\n",
       "      <td>-1.080996e+00+0.000000e+00j</td>\n",
       "      <td>-9.283582e-01+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.073951e+00+0.000000e+00j</td>\n",
       "      <td>-9.561312e-01+0.000000e+00j</td>\n",
       "      <td>-9.546331e-01+0.000000e+00j</td>\n",
       "      <td>-9.348125e-01+0.000000e+00j</td>\n",
       "      <td>-6.666667e-01+0.000000e+00j</td>\n",
       "      <td>-9.413932e-01+0.000000e+00j</td>\n",
       "      <td>-1.043240e+00+0.000000e+00j</td>\n",
       "      <td>-1.0119590+0.0221850j</td>\n",
       "      <td>-1.049173e+00+0.000000e+00j</td>\n",
       "      <td>-9.713591e-01+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-1.131440e-07+0.000000e+00j</td>\n",
       "      <td>-9.431683e-01+0.000000e+00j</td>\n",
       "      <td>-1.017900e+00+0.000000e+00j</td>\n",
       "      <td>-9.540169e-01+0.000000e+00j</td>\n",
       "      <td>-1.047545e+00+0.000000e+00j</td>\n",
       "      <td>-1.103030e+00+0.000000e+00j</td>\n",
       "      <td>-1.082653e+00+0.000000e+00j</td>\n",
       "      <td>-1.051737e+00+0.000000e+00j</td>\n",
       "      <td>-9.670052e-01+0.000000e+00j</td>\n",
       "      <td>-1.055312e+00+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.471056e-01+0.000000e+00j</td>\n",
       "      <td>-9.531611e-01+0.000000e+00j</td>\n",
       "      <td>-1.052300e+00+0.000000e+00j</td>\n",
       "      <td>-1.068181e+00+0.000000e+00j</td>\n",
       "      <td>-1.004816e+00+0.000000e+00j</td>\n",
       "      <td>-9.503138e-01+0.000000e+00j</td>\n",
       "      <td>-9.741198e-01+0.000000e+00j</td>\n",
       "      <td>-1.0119590-0.0221850j</td>\n",
       "      <td>-9.703435e-01+0.000000e+00j</td>\n",
       "      <td>-1.039622e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-2.992171e-10+0.000000e+00j</td>\n",
       "      <td>-9.533766e-01+0.000000e+00j</td>\n",
       "      <td>-9.845790e-01+0.000000e+00j</td>\n",
       "      <td>-1.005259e+00+1.257624e-02j</td>\n",
       "      <td>-9.693773e-01+0.000000e+00j</td>\n",
       "      <td>-1.001605e+00+0.000000e+00j</td>\n",
       "      <td>-1.018074e+00+0.000000e+00j</td>\n",
       "      <td>-1.019676e+00+0.000000e+00j</td>\n",
       "      <td>-9.949231e-01+1.029646e-02j</td>\n",
       "      <td>-1.004626e+00+1.252746e-02j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.005071e+00+3.872870e-03j</td>\n",
       "      <td>-1.037703e+00+0.000000e+00j</td>\n",
       "      <td>-1.018458e+00+0.000000e+00j</td>\n",
       "      <td>-9.969529e-01+1.126147e-02j</td>\n",
       "      <td>-1.976285e-01+0.000000e+00j</td>\n",
       "      <td>-1.033439e+00+0.000000e+00j</td>\n",
       "      <td>-9.780176e-01+0.000000e+00j</td>\n",
       "      <td>-1.0012030+0.0000000j</td>\n",
       "      <td>-1.004684e+00+1.637869e-02j</td>\n",
       "      <td>-1.016630e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-1.976285e-01+0.000000e+00j</td>\n",
       "      <td>-1.001449e+00+0.000000e+00j</td>\n",
       "      <td>-9.938086e-01+0.000000e+00j</td>\n",
       "      <td>-1.005259e+00-1.257624e-02j</td>\n",
       "      <td>-1.021163e+00+0.000000e+00j</td>\n",
       "      <td>-1.102653e+00+0.000000e+00j</td>\n",
       "      <td>-9.889775e-01+0.000000e+00j</td>\n",
       "      <td>-1.005087e+00+0.000000e+00j</td>\n",
       "      <td>-9.949231e-01-1.029646e-02j</td>\n",
       "      <td>-1.004626e+00-1.252746e-02j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.005071e+00-3.872870e-03j</td>\n",
       "      <td>-1.019935e+00+0.000000e+00j</td>\n",
       "      <td>-1.002710e+00+6.527041e-03j</td>\n",
       "      <td>-9.969529e-01-1.126147e-02j</td>\n",
       "      <td>1.410956e-07+0.000000e+00j</td>\n",
       "      <td>-1.019426e+00+0.000000e+00j</td>\n",
       "      <td>-1.019152e+00+0.000000e+00j</td>\n",
       "      <td>-1.0130370+0.0000000j</td>\n",
       "      <td>-1.004684e+00-1.637869e-02j</td>\n",
       "      <td>-9.986679e-01+1.402810e-03j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-6.666667e-01+0.000000e+00j</td>\n",
       "      <td>-1.004696e+00+3.437373e-03j</td>\n",
       "      <td>-9.985471e-01+0.000000e+00j</td>\n",
       "      <td>-1.017366e+00+0.000000e+00j</td>\n",
       "      <td>-1.001953e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-9.994966e-01+0.000000e+00j</td>\n",
       "      <td>-6.666667e-01+0.000000e+00j</td>\n",
       "      <td>-1.002957e+00+3.828176e-03j</td>\n",
       "      <td>-1.005960e+00+4.346537e-03j</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.917700e-01+8.128550e-03j</td>\n",
       "      <td>-1.002047e+00+0.000000e+00j</td>\n",
       "      <td>-1.002710e+00-6.527041e-03j</td>\n",
       "      <td>-1.005138e+00+4.192293e-03j</td>\n",
       "      <td>-8.976516e-10+0.000000e+00j</td>\n",
       "      <td>-1.001584e+00+0.000000e+00j</td>\n",
       "      <td>-1.001234e+00+0.000000e+00j</td>\n",
       "      <td>-1.0000000+0.0000000j</td>\n",
       "      <td>-9.931485e-01+0.000000e+00j</td>\n",
       "      <td>-9.986679e-01-1.402810e-03j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-1.003828e+00+0.000000e+00j</td>\n",
       "      <td>-1.004696e+00-3.437373e-03j</td>\n",
       "      <td>-1.005094e+00+0.000000e+00j</td>\n",
       "      <td>-1.005067e+00+0.000000e+00j</td>\n",
       "      <td>-1.005202e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.005117e+00+0.000000e+00j</td>\n",
       "      <td>-1.003828e+00+0.000000e+00j</td>\n",
       "      <td>-1.002957e+00-3.828176e-03j</td>\n",
       "      <td>-1.005960e+00-4.346537e-03j</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.917700e-01-8.128550e-03j</td>\n",
       "      <td>-1.005169e+00+0.000000e+00j</td>\n",
       "      <td>-1.006315e+00+0.000000e+00j</td>\n",
       "      <td>-1.005138e+00-4.192293e-03j</td>\n",
       "      <td>-1.003828e+00+0.000000e+00j</td>\n",
       "      <td>-1.005205e+00+0.000000e+00j</td>\n",
       "      <td>-1.004816e+00+0.000000e+00j</td>\n",
       "      <td>-1.0180250+0.0000000j</td>\n",
       "      <td>-1.007060e+00+0.000000e+00j</td>\n",
       "      <td>-1.004906e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.0000000+0.0000000j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-5.1357880+0.0000000j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "      <td>-1.018025e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>0.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>0.0000000+0.0000000j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "      <td>-1.000000e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-1.942502e-01+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-0.1942500+0.0000000j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "      <td>-5.135788e+00+0.000000e+00j</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49 rows Ã— 19815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Scenario 1                  Scenario 2  \\\n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "7  -4.996949e+01+0.000000e+00j -4.541522e+01+8.844841e+00j   \n",
       "8  -4.518070e+01+8.875545e+00j -4.541522e+01-8.844841e+00j   \n",
       "9  -4.518070e+01-8.875545e+00j -4.997318e+01+0.000000e+00j   \n",
       "10 -3.360567e+01+0.000000e+00j -4.993132e+01+0.000000e+00j   \n",
       "11 -3.200945e+01+0.000000e+00j -3.380793e+01+0.000000e+00j   \n",
       "12 -2.818329e+01+0.000000e+00j -3.270792e+01+0.000000e+00j   \n",
       "13 -4.991036e+01+0.000000e+00j -2.950612e+01+0.000000e+00j   \n",
       "14 -2.208684e+01+0.000000e+00j -2.656016e+01+0.000000e+00j   \n",
       "15 -2.108769e+00+1.251623e+01j -2.368393e+01+0.000000e+00j   \n",
       "16 -2.108769e+00-1.251623e+01j -2.108382e+00+1.472277e+01j   \n",
       "17 -4.144137e-01+9.808973e+00j -2.108382e+00-1.472277e+01j   \n",
       "18 -4.144137e-01-9.808973e+00j -2.527727e+00+1.325679e+01j   \n",
       "19 -2.230345e+00+9.367706e+00j -2.527727e+00-1.325679e+01j   \n",
       "20 -2.230345e+00-9.367706e+00j -2.650810e+00+1.201203e+01j   \n",
       "21  1.243422e+00+7.567015e+00j -2.650810e+00-1.201203e+01j   \n",
       "22  1.243422e+00-7.567015e+00j  1.446583e+00+7.173574e+00j   \n",
       "23 -1.858957e+01+0.000000e+00j  1.446583e+00-7.173574e+00j   \n",
       "24 -1.745205e+01+0.000000e+00j -2.626455e+00+9.116671e+00j   \n",
       "25 -1.417009e+01+0.000000e+00j -2.626455e+00-9.116671e+00j   \n",
       "26 -8.736220e+00+0.000000e+00j -1.841289e+01+0.000000e+00j   \n",
       "27 -8.176703e+00+0.000000e+00j -1.723120e+01+0.000000e+00j   \n",
       "28 -1.723253e+01+0.000000e+00j -1.484610e+01+0.000000e+00j   \n",
       "29 -4.761905e+00+0.000000e+00j -1.166816e+01+0.000000e+00j   \n",
       "30 -6.568366e-01+2.386633e+00j -8.676042e+00+0.000000e+00j   \n",
       "31 -6.568366e-01-2.386633e+00j -8.200111e+00+0.000000e+00j   \n",
       "32 -5.913697e-01+6.038079e-01j -1.153053e+00+1.371947e+00j   \n",
       "33 -5.913697e-01-6.038079e-01j -1.153053e+00-1.371947e+00j   \n",
       "34 -1.099220e+00+0.000000e+00j  1.565162e-07+0.000000e+00j   \n",
       "35 -1.007849e+00+2.937368e-03j -6.219983e-01+5.349144e-01j   \n",
       "36 -1.007849e+00-2.937368e-03j -6.219983e-01-5.349144e-01j   \n",
       "37 -5.930979e-01+0.000000e+00j -1.874133e-01+0.000000e+00j   \n",
       "38 -7.064281e-01+0.000000e+00j -5.307748e-01+0.000000e+00j   \n",
       "39 -1.896261e-01+0.000000e+00j -1.068050e+00+0.000000e+00j   \n",
       "40 -1.131440e-07+0.000000e+00j -9.431683e-01+0.000000e+00j   \n",
       "41 -2.992171e-10+0.000000e+00j -9.533766e-01+0.000000e+00j   \n",
       "42 -1.976285e-01+0.000000e+00j -1.001449e+00+0.000000e+00j   \n",
       "43 -6.666667e-01+0.000000e+00j -1.004696e+00+3.437373e-03j   \n",
       "44 -1.003828e+00+0.000000e+00j -1.004696e+00-3.437373e-03j   \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
       "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
       "\n",
       "                    Scenario 3                  Scenario 4  \\\n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "7  -4.805986e+01+8.005420e+00j -4.751593e+01+8.246593e+00j   \n",
       "8  -4.805986e+01-8.005420e+00j -4.751593e+01-8.246593e+00j   \n",
       "9  -4.997368e+01+0.000000e+00j -4.997299e+01+0.000000e+00j   \n",
       "10 -4.993512e+01+0.000000e+00j -4.992874e+01+0.000000e+00j   \n",
       "11 -3.813048e+01+0.000000e+00j -3.752501e+01+0.000000e+00j   \n",
       "12 -3.334199e+01+0.000000e+00j -3.330350e+01+0.000000e+00j   \n",
       "13 -3.043924e+01+0.000000e+00j -2.969931e+01+0.000000e+00j   \n",
       "14 -2.760969e+01+0.000000e+00j -2.683373e+01+0.000000e+00j   \n",
       "15 -2.447333e+01+0.000000e+00j -2.215740e+01+0.000000e+00j   \n",
       "16 -2.616708e+00+1.503744e+01j -2.151875e+00+1.449906e+01j   \n",
       "17 -2.616708e+00-1.503744e+01j -2.151875e+00-1.449906e+01j   \n",
       "18 -2.641145e+00+1.299707e+01j -2.593388e+00+1.269962e+01j   \n",
       "19 -2.641145e+00-1.299707e+01j -2.593388e+00-1.269962e+01j   \n",
       "20  1.855439e+00+8.586063e+00j  1.724677e+00+8.494078e+00j   \n",
       "21  1.855439e+00-8.586063e+00j  1.724677e+00-8.494078e+00j   \n",
       "22 -2.389494e+00+1.166413e+01j -7.394821e-01+9.220989e+00j   \n",
       "23 -2.389494e+00-1.166413e+01j -7.394821e-01-9.220989e+00j   \n",
       "24 -4.344945e+00+9.467973e+00j -4.057242e+00+9.497336e+00j   \n",
       "25 -4.344945e+00-9.467973e+00j -4.057242e+00-9.497336e+00j   \n",
       "26 -1.887281e+01+0.000000e+00j -1.884939e+01+0.000000e+00j   \n",
       "27 -1.731042e+01+0.000000e+00j -1.778642e+01+0.000000e+00j   \n",
       "28 -1.621494e+01+0.000000e+00j -1.584647e+01+0.000000e+00j   \n",
       "29 -1.351082e+01+0.000000e+00j -1.322920e+01+0.000000e+00j   \n",
       "30 -8.610389e+00+0.000000e+00j -8.616701e+00+0.000000e+00j   \n",
       "31 -8.343411e+00+0.000000e+00j -8.321725e+00+0.000000e+00j   \n",
       "32 -1.187271e+00+1.260550e+00j -1.070050e+00+1.444852e+00j   \n",
       "33 -1.187271e+00-1.260550e+00j -1.070050e+00-1.444852e+00j   \n",
       "34  5.459872e-07+0.000000e+00j -2.843306e-07+0.000000e+00j   \n",
       "35 -1.863090e-01+0.000000e+00j -1.861114e-01+0.000000e+00j   \n",
       "36 -6.469483e-01+6.245284e-01j -6.402568e-01+6.373359e-01j   \n",
       "37 -6.469483e-01-6.245284e-01j -6.402568e-01-6.373359e-01j   \n",
       "38 -8.373278e-01+0.000000e+00j -8.349445e-01+0.000000e+00j   \n",
       "39 -1.047039e+00+0.000000e+00j -1.070309e+00+0.000000e+00j   \n",
       "40 -1.017900e+00+0.000000e+00j -9.540169e-01+0.000000e+00j   \n",
       "41 -9.845790e-01+0.000000e+00j -1.005259e+00+1.257624e-02j   \n",
       "42 -9.938086e-01+0.000000e+00j -1.005259e+00-1.257624e-02j   \n",
       "43 -9.985471e-01+0.000000e+00j -1.017366e+00+0.000000e+00j   \n",
       "44 -1.005094e+00+0.000000e+00j -1.005067e+00+0.000000e+00j   \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
       "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
       "\n",
       "                    Scenario 5                  Scenario 6  \\\n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "2  -5.000000e+01+0.000000e+00j -3.024101e+01+0.000000e+00j   \n",
       "3  -1.000000e+03+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "4  -9.999998e+02+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "6  -1.000000e+03+0.000000e+00j -4.997225e+01+0.000000e+00j   \n",
       "7  -4.799658e+01+8.065178e+00j -3.178208e+01+0.000000e+00j   \n",
       "8  -4.799658e+01-8.065178e+00j -2.712724e+01+0.000000e+00j   \n",
       "9  -4.997448e+01+0.000000e+00j -2.235854e+00+1.397127e+01j   \n",
       "10 -4.993494e+01+0.000000e+00j -2.235854e+00-1.397127e+01j   \n",
       "11 -3.799214e+01+0.000000e+00j -1.827441e+01+0.000000e+00j   \n",
       "12 -3.324219e+01+0.000000e+00j -1.234670e+01+0.000000e+00j   \n",
       "13 -2.994588e+01+0.000000e+00j -8.689411e+00+0.000000e+00j   \n",
       "14 -2.580198e+01+0.000000e+00j -7.607472e+00+0.000000e+00j   \n",
       "15 -2.259375e+01+0.000000e+00j -5.856738e-01+8.419188e-01j   \n",
       "16 -2.879272e+00+1.323777e+01j -5.856738e-01-8.419188e-01j   \n",
       "17 -2.879272e+00-1.323777e+01j -1.004172e+00+0.000000e+00j   \n",
       "18 -2.632725e+00+1.276021e+01j -1.197868e-06+0.000000e+00j   \n",
       "19 -2.632725e+00-1.276021e+01j -3.078263e-01+0.000000e+00j   \n",
       "20  1.736030e+00+8.562365e+00j -1.529042e-01+0.000000e+00j   \n",
       "21  1.736030e+00-8.562365e+00j -9.999998e+02+0.000000e+00j   \n",
       "22 -1.475727e+00+1.127523e+01j -4.363366e+01+8.743671e+00j   \n",
       "23 -1.475727e+00-1.127523e+01j -4.363366e+01-8.743671e+00j   \n",
       "24 -4.217833e+00+9.408689e+00j -5.552430e-01+1.061762e+01j   \n",
       "25 -4.217833e+00-9.408689e+00j -5.552430e-01-1.061762e+01j   \n",
       "26 -1.829118e+01+0.000000e+00j -2.621537e+01+0.000000e+00j   \n",
       "27 -1.604073e+01+0.000000e+00j -2.202259e+01+0.000000e+00j   \n",
       "28 -1.428456e+01+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "29 -1.264601e+01+0.000000e+00j -4.993822e+01+0.000000e+00j   \n",
       "30 -8.525122e+00+0.000000e+00j -2.640101e+01+0.000000e+00j   \n",
       "31 -8.318721e+00+0.000000e+00j -1.941362e+01+0.000000e+00j   \n",
       "32 -1.187472e+00+1.275286e+00j -1.013037e+00+0.000000e+00j   \n",
       "33 -1.187472e+00-1.275286e+00j -1.221090e+00+1.173747e+00j   \n",
       "34  3.241041e-07+0.000000e+00j -1.221090e+00-1.173747e+00j   \n",
       "35 -1.863265e-01+0.000000e+00j -9.852675e-01+0.000000e+00j   \n",
       "36 -6.462651e-01+6.202891e-01j -1.976285e-01+0.000000e+00j   \n",
       "37 -6.462651e-01-6.202891e-01j -1.976284e-01+0.000000e+00j   \n",
       "38 -7.886194e-01+0.000000e+00j  1.037752e-11+0.000000e+00j   \n",
       "39 -8.380542e-01+0.000000e+00j -5.563854e-08+0.000000e+00j   \n",
       "40 -1.047545e+00+0.000000e+00j -1.103030e+00+0.000000e+00j   \n",
       "41 -9.693773e-01+0.000000e+00j -1.001605e+00+0.000000e+00j   \n",
       "42 -1.021163e+00+0.000000e+00j -1.102653e+00+0.000000e+00j   \n",
       "43 -1.001953e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "44 -1.005202e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "46 -1.018025e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
       "47 -1.000000e+00+0.000000e+00j  0.000000e+00+0.000000e+00j   \n",
       "48 -5.135788e+00+0.000000e+00j -1.942502e-01+0.000000e+00j   \n",
       "\n",
       "                    Scenario 7                  Scenario 8  \\\n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "7  -4.752763e+01+8.254978e+00j -4.801786e+01+8.072489e+00j   \n",
       "8  -4.752763e+01-8.254978e+00j -4.801786e+01-8.072489e+00j   \n",
       "9  -4.997396e+01+0.000000e+00j -4.997339e+01+0.000000e+00j   \n",
       "10 -4.993566e+01+0.000000e+00j -3.802116e+01+0.000000e+00j   \n",
       "11 -3.751461e+01+0.000000e+00j -3.327804e+01+0.000000e+00j   \n",
       "12 -3.335564e+01+0.000000e+00j -2.978243e+01+0.000000e+00j   \n",
       "13 -2.995962e+01+0.000000e+00j -4.991036e+01+0.000000e+00j   \n",
       "14 -2.637017e+01+0.000000e+00j -2.540316e+01+0.000000e+00j   \n",
       "15 -2.348029e+01+0.000000e+00j -2.009746e+00+1.318564e+01j   \n",
       "16 -2.492177e+00+1.323114e+01j -2.009746e+00-1.318564e+01j   \n",
       "17 -2.492177e+00-1.323114e+01j -2.229466e+00+1.238629e+01j   \n",
       "18 -2.252503e+00+1.294271e+01j -2.229466e+00-1.238629e+01j   \n",
       "19 -2.252503e+00-1.294271e+01j  1.607665e+00+8.816554e+00j   \n",
       "20  1.807165e+00+8.528168e+00j  1.607665e+00-8.816554e+00j   \n",
       "21  1.807165e+00-8.528168e+00j -4.107595e+00+9.665444e+00j   \n",
       "22 -1.661407e+00+1.149697e+01j -4.107595e+00-9.665444e+00j   \n",
       "23 -1.661407e+00-1.149697e+01j -1.871028e+01+0.000000e+00j   \n",
       "24 -4.079542e+00+9.425006e+00j -1.593057e+01+0.000000e+00j   \n",
       "25 -4.079542e+00-9.425006e+00j -1.676595e+01+0.000000e+00j   \n",
       "26 -1.900681e+01+0.000000e+00j -8.581714e+00+0.000000e+00j   \n",
       "27 -1.706448e+01+0.000000e+00j -8.370385e+00+0.000000e+00j   \n",
       "28 -1.642414e+01+0.000000e+00j -1.723253e+01+0.000000e+00j   \n",
       "29 -1.356588e+01+0.000000e+00j -4.761905e+00+0.000000e+00j   \n",
       "30 -8.623900e+00+0.000000e+00j -6.568366e-01+2.386633e+00j   \n",
       "31 -8.374520e+00+0.000000e+00j -6.568366e-01-2.386633e+00j   \n",
       "32 -1.193483e+00+1.237424e+00j -6.410903e-01+6.320596e-01j   \n",
       "33 -1.193483e+00-1.237424e+00j -6.410903e-01-6.320596e-01j   \n",
       "34 -8.309433e-07+0.000000e+00j -1.860008e-01+0.000000e+00j   \n",
       "35 -1.861870e-01+0.000000e+00j  2.017632e-07+0.000000e+00j   \n",
       "36 -6.497290e-01+6.147039e-01j -2.694031e-09+0.000000e+00j   \n",
       "37 -6.497290e-01-6.147039e-01j -1.976285e-01+0.000000e+00j   \n",
       "38 -8.266339e-01+1.559102e-02j -8.348231e-01+0.000000e+00j   \n",
       "39 -8.266339e-01-1.559102e-02j -9.347787e-01+0.000000e+00j   \n",
       "40 -1.082653e+00+0.000000e+00j -1.051737e+00+0.000000e+00j   \n",
       "41 -1.018074e+00+0.000000e+00j -1.019676e+00+0.000000e+00j   \n",
       "42 -9.889775e-01+0.000000e+00j -1.005087e+00+0.000000e+00j   \n",
       "43 -9.994966e-01+0.000000e+00j -6.666667e-01+0.000000e+00j   \n",
       "44 -1.005117e+00+0.000000e+00j -1.003828e+00+0.000000e+00j   \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
       "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
       "\n",
       "                    Scenario 9                 Scenario 10  ...  \\\n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j  ...   \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  ...   \n",
       "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j  ...   \n",
       "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  ...   \n",
       "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j  ...   \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  ...   \n",
       "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  ...   \n",
       "7  -4.533189e+01+8.850499e+00j -4.539563e+01+8.832355e+00j  ...   \n",
       "8  -4.533189e+01-8.850499e+00j -4.539563e+01-8.832355e+00j  ...   \n",
       "9  -4.997024e+01+0.000000e+00j -4.996982e+01+0.000000e+00j  ...   \n",
       "10 -4.993630e+01+0.000000e+00j -4.992928e+01+0.000000e+00j  ...   \n",
       "11 -3.366293e+01+0.000000e+00j -3.384288e+01+0.000000e+00j  ...   \n",
       "12 -3.057122e+01+0.000000e+00j -3.125660e+01+0.000000e+00j  ...   \n",
       "13 -2.924791e+01+0.000000e+00j -2.932420e+01+0.000000e+00j  ...   \n",
       "14 -2.757200e+01+0.000000e+00j -2.558983e+01+0.000000e+00j  ...   \n",
       "15 -2.401152e+01+0.000000e+00j -2.268199e+01+0.000000e+00j  ...   \n",
       "16 -2.465046e+00+1.509705e+01j -2.218815e+00+1.476103e+01j  ...   \n",
       "17 -2.465046e+00-1.509705e+01j -2.218815e+00-1.476103e+01j  ...   \n",
       "18 -2.156797e+00+1.261819e+01j -1.436001e+00+1.155366e+01j  ...   \n",
       "19 -2.156797e+00-1.261819e+01j -1.436001e+00-1.155366e+01j  ...   \n",
       "20 -1.234252e+00+1.124197e+01j  1.570066e+00+7.419796e+00j  ...   \n",
       "21 -1.234252e+00-1.124197e+01j  1.570066e+00-7.419796e+00j  ...   \n",
       "22 -2.489253e+00+9.213810e+00j -1.052524e+00+1.013176e+01j  ...   \n",
       "23 -2.489253e+00-9.213810e+00j -1.052524e+00-1.013176e+01j  ...   \n",
       "24  1.420194e+00+7.007686e+00j -2.662958e+00+9.341817e+00j  ...   \n",
       "25  1.420194e+00-7.007686e+00j -2.662958e+00-9.341817e+00j  ...   \n",
       "26 -1.804594e+01+0.000000e+00j -1.767913e+01+0.000000e+00j  ...   \n",
       "27 -1.710448e+01+0.000000e+00j -1.653969e+01+0.000000e+00j  ...   \n",
       "28 -1.512107e+01+0.000000e+00j -1.502977e+01+0.000000e+00j  ...   \n",
       "29 -1.338120e+01+0.000000e+00j -1.330008e+01+0.000000e+00j  ...   \n",
       "30 -8.595434e+00+0.000000e+00j -8.656181e+00+0.000000e+00j  ...   \n",
       "31 -8.402456e+00+0.000000e+00j -8.458188e+00+0.000000e+00j  ...   \n",
       "32 -1.182951e+00+1.177165e+00j -1.045364e+00+1.493667e+00j  ...   \n",
       "33 -1.182951e+00-1.177165e+00j -1.045364e+00-1.493667e+00j  ...   \n",
       "34  6.167129e-08+0.000000e+00j -3.328709e-08+0.000000e+00j  ...   \n",
       "35 -5.784021e-01+5.295259e-01j -5.969817e-01+5.521807e-01j  ...   \n",
       "36 -5.784021e-01-5.295259e-01j -5.969817e-01-5.521807e-01j  ...   \n",
       "37 -1.889523e-01+0.000000e+00j -1.890055e-01+0.000000e+00j  ...   \n",
       "38 -4.460000e-01+0.000000e+00j -5.339992e-01+0.000000e+00j  ...   \n",
       "39 -1.080996e+00+0.000000e+00j -9.283582e-01+0.000000e+00j  ...   \n",
       "40 -9.670052e-01+0.000000e+00j -1.055312e+00+0.000000e+00j  ...   \n",
       "41 -9.949231e-01+1.029646e-02j -1.004626e+00+1.252746e-02j  ...   \n",
       "42 -9.949231e-01-1.029646e-02j -1.004626e+00-1.252746e-02j  ...   \n",
       "43 -1.002957e+00+3.828176e-03j -1.005960e+00+4.346537e-03j  ...   \n",
       "44 -1.002957e+00-3.828176e-03j -1.005960e+00-4.346537e-03j  ...   \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j  ...   \n",
       "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j  ...   \n",
       "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j  ...   \n",
       "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j  ...   \n",
       "\n",
       "                Scenario 19806              Scenario 19807  \\\n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "7  -4.542038e+01+8.825096e+00j -4.822295e+01+7.980638e+00j   \n",
       "8  -4.542038e+01-8.825096e+00j -4.822295e+01-7.980638e+00j   \n",
       "9  -4.997313e+01+0.000000e+00j -4.997451e+01+0.000000e+00j   \n",
       "10 -4.993613e+01+0.000000e+00j -4.993149e+01+0.000000e+00j   \n",
       "11 -3.387425e+01+0.000000e+00j -3.821279e+01+0.000000e+00j   \n",
       "12 -3.292159e+01+0.000000e+00j -3.326046e+01+0.000000e+00j   \n",
       "13 -2.975168e+01+0.000000e+00j -3.046664e+01+0.000000e+00j   \n",
       "14 -2.758380e+01+0.000000e+00j -2.639723e+01+0.000000e+00j   \n",
       "15 -2.418545e+01+0.000000e+00j -2.383425e+01+0.000000e+00j   \n",
       "16 -2.756337e+00+1.566992e+01j -2.278552e+00+1.435003e+01j   \n",
       "17 -2.756337e+00-1.566992e+01j -2.278552e+00-1.435003e+01j   \n",
       "18 -2.528354e+00+1.368498e+01j -2.718879e+00+1.293008e+01j   \n",
       "19 -2.528354e+00-1.368498e+01j -2.718879e+00-1.293008e+01j   \n",
       "20 -2.536526e+00+1.235479e+01j  1.751303e+00+8.581940e+00j   \n",
       "21 -2.536526e+00-1.235479e+01j  1.751303e+00-8.581940e+00j   \n",
       "22  1.612711e+00+7.371855e+00j -2.575343e+00+1.166916e+01j   \n",
       "23  1.612711e+00-7.371855e+00j -2.575343e+00-1.166916e+01j   \n",
       "24 -2.762606e+00+9.174267e+00j -4.323739e+00+9.414391e+00j   \n",
       "25 -2.762606e+00-9.174267e+00j -4.323739e+00-9.414391e+00j   \n",
       "26 -1.910348e+01+0.000000e+00j -1.846759e+01+0.000000e+00j   \n",
       "27 -1.726415e+01+0.000000e+00j -1.666443e+01+0.000000e+00j   \n",
       "28 -1.531079e+01+0.000000e+00j -1.580108e+01+0.000000e+00j   \n",
       "29 -1.314455e+01+0.000000e+00j -1.196060e+01+0.000000e+00j   \n",
       "30 -8.747668e+00+0.000000e+00j -8.552050e+00+0.000000e+00j   \n",
       "31 -8.238276e+00+0.000000e+00j -8.340093e+00+0.000000e+00j   \n",
       "32 -1.175585e+00+1.195252e+00j -1.145635e+00+1.399106e+00j   \n",
       "33 -1.175585e+00-1.195252e+00j -1.145635e+00-1.399106e+00j   \n",
       "34  5.846560e-08+0.000000e+00j  1.150629e-07+0.000000e+00j   \n",
       "35 -6.196099e-01+5.327362e-01j -1.863949e-01+0.000000e+00j   \n",
       "36 -6.196099e-01-5.327362e-01j -6.488509e-01+6.147873e-01j   \n",
       "37 -1.875530e-01+0.000000e+00j -6.488509e-01-6.147873e-01j   \n",
       "38 -5.560640e-01+0.000000e+00j -8.390606e-01+0.000000e+00j   \n",
       "39 -1.073951e+00+0.000000e+00j -9.561312e-01+0.000000e+00j   \n",
       "40 -9.471056e-01+0.000000e+00j -9.531611e-01+0.000000e+00j   \n",
       "41 -1.005071e+00+3.872870e-03j -1.037703e+00+0.000000e+00j   \n",
       "42 -1.005071e+00-3.872870e-03j -1.019935e+00+0.000000e+00j   \n",
       "43 -9.917700e-01+8.128550e-03j -1.002047e+00+0.000000e+00j   \n",
       "44 -9.917700e-01-8.128550e-03j -1.005169e+00+0.000000e+00j   \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
       "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
       "\n",
       "                Scenario 19808              Scenario 19809  \\\n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "7  -4.736156e+01+8.330170e+00j -4.538858e+01+8.831742e+00j   \n",
       "8  -4.736156e+01-8.330170e+00j -4.538858e+01-8.831742e+00j   \n",
       "9  -4.996958e+01+0.000000e+00j -4.997312e+01+0.000000e+00j   \n",
       "10 -4.993465e+01+0.000000e+00j -4.993489e+01+0.000000e+00j   \n",
       "11 -3.722767e+01+0.000000e+00j -3.380255e+01+0.000000e+00j   \n",
       "12 -3.206271e+01+0.000000e+00j -3.267566e+01+0.000000e+00j   \n",
       "13 -2.997472e+01+0.000000e+00j -2.940520e+01+0.000000e+00j   \n",
       "14 -2.706112e+01+0.000000e+00j -2.695835e+01+0.000000e+00j   \n",
       "15 -2.424684e+01+0.000000e+00j -2.398446e+01+0.000000e+00j   \n",
       "16 -2.300784e+00+1.459796e+01j -2.438194e+00+1.515575e+01j   \n",
       "17 -2.300784e+00-1.459796e+01j -2.438194e+00-1.515575e+01j   \n",
       "18 -2.262176e+00+1.234287e+01j -2.492242e+00+1.334721e+01j   \n",
       "19 -2.262176e+00-1.234287e+01j -2.492242e+00-1.334721e+01j   \n",
       "20  1.697650e+00+8.302389e+00j -2.261376e+00+1.217793e+01j   \n",
       "21  1.697650e+00-8.302389e+00j -2.261376e+00-1.217793e+01j   \n",
       "22 -1.641208e+00+1.083610e+01j  1.554996e+00+7.250948e+00j   \n",
       "23 -1.641208e+00-1.083610e+01j  1.554996e+00-7.250948e+00j   \n",
       "24 -3.531841e+00+9.796642e+00j -2.684192e+00+9.158974e+00j   \n",
       "25 -3.531841e+00-9.796642e+00j -2.684192e+00-9.158974e+00j   \n",
       "26 -1.829214e+01+0.000000e+00j -1.848164e+01+0.000000e+00j   \n",
       "27 -1.717154e+01+0.000000e+00j -1.690433e+01+0.000000e+00j   \n",
       "28 -1.633797e+01+0.000000e+00j -1.512386e+01+0.000000e+00j   \n",
       "29 -1.417895e+01+0.000000e+00j -1.337684e+01+0.000000e+00j   \n",
       "30 -8.617552e+00+0.000000e+00j -8.688605e+00+0.000000e+00j   \n",
       "31 -8.511134e+00+0.000000e+00j -8.218134e+00+0.000000e+00j   \n",
       "32 -1.186471e+00+1.258818e+00j -1.165897e+00+1.228425e+00j   \n",
       "33 -1.186471e+00-1.258818e+00j -1.165897e+00-1.228425e+00j   \n",
       "34 -1.801596e-07+0.000000e+00j  4.189514e-07+0.000000e+00j   \n",
       "35 -1.863550e-01+0.000000e+00j -6.180762e-01+5.264913e-01j   \n",
       "36 -6.140420e-01+5.871639e-01j -6.180762e-01-5.264913e-01j   \n",
       "37 -6.140420e-01-5.871639e-01j -1.875584e-01+0.000000e+00j   \n",
       "38 -8.109252e-01+0.000000e+00j -5.183409e-01+0.000000e+00j   \n",
       "39 -9.546331e-01+0.000000e+00j -9.348125e-01+0.000000e+00j   \n",
       "40 -1.052300e+00+0.000000e+00j -1.068181e+00+0.000000e+00j   \n",
       "41 -1.018458e+00+0.000000e+00j -9.969529e-01+1.126147e-02j   \n",
       "42 -1.002710e+00+6.527041e-03j -9.969529e-01-1.126147e-02j   \n",
       "43 -1.002710e+00-6.527041e-03j -1.005138e+00+4.192293e-03j   \n",
       "44 -1.006315e+00+0.000000e+00j -1.005138e+00-4.192293e-03j   \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
       "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
       "\n",
       "                Scenario 19810              Scenario 19811  \\\n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j   \n",
       "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j   \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j   \n",
       "7  -4.801776e+01+8.075638e+00j -4.821297e+01+7.952461e+00j   \n",
       "8  -4.801776e+01-8.075638e+00j -4.821297e+01-7.952461e+00j   \n",
       "9  -4.997023e+01+0.000000e+00j -4.997466e+01+0.000000e+00j   \n",
       "10 -3.808626e+01+0.000000e+00j -4.993227e+01+0.000000e+00j   \n",
       "11 -3.284128e+01+0.000000e+00j -3.828509e+01+0.000000e+00j   \n",
       "12 -2.950077e+01+0.000000e+00j -3.345916e+01+0.000000e+00j   \n",
       "13 -2.597344e+01+0.000000e+00j -3.051328e+01+0.000000e+00j   \n",
       "14 -4.991036e+01+0.000000e+00j -2.638857e+01+0.000000e+00j   \n",
       "15 -2.051855e+00+1.390397e+01j -2.390588e+01+0.000000e+00j   \n",
       "16 -2.051855e+00-1.390397e+01j -2.482523e+00+1.466977e+01j   \n",
       "17 -2.323699e+00+1.196302e+01j -2.482523e+00-1.466977e+01j   \n",
       "18 -2.323699e+00-1.196302e+01j -2.918771e+00+1.323338e+01j   \n",
       "19  1.510684e+00+8.674597e+00j -2.918771e+00-1.323338e+01j   \n",
       "20  1.510684e+00-8.674597e+00j  1.780811e+00+8.643807e+00j   \n",
       "21 -3.910239e+00+9.539650e+00j  1.780811e+00-8.643807e+00j   \n",
       "22 -3.910239e+00-9.539650e+00j -2.840242e+00+1.191385e+01j   \n",
       "23 -1.845406e+01+0.000000e+00j -2.840242e+00-1.191385e+01j   \n",
       "24 -1.497426e+01+0.000000e+00j -4.448062e+00+9.463512e+00j   \n",
       "25 -1.708382e+01+0.000000e+00j -4.448062e+00-9.463512e+00j   \n",
       "26 -8.592392e+00+0.000000e+00j -1.858398e+01+0.000000e+00j   \n",
       "27 -8.196063e+00+0.000000e+00j -1.641643e+01+0.000000e+00j   \n",
       "28 -1.723253e+01+0.000000e+00j -1.550439e+01+0.000000e+00j   \n",
       "29 -4.761905e+00+0.000000e+00j -1.151969e+01+0.000000e+00j   \n",
       "30 -6.568366e-01+2.386633e+00j -8.562531e+00+0.000000e+00j   \n",
       "31 -6.568366e-01-2.386633e+00j -8.285036e+00+0.000000e+00j   \n",
       "32 -6.207826e-01+7.017344e-01j -1.145323e+00+1.375404e+00j   \n",
       "33 -6.207826e-01-7.017344e-01j -1.145323e+00-1.375404e+00j   \n",
       "34 -1.858857e-01+0.000000e+00j -4.930683e-07+0.000000e+00j   \n",
       "35 -8.330554e-01+0.000000e+00j -1.863613e-01+0.000000e+00j   \n",
       "36 -1.051661e+00+0.000000e+00j -6.464288e-01+6.166708e-01j   \n",
       "37 -9.552251e-01+0.000000e+00j -6.464288e-01-6.166708e-01j   \n",
       "38 -1.019924e+00+0.000000e+00j -8.426926e-01+0.000000e+00j   \n",
       "39 -6.666667e-01+0.000000e+00j -9.413932e-01+0.000000e+00j   \n",
       "40 -1.004816e+00+0.000000e+00j -9.503138e-01+0.000000e+00j   \n",
       "41 -1.976285e-01+0.000000e+00j -1.033439e+00+0.000000e+00j   \n",
       "42  1.410956e-07+0.000000e+00j -1.019426e+00+0.000000e+00j   \n",
       "43 -8.976516e-10+0.000000e+00j -1.001584e+00+0.000000e+00j   \n",
       "44 -1.003828e+00+0.000000e+00j -1.005205e+00+0.000000e+00j   \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j   \n",
       "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j   \n",
       "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j   \n",
       "\n",
       "                Scenario 19812        Scenario 19813  \\\n",
       "0  -5.000000e+01+0.000000e+00j  -50.000000+0.000000j   \n",
       "1  -1.000000e+03+0.000000e+00j  -1000.00000+0.00000j   \n",
       "2  -5.000000e+01+0.000000e+00j  -30.241006+0.000000j   \n",
       "3  -1.000000e+03+0.000000e+00j  -50.000000+0.000000j   \n",
       "4  -9.999998e+02+0.000000e+00j  -1000.00000+0.00000j   \n",
       "5  -1.000000e+03+0.000000e+00j -999.999999+0.000000j   \n",
       "6  -1.000000e+03+0.000000e+00j -999.999998+0.000000j   \n",
       "7  -4.809936e+01+7.995917e+00j -999.999753+0.000000j   \n",
       "8  -4.809936e+01-7.995917e+00j  -49.972977+0.000000j   \n",
       "9  -4.997022e+01+0.000000e+00j  -49.941029+0.000000j   \n",
       "10 -4.993494e+01+0.000000e+00j  -43.633658+8.743671j   \n",
       "11 -3.814960e+01+0.000000e+00j  -43.633658-8.743671j   \n",
       "12 -3.288440e+01+0.000000e+00j  -31.030179+0.000000j   \n",
       "13 -2.956625e+01+0.000000e+00j  -28.005535+0.000000j   \n",
       "14 -2.745832e+01+0.000000e+00j  -24.956000+0.000000j   \n",
       "15 -2.417355e+01+0.000000e+00j  -23.662313+0.000000j   \n",
       "16 -2.650365e+00+1.479759e+01j  -2.519163+17.724214j   \n",
       "17 -2.650365e+00-1.479759e+01j  -2.519163-17.724214j   \n",
       "18 -2.783152e+00+1.312907e+01j  -2.581083+15.769474j   \n",
       "19 -2.783152e+00-1.312907e+01j  -2.581083-15.769474j   \n",
       "20  1.764014e+00+8.584571e+00j  -1.885774+14.519988j   \n",
       "21  1.764014e+00-8.584571e+00j  -1.885774-14.519988j   \n",
       "22 -1.997536e+00+1.084007e+01j  -18.985405+0.000000j   \n",
       "23 -1.997536e+00-1.084007e+01j  -17.564882+0.000000j   \n",
       "24 -4.300016e+00+9.523139e+00j  -15.508366+0.000000j   \n",
       "25 -4.300016e+00-9.523139e+00j  -12.406587+0.000000j   \n",
       "26 -1.820156e+01+0.000000e+00j -8.8778430+0.0000000j   \n",
       "27 -1.671510e+01+0.000000e+00j -7.8090830+0.0000000j   \n",
       "28 -1.490134e+01+0.000000e+00j  -0.555243+10.617616j   \n",
       "29 -1.316534e+01+0.000000e+00j  -0.555243-10.617616j   \n",
       "30 -8.558530e+00+0.000000e+00j -1.0225340+1.3294760j   \n",
       "31 -8.146958e+00+0.000000e+00j -1.0225340-1.3294760j   \n",
       "32 -1.185117e+00+1.279897e+00j -0.5289180+0.7937210j   \n",
       "33 -1.185117e+00-1.279897e+00j -0.5289180-0.7937210j   \n",
       "34 -5.654428e-08+0.000000e+00j  0.0000030+0.0000000j   \n",
       "35 -1.860579e-01+0.000000e+00j -0.1405750+0.0000000j   \n",
       "36 -6.195383e-01+7.050075e-01j -0.2530160+0.0000000j   \n",
       "37 -6.195383e-01-7.050075e-01j -0.5546420+0.0000000j   \n",
       "38 -8.326205e-01+0.000000e+00j -1.0912310+0.0000000j   \n",
       "39 -1.043240e+00+0.000000e+00j -1.0119590+0.0221850j   \n",
       "40 -9.741198e-01+0.000000e+00j -1.0119590-0.0221850j   \n",
       "41 -9.780176e-01+0.000000e+00j -1.0012030+0.0000000j   \n",
       "42 -1.019152e+00+0.000000e+00j -1.0130370+0.0000000j   \n",
       "43 -1.001234e+00+0.000000e+00j -1.0000000+0.0000000j   \n",
       "44 -1.004816e+00+0.000000e+00j -1.0180250+0.0000000j   \n",
       "45 -1.000000e+00+0.000000e+00j -1.0000000+0.0000000j   \n",
       "46 -1.018025e+00+0.000000e+00j -5.1357880+0.0000000j   \n",
       "47 -1.000000e+00+0.000000e+00j  0.0000000+0.0000000j   \n",
       "48 -5.135788e+00+0.000000e+00j -0.1942500+0.0000000j   \n",
       "\n",
       "                Scenario 19814              Scenario 19815  \n",
       "0  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j  \n",
       "1  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  \n",
       "2  -5.000000e+01+0.000000e+00j -5.000000e+01+0.000000e+00j  \n",
       "3  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  \n",
       "4  -9.999998e+02+0.000000e+00j -9.999998e+02+0.000000e+00j  \n",
       "5  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  \n",
       "6  -1.000000e+03+0.000000e+00j -1.000000e+03+0.000000e+00j  \n",
       "7  -4.515031e+01+8.862549e+00j -4.757701e+01+8.232589e+00j  \n",
       "8  -4.515031e+01-8.862549e+00j -4.757701e+01-8.232589e+00j  \n",
       "9  -4.997286e+01+0.000000e+00j -4.997013e+01+0.000000e+00j  \n",
       "10 -4.993628e+01+0.000000e+00j -4.993509e+01+0.000000e+00j  \n",
       "11 -3.266495e+01+0.000000e+00j -3.756699e+01+0.000000e+00j  \n",
       "12 -3.343166e+01+0.000000e+00j -3.290894e+01+0.000000e+00j  \n",
       "13 -2.949043e+01+0.000000e+00j -2.967164e+01+0.000000e+00j  \n",
       "14 -2.650026e+01+0.000000e+00j -2.736157e+01+0.000000e+00j  \n",
       "15 -2.409646e+01+0.000000e+00j -2.430230e+01+0.000000e+00j  \n",
       "16 -2.246570e+00+1.477584e+01j -2.547493e+00+1.479194e+01j  \n",
       "17 -2.246570e+00-1.477584e+01j -2.547493e+00-1.479194e+01j  \n",
       "18 -2.266112e+00+1.333104e+01j -2.529688e+00+1.290237e+01j  \n",
       "19 -2.266112e+00-1.333104e+01j -2.529688e+00-1.290237e+01j  \n",
       "20 -2.227150e+00+1.269666e+01j  1.830223e+00+8.494314e+00j  \n",
       "21 -2.227150e+00-1.269666e+01j  1.830223e+00-8.494314e+00j  \n",
       "22 -2.233387e+00+9.414941e+00j -1.807129e+00+1.074757e+01j  \n",
       "23 -2.233387e+00-9.414941e+00j -1.807129e+00-1.074757e+01j  \n",
       "24  1.381332e+00+7.100172e+00j -4.094805e+00+9.482176e+00j  \n",
       "25  1.381332e+00-7.100172e+00j -4.094805e+00-9.482176e+00j  \n",
       "26 -1.946046e+01+0.000000e+00j -1.883194e+01+0.000000e+00j  \n",
       "27 -1.670538e+01+0.000000e+00j -1.658529e+01+0.000000e+00j  \n",
       "28 -1.570627e+01+0.000000e+00j -1.525657e+01+0.000000e+00j  \n",
       "29 -1.374166e+01+0.000000e+00j -1.389188e+01+0.000000e+00j  \n",
       "30 -8.790636e+00+0.000000e+00j -8.637091e+00+0.000000e+00j  \n",
       "31 -8.331044e+00+0.000000e+00j -8.227269e+00+0.000000e+00j  \n",
       "32 -1.135757e+00+1.191575e+00j -1.191162e+00+1.258817e+00j  \n",
       "33 -1.135757e+00-1.191575e+00j -1.191162e+00-1.258817e+00j  \n",
       "34 -1.825353e-08+0.000000e+00j -5.004686e-07+0.000000e+00j  \n",
       "35 -6.041052e-01+5.131972e-01j -1.861383e-01+0.000000e+00j  \n",
       "36 -6.041052e-01-5.131972e-01j -6.210831e-01+6.956186e-01j  \n",
       "37 -1.890564e-01+0.000000e+00j -6.210831e-01-6.956186e-01j  \n",
       "38 -5.296216e-01+0.000000e+00j -8.270570e-01+0.000000e+00j  \n",
       "39 -1.049173e+00+0.000000e+00j -9.713591e-01+0.000000e+00j  \n",
       "40 -9.703435e-01+0.000000e+00j -1.039622e+00+0.000000e+00j  \n",
       "41 -1.004684e+00+1.637869e-02j -1.016630e+00+0.000000e+00j  \n",
       "42 -1.004684e+00-1.637869e-02j -9.986679e-01+1.402810e-03j  \n",
       "43 -9.931485e-01+0.000000e+00j -9.986679e-01-1.402810e-03j  \n",
       "44 -1.007060e+00+0.000000e+00j -1.004906e+00+0.000000e+00j  \n",
       "45 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j  \n",
       "46 -1.018025e+00+0.000000e+00j -1.018025e+00+0.000000e+00j  \n",
       "47 -1.000000e+00+0.000000e+00j -1.000000e+00+0.000000e+00j  \n",
       "48 -5.135788e+00+0.000000e+00j -5.135788e+00+0.000000e+00j  \n",
       "\n",
       "[49 rows x 19815 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The organized, lebeled data that is saved on the 'df_eigenvalue.pkl' file\n",
    "with open(\"_preproc_data/df_eigenvalues.pkl\", 'rb') as f:\n",
    "    df_eigenvalues = pickle.load(f)\n",
    "df_eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1edab7ec9c8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a random sample of 10 scenarios (10 random numbers from 1 to 19185)\n",
    "plot_scenarios = np.random.randint(1, df_eigenvalues.shape[1], 10)\n",
    "\n",
    "\n",
    "# Plot the eigenvalues\n",
    "fig, ax = plt.subplots(figsize = (10, 5))\n",
    "\n",
    "#get the real and imaginary part of each eigenvalue\n",
    "for sc in plot_scenarios:\n",
    "    real_part = np.real(df_eigenvalues[f\"Scenario {sc}\"][:]) \n",
    "    imag_part = np.imag(df_eigenvalues[f\"Scenario {sc}\"][:])\n",
    "    \n",
    "    # Plotting the eigenvalue\n",
    "    ax.scatter(real_part, imag_part, marker = 'x')\n",
    "\n",
    "# Formatting axes\n",
    "ax.set_title('Raw Eigenvalue Distribution', fontname = 'Times New Roman', fontsize = 20)\n",
    "ax.set_xlabel('Real Axis', fontname = 'Times New Roman', fontsize = 16)\n",
    "ax.set_ylabel('Imaginary Axis', fontname = 'Times New Roman', fontsize = 16)\n",
    "\n",
    "# More formatting stuff\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_fontname('Times New Roman')\n",
    "    tick.set_fontsize(14)\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontname('Times New Roman')\n",
    "    tick.set_fontsize(14)\n",
    "    \n",
    "# Draw stability boundary\n",
    "ax.axvline(x = 0, color = 'black', linestyle = '--', label = \"Stability boundary\")\n",
    "ax.legend(prop = {'family' : 'Times New Roman', 'size' : 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_data_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Loading Organized Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The organized, lebeled data that is saved on the 'df_eigenvalue.pkl' file\n",
    "with open(\"_preproc_data/df_eigenvalues.pkl\", 'rb') as f:\n",
    "    df_eigenvalues = pickle.load(f)\n",
    "#df_eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataset to panda frame\n",
    "#df = pd.DataFrame(eigs19branches)\n",
    "#print(df)\n",
    "#75% Dataset\n",
    "#eigs19branches=df.head(14861)\n",
    "#eigs19branches=eigs19branches.to_numpy()\n",
    "#df_eigenvalues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Normalizing Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numpy.vectorize object at 0x000001EDAB1AE708>\n"
     ]
    }
   ],
   "source": [
    "# Creating new Pandas Dataframe for normalized eigenvalues\n",
    "df_normalized_eigenvalues = pd.DataFrame([], columns = list(df_eigenvalues.columns.values))\n",
    "\n",
    "# Unit circle normalization, using lamda function\n",
    "# Function to normalize a single eigenvalue (scalar fcn)\n",
    "# This has to be conditioned: only the eigs outside the unit circle will be normalized\n",
    "norm_scalar = lambda eig : (eig / (np.abs(eig) + EPS) if np.abs(eig) >= 1 else eig)\n",
    "\n",
    "# Vectorizing the lambda expression (vector elementwise fcn)\n",
    "norm_vector = np.vectorize(norm_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the lamda expression on the eigenvalue dataframe\n",
    "for sc in list(df_eigenvalues.columns.values):\n",
    "    df_normalized_eigenvalues[sc] = norm_vector(df_eigenvalues[sc][:])\n",
    "\n",
    "# Saving the normalized eigenvalues\n",
    "with open('_preproc_data/df_normalized_eigenvalues.pkl', 'wb') as f:\n",
    "    pickle.dump(df_normalized_eigenvalues, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Exploratory Data Analysis (after normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting 10 random scenarios\n",
    "plot_scenarios = np.random.randint(1, df_normalized_eigenvalues.shape[1], 10)\n",
    "\n",
    "# Plotting the eigenvalues for the scenarios\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "\n",
    "for sc in plot_scenarios:\n",
    "    real_part = np.real(df_normalized_eigenvalues[\"Scenario {}\".format(sc)][:])\n",
    "    imag_part = np.imag(df_normalized_eigenvalues[\"Scenario {}\".format(sc)][:])\n",
    "    ax.scatter(real_part, imag_part, marker = 'x')\n",
    "\n",
    "#ax.set_title('Normalized Eigenvalue Distribution', fontname = 'Times New Roman', fontsize = 20)\n",
    "ax.set_xlabel('Real Axis', fontname = 'Times New Roman', fontsize = 16)\n",
    "ax.set_ylabel('Imaginary Axis', fontname = 'Times New Roman', fontsize = 16)\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_fontname('Times New Roman')\n",
    "    tick.set_fontsize(14)\n",
    "for tick in ax.get_yticklabels():\n",
    "    tick.set_fontname('Times New Roman')\n",
    "    tick.set_fontsize(14)\n",
    "    \n",
    "ax.axvline(x = 0, color = 'black', linestyle = '--', label = 'Stability boundary') # Drawing stability boundary\n",
    "ax.legend(prop = {'family' : 'Times New Roman', 'size' : 14})\n",
    "\n",
    "ax.axes.set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth data saved!\n",
      "\n",
      "normalized Eigenvalues (shape): 49 (eigs) x 19815 (scenarios)\n",
      "normalized Text labels (shape): 49 (eigs) x 19815 (scenarios)\n",
      "normalized Damping ratio (shape): 49 (eigs) x 19815 (scenarios)\n",
      "normalized Tag label (shape): 49 (eigs) x 19815 (scenarios)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hard-coded ground truth\n",
    "\n",
    "# use the df_normalized_eigenvalues.pkl for creating ground truth\n",
    "create_ground_truth(df_normalized_eigenvalues, os.getcwd(), 'normalized')\n",
    "# the create_ground_truth.py under 'utilis' stores the detailed function\n",
    "\n",
    "# Takes these data from the normalized eigen values\n",
    "# uses built in function to get the stability (stable, unstable, etc.) from damping ratio\n",
    "# store the actual results for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_label': array([['Real, stable', 'Real, stable', 'Real, stable', ...,\n",
       "         'Real, stable', 'Real, stable', 'Real, stable'],\n",
       "        ['Real, stable', 'Real, stable', 'Real, stable', ...,\n",
       "         'Real, stable', 'Real, stable', 'Real, stable'],\n",
       "        ['Real, stable', 'Real, stable', 'Real, stable', ...,\n",
       "         'Real, stable', 'Real, stable', 'Real, stable'],\n",
       "        ...,\n",
       "        ['Real, stable', 'Real, stable', 'Real, stable', ...,\n",
       "         'Real, stable', 'Real, stable', 'Real, stable'],\n",
       "        ['Real, stable', 'Real, stable', 'Real, stable', ...,\n",
       "         'Real, stable', 'Real, stable', 'Real, stable'],\n",
       "        ['Real, stable', 'Real, stable', 'Real, stable', ...,\n",
       "         'Real, stable', 'Real, stable', 'Real, stable']], dtype=object),\n",
       " 'damping_ratio': array([[1.1, 1.1, 1.1, ..., 1.1, 1.1, 1.1],\n",
       "        [1.1, 1.1, 1.1, ..., 1.1, 1.1, 1.1],\n",
       "        [1.1, 1.1, 1.1, ..., 1.1, 1.1, 1.1],\n",
       "        ...,\n",
       "        [1.1, 1.1, 1.1, ..., 1.1, 1.1, 1.1],\n",
       "        [1.1, 1.1, 1.1, ..., 1.1, 1.1, 1.1],\n",
       "        [1.1, 1.1, 1.1, ..., 1.1, 1.1, 1.1]]),\n",
       " 'tag_label': array([[5., 5., 5., ..., 5., 5., 5.],\n",
       "        [5., 5., 5., ..., 5., 5., 5.],\n",
       "        [5., 5., 5., ..., 5., 5., 5.],\n",
       "        ...,\n",
       "        [5., 5., 5., ..., 5., 5., 5.],\n",
       "        [5., 5., 5., ..., 6., 5., 5.],\n",
       "        [5., 5., 5., ..., 2., 5., 5.]])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the saved normalized ground truth data\n",
    "# Loading normalized ground truth data\n",
    "with open (\"_preproc_data/normalized_ground_truth_data.pkl\", 'rb') as f:\n",
    "    normalized_ground_truth_data = pickle.load(f)\n",
    "normalized_ground_truth_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_data_preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Importing Libraries and Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading normalized eigenvalues data\n",
    "with open (\"_preproc_data/df_normalized_eigenvalues.pkl\", 'rb') as f:\n",
    "    df_normalized_eigenvalues = pickle.load(f)\n",
    "\n",
    "# Loading normalized ground truth data\n",
    "with open (\"_preproc_data/normalized_ground_truth_data.pkl\", 'rb') as f:\n",
    "    normalized_ground_truth_data = pickle.load(f)\n",
    "    \n",
    "# Extract the labels and the damping ratio\n",
    "tag_labels = normalized_ground_truth_data['tag_label']\n",
    "damping_ratio = normalized_ground_truth_data['damping_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Separating the Eigenvalues into Real and Imaginary Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigs_flatten (shape) = (970935, 1)\n",
      "input_features (shape) = (970935, 2)\n",
      "input_labels (shape) = (970935, 1)\n"
     ]
    }
   ],
   "source": [
    "#Get total number of eigenvalue in each scenario\n",
    "n_eigs = df_normalized_eigenvalues.shape[0]\n",
    "\n",
    "#Get total number of scenarios\n",
    "n_scenarios = df_normalized_eigenvalues.shape[1]\n",
    "\n",
    "#Get normalized complex eigen value for each scenario converted to single vector\n",
    "eigs_flatten = np.reshape(np.transpose(df_normalized_eigenvalues.values),\n",
    "                         [n_eigs * n_scenarios, 1])\n",
    "\n",
    "print(f\"eigs_flatten (shape) = {eigs_flatten.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# INPUT\n",
    "# ------------------------------\n",
    "\n",
    "# Get the 'input features' and 'input labels'\n",
    "\n",
    "# Initialize input features matrix\n",
    "input_features = np.zeros(shape = (eigs_flatten.shape[0], 2))\n",
    "\n",
    "# Populate the input features matrix with real and imaginary part of the eigenvalues\n",
    "input_features[:, 0] = np.reshape(np.real(eigs_flatten), [eigs_flatten.shape[0], ])\n",
    "input_features[:, 1] = np.reshape(np.imag(eigs_flatten), [eigs_flatten.shape[0], ])\n",
    "print(f\"input_features (shape) = {input_features.shape}\")\n",
    "\n",
    "# Flattening tag labels (ground truth)\n",
    "input_labels = np.reshape(np.transpose(tag_labels), \n",
    "                          [tag_labels.shape[0] * tag_labels.shape[1], 1])\n",
    "print(f\"input_labels (shape) = {input_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Splitting data into Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data (shape) = (728201, 2)\n",
      "Train labels (shape) = (728201, 1)\n",
      "Test data (shape) = (242734, 2)\n",
      "Test labels (shape) = (242734, 1)\n"
     ]
    }
   ],
   "source": [
    "# INPUTS: input_features\n",
    "# LABELS (of inputs): input_labels\n",
    "\n",
    "# Extracting training and testing data using scikit-learn\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(input_features, \n",
    "                                                    input_labels, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "print(f\"Train data (shape) = {X_train.shape}\")\n",
    "print(f\"Train labels (shape) = {Y_train.shape}\")\n",
    "print(f\"Test data (shape) = {X_test.shape}\")\n",
    "print(f\"Test labels (shape) = {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Converting the Labels to One-Hot Encoding\n",
    "See the details here <https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelBinarizer()\n",
      "Train labels (one-hot encoding - shape): (728201, 6)\n",
      "Test labels (one-hot encoding - shape): (242734, 6)\n"
     ]
    }
   ],
   "source": [
    "# Converting labels to one-hot using scikit-learn\n",
    "# Label Binarizer is an SciKit Learn class that accepts Categorical data as input and returns an Numpy array\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# np.float64 is a fixed-sized float value (always 64bit)  \n",
    "# It means that each value in the numpy array would be a float of size 64\n",
    "T_train = np.float64(lb.fit_transform(Y_train))\n",
    "T_train = np.float64(lb.transform(Y_train))\n",
    "\n",
    "T_test = np.float64(lb.fit_transform(Y_test))\n",
    "T_test = np.float64(lb.transform(Y_test))\n",
    "\n",
    "print(f\"Train labels (one-hot encoding - shape): {T_train.shape}\")\n",
    "print(f\"Test labels (one-hot encoding - shape): {T_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Compensating Skewed Dataset\n",
    "See the details here <https://builtin.com/data-science/skewed-data>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0: 36843, 2.0: 25312, 3.0: 5686, 4.0: 207371, 5.0: 439513, 6.0: 13476}\n"
     ]
    }
   ],
   "source": [
    "# Taking the same amount of eigenvalues for each category\n",
    "unique, counts = np.unique(Y_train, return_counts = True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the indices for the elements with the same category in the training data\n",
    "numeigs_1 = np.where(Y_train == 1)\n",
    "numeigs_1 = np.array(numeigs_1)\n",
    "numeigs_1 = numeigs_1[0, :]\n",
    "\n",
    "numeigs_2 = np.where(Y_train == 2)\n",
    "numeigs_2 = np.array(numeigs_2)\n",
    "numeigs_2 = numeigs_2[0, :]\n",
    "\n",
    "numeigs_3 = np.where(Y_train == 3)\n",
    "numeigs_3 = np.array(numeigs_3)\n",
    "numeigs_3 = numeigs_3[0, :]\n",
    "\n",
    "numeigs_4 = np.where(Y_train == 4)\n",
    "numeigs_4 = np.array(numeigs_4)\n",
    "numeigs_4 = numeigs_4[0, :]\n",
    "\n",
    "numeigs_5 = np.where(Y_train == 5)\n",
    "numeigs_5 = np.array(numeigs_5)\n",
    "numeigs_5 = numeigs_5[0, :]\n",
    "\n",
    "numeigs_6 = np.where(Y_train == 6)\n",
    "numeigs_6 = np.array(numeigs_6)\n",
    "numeigs_6 = numeigs_6[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomizing indices\n",
    "numeigs_r_1 = np.random.choice(numeigs_1, np.amin(counts))\n",
    "numeigs_r_2 = np.random.choice(numeigs_2, np.amin(counts))\n",
    "numeigs_r_3 = np.random.choice(numeigs_3, np.amin(counts))\n",
    "numeigs_r_4 = np.random.choice(numeigs_4, np.amin(counts))\n",
    "numeigs_r_5 = np.random.choice(numeigs_5, np.amin(counts))\n",
    "numeigs_r_6 = np.random.choice(numeigs_6, np.amin(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Creating Reduced Training/Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_train_red (shape) = (34116, 2)\n",
      " Y_train_red (shape) = (34116, 1)\n",
      " T_train_red (shape) = (34116, 6)\n"
     ]
    }
   ],
   "source": [
    "# Only get the data associated with the radomized indices to reduce skewness\n",
    "X_train_red = np.concatenate((X_train[numeigs_r_1, ...],\n",
    "                             X_train[numeigs_r_2, ...], X_train[numeigs_r_3, ...], \n",
    "                             X_train[numeigs_r_4, ...], X_train[numeigs_r_5, ...], \n",
    "                             X_train[numeigs_r_6, ...]), axis = 0)\n",
    "print(f\" X_train_red (shape) = {X_train_red.shape}\")\n",
    "\n",
    "Y_train_red = np.concatenate((Y_train[numeigs_r_1, ...],\n",
    "                             Y_train[numeigs_r_2, ...], Y_train[numeigs_r_3, ...], \n",
    "                             Y_train[numeigs_r_4, ...], Y_train[numeigs_r_5, ...], \n",
    "                             Y_train[numeigs_r_6, ...]), axis = 0)\n",
    "print(f\" Y_train_red (shape) = {Y_train_red.shape}\")\n",
    "\n",
    "T_train_red = np.concatenate((T_train[numeigs_r_1, ...],\n",
    "                             T_train[numeigs_r_2, ...], T_train[numeigs_r_3, ...], \n",
    "                             T_train[numeigs_r_4, ...], T_train[numeigs_r_5, ...], \n",
    "                             T_train[numeigs_r_6, ...]), axis = 0)\n",
    "print(f\" T_train_red (shape) = {T_train_red.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized the reduced testing and training data\n",
    "normalized_testing_training_data_red = {'X_train' : X_train,\n",
    "                                        'Y_train' : Y_train,\n",
    "                                        'T_train' : T_train,\n",
    "                                       'X_train_red' : X_train_red,\n",
    "                                       'Y_train_red' : Y_train_red,\n",
    "                                       'T_train_red' : T_train_red,\n",
    "                                       'X_test' : X_test,\n",
    "                                       'Y_test' : Y_test,\n",
    "                                       'T_test' : T_test}\n",
    "\n",
    "with open('_preproc_data/normalized_testing_training_data_red_.pkl', 'wb') as f:\n",
    "    pickle.dump(normalized_testing_training_data_red, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4_neural_network_design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the normalized reduced training and testing data\n",
    "with open('_preproc_data/normalized_testing_training_data_red_.pkl', 'rb') as f:\n",
    "    normalized_testing_training_data_red = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train', 'Y_train', 'T_train', 'X_train_red', 'Y_train_red', 'T_train_red', 'X_test', 'Y_test', 'T_test'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the keys\n",
    "normalized_testing_training_data_red.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key-wise variable creation\n",
    "X_train = normalized_testing_training_data_red['X_train']\n",
    "X_train_red = normalized_testing_training_data_red['X_train_red']\n",
    "Y_train = normalized_testing_training_data_red['Y_train']\n",
    "Y_train_red = normalized_testing_training_data_red['Y_train_red']\n",
    "T_train = normalized_testing_training_data_red['T_train']\n",
    "T_train_red = normalized_testing_training_data_red['T_train_red']\n",
    "\n",
    "X_test = normalized_testing_training_data_red['X_test']\n",
    "Y_test = normalized_testing_training_data_red['Y_test']\n",
    "T_test = normalized_testing_training_data_red['T_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the NN Model - 3 layer (1 hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Understand the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call keras framework\n",
    "from tensorflow import keras\n",
    "# initiate a sequentia keras neural model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# First layer (input layer)\n",
    "model.add(keras.layers.Dense(100,  # neuron size in this layer\n",
    "                             activation = \"selu\",  # activation function\n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "model.add(keras.layers.Dropout(rate = 0.25)) # Dropout layer (rate = 25%), dropout reduces overfitting\n",
    "\n",
    "# Second layer\n",
    "model.add(keras.layers.Dense(100, # neuron size in this layer\n",
    "                             activation = \"selu\", # activation function\n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "\n",
    "# Third layer (output layer)\n",
    "model.add(keras.layers.Dense(6, activation = \"softmax\"))\n",
    "\n",
    "model.build(input_shape = [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               300       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,006\n",
      "Trainable params: 11,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8661\n",
      "Epoch 1: loss improved from inf to 0.42115, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.4211 - accuracy: 0.8662\n",
      "Epoch 2/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.9125\n",
      "Epoch 2: loss improved from 0.42115 to 0.27614, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.2761 - accuracy: 0.9126\n",
      "Epoch 3/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9249\n",
      "Epoch 3: loss improved from 0.27614 to 0.23632, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.2363 - accuracy: 0.9249\n",
      "Epoch 4/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.2167 - accuracy: 0.9291\n",
      "Epoch 4: loss improved from 0.23632 to 0.21701, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.2170 - accuracy: 0.9290\n",
      "Epoch 5/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.2050 - accuracy: 0.9308\n",
      "Epoch 5: loss improved from 0.21701 to 0.20517, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.2052 - accuracy: 0.9307\n",
      "Epoch 6/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9333\n",
      "Epoch 6: loss improved from 0.20517 to 0.19770, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9332\n",
      "Epoch 7/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9353\n",
      "Epoch 7: loss improved from 0.19770 to 0.19136, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9353\n",
      "Epoch 8/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1883 - accuracy: 0.9356\n",
      "Epoch 8: loss improved from 0.19136 to 0.18759, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9359\n",
      "Epoch 9/200\n",
      "1044/1067 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9372\n",
      "Epoch 9: loss improved from 0.18759 to 0.18367, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9373\n",
      "Epoch 10/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9378\n",
      "Epoch 10: loss improved from 0.18367 to 0.18109, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.1811 - accuracy: 0.9379\n",
      "Epoch 11/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1784 - accuracy: 0.9390\n",
      "Epoch 11: loss improved from 0.18109 to 0.17858, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1786 - accuracy: 0.9389\n",
      "Epoch 12/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9391\n",
      "Epoch 12: loss improved from 0.17858 to 0.17770, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9393\n",
      "Epoch 13/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.9395\n",
      "Epoch 13: loss improved from 0.17770 to 0.17669, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.1767 - accuracy: 0.9395\n",
      "Epoch 14/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.9392\n",
      "Epoch 14: loss improved from 0.17669 to 0.17647, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9393\n",
      "Epoch 15/200\n",
      "1040/1067 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.9401\n",
      "Epoch 15: loss improved from 0.17647 to 0.17430, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1743 - accuracy: 0.9400\n",
      "Epoch 16/200\n",
      "1036/1067 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.9399\n",
      "Epoch 16: loss did not improve from 0.17430\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1757 - accuracy: 0.9399\n",
      "Epoch 17/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9406\n",
      "Epoch 17: loss improved from 0.17430 to 0.17202, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1720 - accuracy: 0.9405\n",
      "Epoch 18/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9408\n",
      "Epoch 18: loss did not improve from 0.17202\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1727 - accuracy: 0.9408\n",
      "Epoch 19/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9409\n",
      "Epoch 19: loss did not improve from 0.17202\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1721 - accuracy: 0.9407\n",
      "Epoch 20/200\n",
      "1045/1067 [============================>.] - ETA: 0s - loss: 0.1705 - accuracy: 0.9410\n",
      "Epoch 20: loss improved from 0.17202 to 0.16991, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1699 - accuracy: 0.9414\n",
      "Epoch 21/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.9414\n",
      "Epoch 21: loss did not improve from 0.16991\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1703 - accuracy: 0.9413\n",
      "Epoch 22/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.9411\n",
      "Epoch 22: loss did not improve from 0.16991\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1713 - accuracy: 0.9411\n",
      "Epoch 23/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9409\n",
      "Epoch 23: loss did not improve from 0.16991\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1724 - accuracy: 0.9407\n",
      "Epoch 24/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9414\n",
      "Epoch 24: loss did not improve from 0.16991\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1700 - accuracy: 0.9414\n",
      "Epoch 25/200\n",
      "1037/1067 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9413\n",
      "Epoch 25: loss did not improve from 0.16991\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1701 - accuracy: 0.9412\n",
      "Epoch 26/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9424\n",
      "Epoch 26: loss improved from 0.16991 to 0.16853, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1685 - accuracy: 0.9421\n",
      "Epoch 27/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9419\n",
      "Epoch 27: loss did not improve from 0.16853\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1694 - accuracy: 0.9418\n",
      "Epoch 28/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1694 - accuracy: 0.9416\n",
      "Epoch 28: loss did not improve from 0.16853\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1695 - accuracy: 0.9416\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9417\n",
      "Epoch 29: loss improved from 0.16853 to 0.16846, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1685 - accuracy: 0.9419\n",
      "Epoch 30/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9417\n",
      "Epoch 30: loss did not improve from 0.16846\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1694 - accuracy: 0.9418\n",
      "Epoch 31/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1684 - accuracy: 0.9416\n",
      "Epoch 31: loss did not improve from 0.16846\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1685 - accuracy: 0.9416\n",
      "Epoch 32/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1687 - accuracy: 0.9420\n",
      "Epoch 32: loss did not improve from 0.16846\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1686 - accuracy: 0.9421\n",
      "Epoch 33/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9422\n",
      "Epoch 33: loss improved from 0.16846 to 0.16684, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1668 - accuracy: 0.9422\n",
      "Epoch 34/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1665 - accuracy: 0.9428\n",
      "Epoch 34: loss did not improve from 0.16684\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1670 - accuracy: 0.9425\n",
      "Epoch 35/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9424\n",
      "Epoch 35: loss did not improve from 0.16684\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1671 - accuracy: 0.9423\n",
      "Epoch 36/200\n",
      "1038/1067 [============================>.] - ETA: 0s - loss: 0.1673 - accuracy: 0.9422\n",
      "Epoch 36: loss did not improve from 0.16684\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1669 - accuracy: 0.9425\n",
      "Epoch 37/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1674 - accuracy: 0.9420\n",
      "Epoch 37: loss did not improve from 0.16684\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1672 - accuracy: 0.9421\n",
      "Epoch 38/200\n",
      "1045/1067 [============================>.] - ETA: 0s - loss: 0.1663 - accuracy: 0.9423\n",
      "Epoch 38: loss improved from 0.16684 to 0.16640, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1664 - accuracy: 0.9423\n",
      "Epoch 39/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9424\n",
      "Epoch 39: loss improved from 0.16640 to 0.16598, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1660 - accuracy: 0.9423\n",
      "Epoch 40/200\n",
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9426\n",
      "Epoch 40: loss improved from 0.16598 to 0.16456, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1646 - accuracy: 0.9427\n",
      "Epoch 41/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.9426\n",
      "Epoch 41: loss did not improve from 0.16456\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1656 - accuracy: 0.9424\n",
      "Epoch 42/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1642 - accuracy: 0.9423\n",
      "Epoch 42: loss did not improve from 0.16456\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1646 - accuracy: 0.9421\n",
      "Epoch 43/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1634 - accuracy: 0.9430\n",
      "Epoch 43: loss improved from 0.16456 to 0.16357, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1636 - accuracy: 0.9430\n",
      "Epoch 44/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1631 - accuracy: 0.9425\n",
      "Epoch 44: loss improved from 0.16357 to 0.16282, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1628 - accuracy: 0.9426\n",
      "Epoch 45/200\n",
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1611 - accuracy: 0.9430\n",
      "Epoch 45: loss improved from 0.16282 to 0.16115, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1611 - accuracy: 0.9430\n",
      "Epoch 46/200\n",
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1583 - accuracy: 0.9437\n",
      "Epoch 46: loss improved from 0.16115 to 0.15856, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1586 - accuracy: 0.9436\n",
      "Epoch 47/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1584 - accuracy: 0.9435\n",
      "Epoch 47: loss did not improve from 0.15856\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1588 - accuracy: 0.9433\n",
      "Epoch 48/200\n",
      "1043/1067 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9442\n",
      "Epoch 48: loss improved from 0.15856 to 0.15715, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1572 - accuracy: 0.9444\n",
      "Epoch 49/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1551 - accuracy: 0.9446\n",
      "Epoch 49: loss improved from 0.15715 to 0.15464, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1546 - accuracy: 0.9448\n",
      "Epoch 50/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9451\n",
      "Epoch 50: loss improved from 0.15464 to 0.15382, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1538 - accuracy: 0.9451\n",
      "Epoch 51/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1519 - accuracy: 0.9458\n",
      "Epoch 51: loss improved from 0.15382 to 0.15154, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1515 - accuracy: 0.9461\n",
      "Epoch 52/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9455\n",
      "Epoch 52: loss did not improve from 0.15154\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1543 - accuracy: 0.9456\n",
      "Epoch 53/200\n",
      "1043/1067 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.9473\n",
      "Epoch 53: loss improved from 0.15154 to 0.14677, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1468 - accuracy: 0.9474\n",
      "Epoch 54/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9471\n",
      "Epoch 54: loss did not improve from 0.14677\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1502 - accuracy: 0.9472\n",
      "Epoch 55/200\n",
      "1035/1067 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9479\n",
      "Epoch 55: loss did not improve from 0.14677\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1473 - accuracy: 0.9479\n",
      "Epoch 56/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.9489\n",
      "Epoch 56: loss improved from 0.14677 to 0.14384, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1438 - accuracy: 0.9488\n",
      "Epoch 57/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.9490\n",
      "Epoch 57: loss improved from 0.14384 to 0.14171, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1417 - accuracy: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1402 - accuracy: 0.9503\n",
      "Epoch 58: loss improved from 0.14171 to 0.14015, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1402 - accuracy: 0.9503\n",
      "Epoch 59/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1403 - accuracy: 0.9497\n",
      "Epoch 59: loss did not improve from 0.14015\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1404 - accuracy: 0.9498\n",
      "Epoch 60/200\n",
      "1041/1067 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.9505\n",
      "Epoch 60: loss improved from 0.14015 to 0.13882, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1388 - accuracy: 0.9506\n",
      "Epoch 61/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1379 - accuracy: 0.9505\n",
      "Epoch 61: loss improved from 0.13882 to 0.13787, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1379 - accuracy: 0.9505\n",
      "Epoch 62/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9512\n",
      "Epoch 62: loss improved from 0.13787 to 0.13718, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1372 - accuracy: 0.9512\n",
      "Epoch 63/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9515\n",
      "Epoch 63: loss improved from 0.13718 to 0.13544, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1354 - accuracy: 0.9515\n",
      "Epoch 64/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9518\n",
      "Epoch 64: loss did not improve from 0.13544\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1359 - accuracy: 0.9517\n",
      "Epoch 65/200\n",
      "1043/1067 [============================>.] - ETA: 0s - loss: 0.1347 - accuracy: 0.9516\n",
      "Epoch 65: loss improved from 0.13544 to 0.13427, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1343 - accuracy: 0.9517\n",
      "Epoch 66/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.9521\n",
      "Epoch 66: loss did not improve from 0.13427\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1349 - accuracy: 0.9523\n",
      "Epoch 67/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1323 - accuracy: 0.9527\n",
      "Epoch 67: loss improved from 0.13427 to 0.13206, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1321 - accuracy: 0.9528\n",
      "Epoch 68/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9532\n",
      "Epoch 68: loss improved from 0.13206 to 0.13060, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1306 - accuracy: 0.9531\n",
      "Epoch 69/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1320 - accuracy: 0.9526\n",
      "Epoch 69: loss did not improve from 0.13060\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1319 - accuracy: 0.9526\n",
      "Epoch 70/200\n",
      "1039/1067 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9532\n",
      "Epoch 70: loss improved from 0.13060 to 0.13003, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1300 - accuracy: 0.9532\n",
      "Epoch 71/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9528\n",
      "Epoch 71: loss did not improve from 0.13003\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1329 - accuracy: 0.9528\n",
      "Epoch 72/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9532\n",
      "Epoch 72: loss did not improve from 0.13003\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1309 - accuracy: 0.9531\n",
      "Epoch 73/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9542\n",
      "Epoch 73: loss improved from 0.13003 to 0.12732, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1273 - accuracy: 0.9542\n",
      "Epoch 74/200\n",
      "1036/1067 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9536\n",
      "Epoch 74: loss did not improve from 0.12732\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1299 - accuracy: 0.9535\n",
      "Epoch 75/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9534\n",
      "Epoch 75: loss did not improve from 0.12732\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1297 - accuracy: 0.9534\n",
      "Epoch 76/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9542\n",
      "Epoch 76: loss did not improve from 0.12732\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1274 - accuracy: 0.9542\n",
      "Epoch 77/200\n",
      "1044/1067 [============================>.] - ETA: 0s - loss: 0.1329 - accuracy: 0.9535\n",
      "Epoch 77: loss did not improve from 0.12732\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1327 - accuracy: 0.9537\n",
      "Epoch 78/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1299 - accuracy: 0.9534\n",
      "Epoch 78: loss did not improve from 0.12732\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1298 - accuracy: 0.9534\n",
      "Epoch 79/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9541\n",
      "Epoch 79: loss did not improve from 0.12732\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1276 - accuracy: 0.9542\n",
      "Epoch 80/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9545\n",
      "Epoch 80: loss improved from 0.12732 to 0.12606, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1261 - accuracy: 0.9545\n",
      "Epoch 81/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9546\n",
      "Epoch 81: loss improved from 0.12606 to 0.12561, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1256 - accuracy: 0.9548\n",
      "Epoch 82/200\n",
      "1039/1067 [============================>.] - ETA: 0s - loss: 0.1272 - accuracy: 0.9548\n",
      "Epoch 82: loss did not improve from 0.12561\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1274 - accuracy: 0.9548\n",
      "Epoch 83/200\n",
      "1041/1067 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9544\n",
      "Epoch 83: loss did not improve from 0.12561\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1285 - accuracy: 0.9543\n",
      "Epoch 84/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9552\n",
      "Epoch 84: loss did not improve from 0.12561\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1261 - accuracy: 0.9549\n",
      "Epoch 85/200\n",
      "1037/1067 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9547\n",
      "Epoch 85: loss did not improve from 0.12561\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1286 - accuracy: 0.9548\n",
      "Epoch 86/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9553\n",
      "Epoch 86: loss improved from 0.12561 to 0.12488, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1249 - accuracy: 0.9552\n",
      "Epoch 87/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9543\n",
      "Epoch 87: loss did not improve from 0.12488\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1296 - accuracy: 0.9544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9550\n",
      "Epoch 88: loss improved from 0.12488 to 0.12454, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1245 - accuracy: 0.9550\n",
      "Epoch 89/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9549\n",
      "Epoch 89: loss did not improve from 0.12454\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1250 - accuracy: 0.9550\n",
      "Epoch 90/200\n",
      "1043/1067 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9555\n",
      "Epoch 90: loss improved from 0.12454 to 0.12432, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1243 - accuracy: 0.9555\n",
      "Epoch 91/200\n",
      "1040/1067 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9554\n",
      "Epoch 91: loss did not improve from 0.12432\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1261 - accuracy: 0.9554\n",
      "Epoch 92/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9544\n",
      "Epoch 92: loss did not improve from 0.12432\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1289 - accuracy: 0.9544\n",
      "Epoch 93/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9554\n",
      "Epoch 93: loss improved from 0.12432 to 0.12310, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1231 - accuracy: 0.9553\n",
      "Epoch 94/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9560\n",
      "Epoch 94: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1235 - accuracy: 0.9560\n",
      "Epoch 95/200\n",
      "1045/1067 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9553\n",
      "Epoch 95: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1248 - accuracy: 0.9552\n",
      "Epoch 96/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9555\n",
      "Epoch 96: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1240 - accuracy: 0.9554\n",
      "Epoch 97/200\n",
      "1042/1067 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9558\n",
      "Epoch 97: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1254 - accuracy: 0.9556\n",
      "Epoch 98/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9551\n",
      "Epoch 98: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1256 - accuracy: 0.9551\n",
      "Epoch 99/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9560\n",
      "Epoch 99: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1244 - accuracy: 0.9556\n",
      "Epoch 100/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9559\n",
      "Epoch 100: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1257 - accuracy: 0.9560\n",
      "Epoch 101/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9551\n",
      "Epoch 101: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1260 - accuracy: 0.9551\n",
      "Epoch 102/200\n",
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9557\n",
      "Epoch 102: loss did not improve from 0.12310\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1237 - accuracy: 0.9556\n",
      "Epoch 103/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9559\n",
      "Epoch 103: loss improved from 0.12310 to 0.12214, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1221 - accuracy: 0.9559\n",
      "Epoch 104/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9554\n",
      "Epoch 104: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1252 - accuracy: 0.9554\n",
      "Epoch 105/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9551\n",
      "Epoch 105: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1252 - accuracy: 0.9552\n",
      "Epoch 106/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9553\n",
      "Epoch 106: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1237 - accuracy: 0.9555\n",
      "Epoch 107/200\n",
      "1038/1067 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9560\n",
      "Epoch 107: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1241 - accuracy: 0.9558\n",
      "Epoch 108/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9560\n",
      "Epoch 108: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1222 - accuracy: 0.9560\n",
      "Epoch 109/200\n",
      "1043/1067 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9557\n",
      "Epoch 109: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1236 - accuracy: 0.9556\n",
      "Epoch 110/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9555\n",
      "Epoch 110: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1245 - accuracy: 0.9557\n",
      "Epoch 111/200\n",
      "1042/1067 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9546\n",
      "Epoch 111: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1263 - accuracy: 0.9548\n",
      "Epoch 112/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9562\n",
      "Epoch 112: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1238 - accuracy: 0.9561\n",
      "Epoch 113/200\n",
      "1040/1067 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9556\n",
      "Epoch 113: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1225 - accuracy: 0.9555\n",
      "Epoch 114/200\n",
      "1039/1067 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9558\n",
      "Epoch 114: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1251 - accuracy: 0.9559\n",
      "Epoch 115/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9559\n",
      "Epoch 115: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1248 - accuracy: 0.9557\n",
      "Epoch 116/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9561\n",
      "Epoch 116: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1231 - accuracy: 0.9560\n",
      "Epoch 117/200\n",
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9553\n",
      "Epoch 117: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1268 - accuracy: 0.9554\n",
      "Epoch 118/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9554\n",
      "Epoch 118: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1244 - accuracy: 0.9554\n",
      "Epoch 119/200\n",
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9563\n",
      "Epoch 119: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1232 - accuracy: 0.9561\n",
      "Epoch 120/200\n",
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9557\n",
      "Epoch 120: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1225 - accuracy: 0.9559\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9560\n",
      "Epoch 121: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1238 - accuracy: 0.9560\n",
      "Epoch 122/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9560\n",
      "Epoch 122: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1242 - accuracy: 0.9559\n",
      "Epoch 123/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9558\n",
      "Epoch 123: loss did not improve from 0.12214\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1232 - accuracy: 0.9558\n",
      "Epoch 124/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9561\n",
      "Epoch 124: loss improved from 0.12214 to 0.12053, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1205 - accuracy: 0.9561\n",
      "Epoch 125/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9557\n",
      "Epoch 125: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1238 - accuracy: 0.9556\n",
      "Epoch 126/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9560\n",
      "Epoch 126: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1226 - accuracy: 0.9559\n",
      "Epoch 127/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9557\n",
      "Epoch 127: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1219 - accuracy: 0.9557\n",
      "Epoch 128/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9557\n",
      "Epoch 128: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1230 - accuracy: 0.9560\n",
      "Epoch 129/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9558\n",
      "Epoch 129: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1246 - accuracy: 0.9558\n",
      "Epoch 130/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9564\n",
      "Epoch 130: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1227 - accuracy: 0.9564\n",
      "Epoch 131/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9563\n",
      "Epoch 131: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1218 - accuracy: 0.9562\n",
      "Epoch 132/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9566\n",
      "Epoch 132: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1234 - accuracy: 0.9566\n",
      "Epoch 133/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9557\n",
      "Epoch 133: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1253 - accuracy: 0.9557\n",
      "Epoch 134/200\n",
      "1044/1067 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9566\n",
      "Epoch 134: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1216 - accuracy: 0.9564\n",
      "Epoch 135/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9571\n",
      "Epoch 135: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1213 - accuracy: 0.9568\n",
      "Epoch 136/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9564\n",
      "Epoch 136: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1213 - accuracy: 0.9564\n",
      "Epoch 137/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9563\n",
      "Epoch 137: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1224 - accuracy: 0.9563\n",
      "Epoch 138/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9561\n",
      "Epoch 138: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1224 - accuracy: 0.9562\n",
      "Epoch 139/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9563\n",
      "Epoch 139: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1209 - accuracy: 0.9564\n",
      "Epoch 140/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9561\n",
      "Epoch 140: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1244 - accuracy: 0.9560\n",
      "Epoch 141/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9562\n",
      "Epoch 141: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1216 - accuracy: 0.9562\n",
      "Epoch 142/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9564\n",
      "Epoch 142: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1220 - accuracy: 0.9563\n",
      "Epoch 143/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9559\n",
      "Epoch 143: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1227 - accuracy: 0.9561\n",
      "Epoch 144/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9560\n",
      "Epoch 144: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1227 - accuracy: 0.9560\n",
      "Epoch 145/200\n",
      "1047/1067 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9554\n",
      "Epoch 145: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1251 - accuracy: 0.9556\n",
      "Epoch 146/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9556\n",
      "Epoch 146: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1234 - accuracy: 0.9556\n",
      "Epoch 147/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9558\n",
      "Epoch 147: loss did not improve from 0.12053\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1229 - accuracy: 0.9559\n",
      "Epoch 148/200\n",
      "1039/1067 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9564\n",
      "Epoch 148: loss improved from 0.12053 to 0.11908, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1191 - accuracy: 0.9568\n",
      "Epoch 149/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9562\n",
      "Epoch 149: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1233 - accuracy: 0.9562\n",
      "Epoch 150/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9561\n",
      "Epoch 150: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1232 - accuracy: 0.9561\n",
      "Epoch 151/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9562\n",
      "Epoch 151: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1223 - accuracy: 0.9563\n",
      "Epoch 152/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9566\n",
      "Epoch 152: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1231 - accuracy: 0.9565\n",
      "Epoch 153/200\n",
      "1044/1067 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9561\n",
      "Epoch 153: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1232 - accuracy: 0.9561\n",
      "Epoch 154/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9568\n",
      "Epoch 154: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1216 - accuracy: 0.9566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "1045/1067 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9565\n",
      "Epoch 155: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1202 - accuracy: 0.9564\n",
      "Epoch 156/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9564\n",
      "Epoch 156: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1218 - accuracy: 0.9564\n",
      "Epoch 157/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9566\n",
      "Epoch 157: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1213 - accuracy: 0.9565\n",
      "Epoch 158/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9564\n",
      "Epoch 158: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1213 - accuracy: 0.9564\n",
      "Epoch 159/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9564\n",
      "Epoch 159: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1225 - accuracy: 0.9565\n",
      "Epoch 160/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9570\n",
      "Epoch 160: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1206 - accuracy: 0.9569\n",
      "Epoch 161/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9564\n",
      "Epoch 161: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1214 - accuracy: 0.9565\n",
      "Epoch 162/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9560\n",
      "Epoch 162: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1241 - accuracy: 0.9559\n",
      "Epoch 163/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9573\n",
      "Epoch 163: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1206 - accuracy: 0.9572\n",
      "Epoch 164/200\n",
      "1044/1067 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9558\n",
      "Epoch 164: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1245 - accuracy: 0.9559\n",
      "Epoch 165/200\n",
      "1043/1067 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9569\n",
      "Epoch 165: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1208 - accuracy: 0.9566\n",
      "Epoch 166/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9569\n",
      "Epoch 166: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1206 - accuracy: 0.9569\n",
      "Epoch 167/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9563\n",
      "Epoch 167: loss did not improve from 0.11908\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.1210 - accuracy: 0.9564\n",
      "Epoch 168/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9570\n",
      "Epoch 168: loss improved from 0.11908 to 0.11896, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1190 - accuracy: 0.9570\n",
      "Epoch 169/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9567\n",
      "Epoch 169: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1229 - accuracy: 0.9567\n",
      "Epoch 170/200\n",
      "1041/1067 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9558\n",
      "Epoch 170: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1227 - accuracy: 0.9561\n",
      "Epoch 171/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9571\n",
      "Epoch 171: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1190 - accuracy: 0.9571\n",
      "Epoch 172/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9557\n",
      "Epoch 172: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1224 - accuracy: 0.9561\n",
      "Epoch 173/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9565\n",
      "Epoch 173: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1229 - accuracy: 0.9565\n",
      "Epoch 174/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9566\n",
      "Epoch 174: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.1226 - accuracy: 0.9564\n",
      "Epoch 175/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9561\n",
      "Epoch 175: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1242 - accuracy: 0.9561\n",
      "Epoch 176/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9566\n",
      "Epoch 176: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1207 - accuracy: 0.9566\n",
      "Epoch 177/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9563\n",
      "Epoch 177: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1236 - accuracy: 0.9563\n",
      "Epoch 178/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9573\n",
      "Epoch 178: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1192 - accuracy: 0.9573\n",
      "Epoch 179/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9565\n",
      "Epoch 179: loss did not improve from 0.11896\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1217 - accuracy: 0.9565\n",
      "Epoch 180/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9569\n",
      "Epoch 180: loss improved from 0.11896 to 0.11894, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1189 - accuracy: 0.9570\n",
      "Epoch 181/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9570\n",
      "Epoch 181: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1200 - accuracy: 0.9570\n",
      "Epoch 182/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9566\n",
      "Epoch 182: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1201 - accuracy: 0.9566\n",
      "Epoch 183/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9570\n",
      "Epoch 183: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1211 - accuracy: 0.9569\n",
      "Epoch 184/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9565\n",
      "Epoch 184: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1236 - accuracy: 0.9566\n",
      "Epoch 185/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9559\n",
      "Epoch 185: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1209 - accuracy: 0.9560\n",
      "Epoch 186/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9571\n",
      "Epoch 186: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1193 - accuracy: 0.9571\n",
      "Epoch 187/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9569\n",
      "Epoch 187: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.1205 - accuracy: 0.9569\n",
      "Epoch 188/200\n",
      "1037/1067 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9568\n",
      "Epoch 188: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1203 - accuracy: 0.9566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9562\n",
      "Epoch 189: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1232 - accuracy: 0.9562\n",
      "Epoch 190/200\n",
      "1046/1067 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9567\n",
      "Epoch 190: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1217 - accuracy: 0.9565\n",
      "Epoch 191/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9566\n",
      "Epoch 191: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1210 - accuracy: 0.9566\n",
      "Epoch 192/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9571\n",
      "Epoch 192: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.1200 - accuracy: 0.9571\n",
      "Epoch 193/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9567\n",
      "Epoch 193: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1223 - accuracy: 0.9567\n",
      "Epoch 194/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9566\n",
      "Epoch 194: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1211 - accuracy: 0.9567\n",
      "Epoch 195/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9572\n",
      "Epoch 195: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.1190 - accuracy: 0.9573\n",
      "Epoch 196/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9566\n",
      "Epoch 196: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1221 - accuracy: 0.9567\n",
      "Epoch 197/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9565\n",
      "Epoch 197: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1219 - accuracy: 0.9566\n",
      "Epoch 198/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9568\n",
      "Epoch 198: loss did not improve from 0.11894\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1206 - accuracy: 0.9569\n",
      "Epoch 199/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9568\n",
      "Epoch 199: loss improved from 0.11894 to 0.11822, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.1182 - accuracy: 0.9568\n",
      "Epoch 200/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9571\n",
      "Epoch 200: loss did not improve from 0.11822\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.1187 - accuracy: 0.9571\n"
     ]
    }
   ],
   "source": [
    "filepath = os.path.abspath(os.path.join(os.getcwd(), '_nn_model', 'eigs_classification.hdf5'))\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(filepath = filepath, \n",
    "                                             monitor = 'loss', \n",
    "                                             mode = 'min', save_best_only = True, verbose = 1)]\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", \n",
    "              optimizer = \"Adam\", \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train_red, T_train_red, epochs = 200, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Prediction with the NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7586/7586 [==============================] - 12s 2ms/step - loss: 0.0470 - accuracy: 0.9840\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "\n",
    "scores = model.evaluate(X_test, keras.utils.to_categorical(Y_test - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of the input to belong to each of the classes\n",
    "Y_logits = model.predict(X_test) \n",
    "Y_pred = np.argmax(Y_logits, axis = 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 5, 5, 1], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_plotting(X_test, Y_test, Y_pred, model_name = \"NN - 1 Hidden Layer\", fontname = \"Times New Roman\")\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.983957747987509\n",
      "Precision: 0.9025883708200393\n",
      "Recall: 0.9578566456447763\n",
      "F1-score: 0.9182176716639484\n",
      "7586/7586 [==============================] - 11s 1ms/step - loss: 0.0470 - accuracy: 0.9840\n",
      "Execution time:  10.80734372138977 secs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print('Accuracy:',accuracy_score(Y_test, Y_pred))\n",
    "print('Precision:',precision_score(Y_test, Y_pred, average = 'macro'))\n",
    "print('Recall:',recall_score(Y_test, Y_pred, average = 'macro'))\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score:',f1_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "import time    # this is an inbuilt library in Python\n",
    "start_time = time.time()\n",
    "model.evaluate(X_test, keras.utils.to_categorical(Y_test - 1))\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time,\"secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the NN Model - 4 layer (2 hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# First layer\n",
    "model.add(keras.layers.Dense(100, \n",
    "                             activation = \"selu\", \n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "model.add(keras.layers.Dropout(rate = 0.25)) # Dropout layer (rate = 0.25)\n",
    "\n",
    "# Second layer\n",
    "model.add(keras.layers.Dense(100, \n",
    "                             activation = \"selu\", \n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "model.add(keras.layers.Dropout(rate = 0.25)) # Dropout layer (rate = 0.25)\n",
    "\n",
    "# Third layer\n",
    "model.add(keras.layers.Dense(100, \n",
    "                             activation = \"selu\", \n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "# Fourth layer (output layer)\n",
    "model.add(keras.layers.Dense(6, activation = \"softmax\"))\n",
    "\n",
    "model.build(input_shape = [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 100)               300       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 606       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,106\n",
      "Trainable params: 21,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8560\n",
      "Epoch 1: loss improved from inf to 0.42054, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.4205 - accuracy: 0.8565\n",
      "Epoch 2/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.2673 - accuracy: 0.9100\n",
      "Epoch 2: loss improved from 0.42054 to 0.26763, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.2676 - accuracy: 0.9099\n",
      "Epoch 3/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.2327 - accuracy: 0.9203\n",
      "Epoch 3: loss improved from 0.26763 to 0.23288, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 2ms/step - loss: 0.2329 - accuracy: 0.9204\n",
      "Epoch 4/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9237\n",
      "Epoch 4: loss improved from 0.23288 to 0.22290, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.2229 - accuracy: 0.9237\n",
      "Epoch 5/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.2098 - accuracy: 0.9293\n",
      "Epoch 5: loss improved from 0.22290 to 0.21027, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.2103 - accuracy: 0.9291\n",
      "Epoch 6/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.9283\n",
      "Epoch 6: loss improved from 0.21027 to 0.20814, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 2s 2ms/step - loss: 0.2081 - accuracy: 0.9283\n",
      "Epoch 7/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.2010 - accuracy: 0.9315\n",
      "Epoch 7: loss improved from 0.20814 to 0.20071, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.2007 - accuracy: 0.9317\n",
      "Epoch 8/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.2005 - accuracy: 0.9311\n",
      "Epoch 8: loss improved from 0.20071 to 0.20020, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.2002 - accuracy: 0.9313\n",
      "Epoch 9/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9326\n",
      "Epoch 9: loss improved from 0.20020 to 0.19626, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1963 - accuracy: 0.9326\n",
      "Epoch 10/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9340\n",
      "Epoch 10: loss improved from 0.19626 to 0.19357, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1936 - accuracy: 0.9342\n",
      "Epoch 11/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1894 - accuracy: 0.9353\n",
      "Epoch 11: loss improved from 0.19357 to 0.18940, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1894 - accuracy: 0.9353\n",
      "Epoch 12/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1894 - accuracy: 0.9353\n",
      "Epoch 12: loss improved from 0.18940 to 0.18933, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1893 - accuracy: 0.9353\n",
      "Epoch 13/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1861 - accuracy: 0.9359\n",
      "Epoch 13: loss improved from 0.18933 to 0.18619, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1862 - accuracy: 0.9359\n",
      "Epoch 14/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1871 - accuracy: 0.9363\n",
      "Epoch 14: loss did not improve from 0.18619\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1871 - accuracy: 0.9363\n",
      "Epoch 15/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9367\n",
      "Epoch 15: loss improved from 0.18619 to 0.18471, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1847 - accuracy: 0.9367\n",
      "Epoch 16/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9374\n",
      "Epoch 16: loss improved from 0.18471 to 0.18228, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1823 - accuracy: 0.9374\n",
      "Epoch 17/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1835 - accuracy: 0.9374\n",
      "Epoch 17: loss did not improve from 0.18228\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1836 - accuracy: 0.9373\n",
      "Epoch 18/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1804 - accuracy: 0.9381\n",
      "Epoch 18: loss improved from 0.18228 to 0.18031, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1803 - accuracy: 0.9381\n",
      "Epoch 19/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1829 - accuracy: 0.9374\n",
      "Epoch 19: loss did not improve from 0.18031\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1827 - accuracy: 0.9374\n",
      "Epoch 20/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1794 - accuracy: 0.9385\n",
      "Epoch 20: loss improved from 0.18031 to 0.17921, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1792 - accuracy: 0.9386\n",
      "Epoch 21/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9389\n",
      "Epoch 21: loss improved from 0.17921 to 0.17781, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1778 - accuracy: 0.9389\n",
      "Epoch 22/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1798 - accuracy: 0.9376\n",
      "Epoch 22: loss did not improve from 0.17781\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1798 - accuracy: 0.9377\n",
      "Epoch 23/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9394\n",
      "Epoch 23: loss improved from 0.17781 to 0.17745, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 0.1775 - accuracy: 0.9392\n",
      "Epoch 24/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.9388\n",
      "Epoch 24: loss did not improve from 0.17745\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1778 - accuracy: 0.9387\n",
      "Epoch 25/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9395\n",
      "Epoch 25: loss improved from 0.17745 to 0.17613, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1761 - accuracy: 0.9395\n",
      "Epoch 26/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1749 - accuracy: 0.9400\n",
      "Epoch 26: loss improved from 0.17613 to 0.17460, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1746 - accuracy: 0.9402\n",
      "Epoch 27/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1787 - accuracy: 0.9391\n",
      "Epoch 27: loss did not improve from 0.17460\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1786 - accuracy: 0.9391\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1761 - accuracy: 0.9390\n",
      "Epoch 28: loss did not improve from 0.17460\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1762 - accuracy: 0.9390\n",
      "Epoch 29/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9396\n",
      "Epoch 29: loss did not improve from 0.17460\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1749 - accuracy: 0.9396\n",
      "Epoch 30/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9391\n",
      "Epoch 30: loss did not improve from 0.17460\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1766 - accuracy: 0.9391\n",
      "Epoch 31/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9398\n",
      "Epoch 31: loss did not improve from 0.17460\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9399\n",
      "Epoch 32/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1764 - accuracy: 0.9397\n",
      "Epoch 32: loss did not improve from 0.17460\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1759 - accuracy: 0.9399\n",
      "Epoch 33/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1748 - accuracy: 0.9401\n",
      "Epoch 33: loss did not improve from 0.17460\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9401\n",
      "Epoch 34/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1753 - accuracy: 0.9398\n",
      "Epoch 34: loss did not improve from 0.17460\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1753 - accuracy: 0.9398\n",
      "Epoch 35/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9406\n",
      "Epoch 35: loss improved from 0.17460 to 0.17330, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1733 - accuracy: 0.9406\n",
      "Epoch 36/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9403\n",
      "Epoch 36: loss did not improve from 0.17330\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1746 - accuracy: 0.9403\n",
      "Epoch 37/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9402\n",
      "Epoch 37: loss did not improve from 0.17330\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1754 - accuracy: 0.9399\n",
      "Epoch 38/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9404\n",
      "Epoch 38: loss did not improve from 0.17330\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1748 - accuracy: 0.9405\n",
      "Epoch 39/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1743 - accuracy: 0.9401\n",
      "Epoch 39: loss did not improve from 0.17330\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1743 - accuracy: 0.9401\n",
      "Epoch 40/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1739 - accuracy: 0.9404\n",
      "Epoch 40: loss did not improve from 0.17330\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1740 - accuracy: 0.9403\n",
      "Epoch 41/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1747 - accuracy: 0.9403\n",
      "Epoch 41: loss did not improve from 0.17330\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1747 - accuracy: 0.9402\n",
      "Epoch 42/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1733 - accuracy: 0.9408\n",
      "Epoch 42: loss improved from 0.17330 to 0.17329, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1733 - accuracy: 0.9408\n",
      "Epoch 43/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9405\n",
      "Epoch 43: loss improved from 0.17329 to 0.17313, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1731 - accuracy: 0.9405\n",
      "Epoch 44/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9405\n",
      "Epoch 44: loss did not improve from 0.17313\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1746 - accuracy: 0.9405\n",
      "Epoch 45/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9399\n",
      "Epoch 45: loss improved from 0.17313 to 0.17296, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1730 - accuracy: 0.9400\n",
      "Epoch 46/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9403\n",
      "Epoch 46: loss did not improve from 0.17296\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1734 - accuracy: 0.9404\n",
      "Epoch 47/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.9408\n",
      "Epoch 47: loss improved from 0.17296 to 0.17088, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 0.1709 - accuracy: 0.9408\n",
      "Epoch 48/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1705 - accuracy: 0.9408\n",
      "Epoch 48: loss improved from 0.17088 to 0.17057, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1706 - accuracy: 0.9407\n",
      "Epoch 49/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1736 - accuracy: 0.9401\n",
      "Epoch 49: loss did not improve from 0.17057\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1739 - accuracy: 0.9401\n",
      "Epoch 50/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9405\n",
      "Epoch 50: loss did not improve from 0.17057\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1726 - accuracy: 0.9407\n",
      "Epoch 51/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9412\n",
      "Epoch 51: loss did not improve from 0.17057\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1713 - accuracy: 0.9412\n",
      "Epoch 52/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9414\n",
      "Epoch 52: loss did not improve from 0.17057\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 0.1711 - accuracy: 0.9413\n",
      "Epoch 53/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9405\n",
      "Epoch 53: loss did not improve from 0.17057\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1710 - accuracy: 0.9407\n",
      "Epoch 54/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1711 - accuracy: 0.9414\n",
      "Epoch 54: loss did not improve from 0.17057\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1712 - accuracy: 0.9413\n",
      "Epoch 55/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9417\n",
      "Epoch 55: loss improved from 0.17057 to 0.16758, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1676 - accuracy: 0.9417\n",
      "Epoch 56/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1655 - accuracy: 0.9428\n",
      "Epoch 56: loss improved from 0.16758 to 0.16567, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1657 - accuracy: 0.9426\n",
      "Epoch 57/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9427\n",
      "Epoch 57: loss improved from 0.16567 to 0.16514, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 0.1651 - accuracy: 0.9426\n",
      "Epoch 58/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1576 - accuracy: 0.9449\n",
      "Epoch 58: loss improved from 0.16514 to 0.15777, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1578 - accuracy: 0.9449\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1554 - accuracy: 0.9454\n",
      "Epoch 59: loss improved from 0.15777 to 0.15546, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1555 - accuracy: 0.9454\n",
      "Epoch 60/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.9478\n",
      "Epoch 60: loss improved from 0.15546 to 0.15015, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1501 - accuracy: 0.9478\n",
      "Epoch 61/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1513 - accuracy: 0.9474\n",
      "Epoch 61: loss did not improve from 0.15015\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1512 - accuracy: 0.9474\n",
      "Epoch 62/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9489\n",
      "Epoch 62: loss improved from 0.15015 to 0.14498, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 4ms/step - loss: 0.1450 - accuracy: 0.9490\n",
      "Epoch 63/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9491\n",
      "Epoch 63: loss did not improve from 0.14498\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1451 - accuracy: 0.9492\n",
      "Epoch 64/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9510\n",
      "Epoch 64: loss improved from 0.14498 to 0.13943, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1394 - accuracy: 0.9510\n",
      "Epoch 65/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9511\n",
      "Epoch 65: loss did not improve from 0.13943\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1406 - accuracy: 0.9510\n",
      "Epoch 66/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9512\n",
      "Epoch 66: loss improved from 0.13943 to 0.13703, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1370 - accuracy: 0.9511\n",
      "Epoch 67/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9517\n",
      "Epoch 67: loss improved from 0.13703 to 0.13516, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1352 - accuracy: 0.9519\n",
      "Epoch 68/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.9506\n",
      "Epoch 68: loss did not improve from 0.13516\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1415 - accuracy: 0.9506\n",
      "Epoch 69/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1329 - accuracy: 0.9527\n",
      "Epoch 69: loss improved from 0.13516 to 0.13324, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1332 - accuracy: 0.9526\n",
      "Epoch 70/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1338 - accuracy: 0.9524\n",
      "Epoch 70: loss did not improve from 0.13324\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1339 - accuracy: 0.9523\n",
      "Epoch 71/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9523\n",
      "Epoch 71: loss did not improve from 0.13324\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1372 - accuracy: 0.9523\n",
      "Epoch 72/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9521\n",
      "Epoch 72: loss did not improve from 0.13324\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1370 - accuracy: 0.9521\n",
      "Epoch 73/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1328 - accuracy: 0.9526\n",
      "Epoch 73: loss improved from 0.13324 to 0.13268, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1327 - accuracy: 0.9527\n",
      "Epoch 74/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9531\n",
      "Epoch 74: loss improved from 0.13268 to 0.13265, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1327 - accuracy: 0.9532\n",
      "Epoch 75/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9537\n",
      "Epoch 75: loss improved from 0.13265 to 0.12987, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1299 - accuracy: 0.9537\n",
      "Epoch 76/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9529\n",
      "Epoch 76: loss did not improve from 0.12987\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1322 - accuracy: 0.9530\n",
      "Epoch 77/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1302 - accuracy: 0.9543\n",
      "Epoch 77: loss did not improve from 0.12987\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1301 - accuracy: 0.9543\n",
      "Epoch 78/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1323 - accuracy: 0.9532\n",
      "Epoch 78: loss did not improve from 0.12987\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1322 - accuracy: 0.9532\n",
      "Epoch 79/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1297 - accuracy: 0.9536\n",
      "Epoch 79: loss did not improve from 0.12987\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1300 - accuracy: 0.9535\n",
      "Epoch 80/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9542\n",
      "Epoch 80: loss did not improve from 0.12987\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1302 - accuracy: 0.9542\n",
      "Epoch 81/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9535\n",
      "Epoch 81: loss improved from 0.12987 to 0.12824, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1282 - accuracy: 0.9536\n",
      "Epoch 82/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1317 - accuracy: 0.9533\n",
      "Epoch 82: loss did not improve from 0.12824\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1317 - accuracy: 0.9533\n",
      "Epoch 83/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9537\n",
      "Epoch 83: loss did not improve from 0.12824\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1299 - accuracy: 0.9538\n",
      "Epoch 84/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9541\n",
      "Epoch 84: loss did not improve from 0.12824\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1291 - accuracy: 0.9543\n",
      "Epoch 85/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9542\n",
      "Epoch 85: loss did not improve from 0.12824\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1298 - accuracy: 0.9542\n",
      "Epoch 86/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9546\n",
      "Epoch 86: loss improved from 0.12824 to 0.12820, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1282 - accuracy: 0.9548\n",
      "Epoch 87/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9540\n",
      "Epoch 87: loss did not improve from 0.12820\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1304 - accuracy: 0.9540\n",
      "Epoch 88/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9536\n",
      "Epoch 88: loss did not improve from 0.12820\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1310 - accuracy: 0.9536\n",
      "Epoch 89/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9539\n",
      "Epoch 89: loss did not improve from 0.12820\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1296 - accuracy: 0.9540\n",
      "Epoch 90/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9542\n",
      "Epoch 90: loss did not improve from 0.12820\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1295 - accuracy: 0.9542\n",
      "Epoch 91/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9549\n",
      "Epoch 91: loss improved from 0.12820 to 0.12672, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1267 - accuracy: 0.9549\n",
      "Epoch 92/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1299 - accuracy: 0.9542\n",
      "Epoch 92: loss did not improve from 0.12672\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1299 - accuracy: 0.9542\n",
      "Epoch 93/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9551\n",
      "Epoch 93: loss improved from 0.12672 to 0.12648, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1265 - accuracy: 0.9551\n",
      "Epoch 94/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9544\n",
      "Epoch 94: loss did not improve from 0.12648\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1279 - accuracy: 0.9541\n",
      "Epoch 95/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9543\n",
      "Epoch 95: loss did not improve from 0.12648\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1273 - accuracy: 0.9543\n",
      "Epoch 96/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9546\n",
      "Epoch 96: loss did not improve from 0.12648\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1266 - accuracy: 0.9546\n",
      "Epoch 97/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9552\n",
      "Epoch 97: loss did not improve from 0.12648\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1277 - accuracy: 0.9551\n",
      "Epoch 98/200\n",
      "1048/1067 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9543\n",
      "Epoch 98: loss did not improve from 0.12648\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1278 - accuracy: 0.9544\n",
      "Epoch 99/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9547\n",
      "Epoch 99: loss did not improve from 0.12648\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1269 - accuracy: 0.9546\n",
      "Epoch 100/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9546\n",
      "Epoch 100: loss did not improve from 0.12648\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1272 - accuracy: 0.9547\n",
      "Epoch 101/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9544\n",
      "Epoch 101: loss did not improve from 0.12648\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1298 - accuracy: 0.9544\n",
      "Epoch 102/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9550\n",
      "Epoch 102: loss improved from 0.12648 to 0.12575, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1257 - accuracy: 0.9552\n",
      "Epoch 103/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9551\n",
      "Epoch 103: loss did not improve from 0.12575\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1283 - accuracy: 0.9551\n",
      "Epoch 104/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9552\n",
      "Epoch 104: loss did not improve from 0.12575\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1264 - accuracy: 0.9551\n",
      "Epoch 105/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9550\n",
      "Epoch 105: loss did not improve from 0.12575\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1276 - accuracy: 0.9550\n",
      "Epoch 106/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9551\n",
      "Epoch 106: loss improved from 0.12575 to 0.12525, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1252 - accuracy: 0.9550\n",
      "Epoch 107/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9555\n",
      "Epoch 107: loss improved from 0.12525 to 0.12520, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1252 - accuracy: 0.9552\n",
      "Epoch 108/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9545\n",
      "Epoch 108: loss did not improve from 0.12520\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1269 - accuracy: 0.9547\n",
      "Epoch 109/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9554\n",
      "Epoch 109: loss improved from 0.12520 to 0.12468, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1247 - accuracy: 0.9554\n",
      "Epoch 110/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9552\n",
      "Epoch 110: loss did not improve from 0.12468\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1270 - accuracy: 0.9552\n",
      "Epoch 111/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9548\n",
      "Epoch 111: loss improved from 0.12468 to 0.12467, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1247 - accuracy: 0.9548\n",
      "Epoch 112/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9557\n",
      "Epoch 112: loss did not improve from 0.12467\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1248 - accuracy: 0.9556\n",
      "Epoch 113/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9553\n",
      "Epoch 113: loss did not improve from 0.12467\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1272 - accuracy: 0.9552\n",
      "Epoch 114/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9554\n",
      "Epoch 114: loss improved from 0.12467 to 0.12394, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1239 - accuracy: 0.9554\n",
      "Epoch 115/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9556\n",
      "Epoch 115: loss improved from 0.12394 to 0.12257, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1226 - accuracy: 0.9554\n",
      "Epoch 116/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9555\n",
      "Epoch 116: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1246 - accuracy: 0.9554\n",
      "Epoch 117/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9554\n",
      "Epoch 117: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1271 - accuracy: 0.9553\n",
      "Epoch 118/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9555\n",
      "Epoch 118: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1256 - accuracy: 0.9556\n",
      "Epoch 119/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9554\n",
      "Epoch 119: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1246 - accuracy: 0.9555\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9553\n",
      "Epoch 120: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1254 - accuracy: 0.9553\n",
      "Epoch 121/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9554\n",
      "Epoch 121: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1240 - accuracy: 0.9554\n",
      "Epoch 122/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1259 - accuracy: 0.9553\n",
      "Epoch 122: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1260 - accuracy: 0.9552\n",
      "Epoch 123/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9553\n",
      "Epoch 123: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1243 - accuracy: 0.9554\n",
      "Epoch 124/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9553\n",
      "Epoch 124: loss did not improve from 0.12257\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1234 - accuracy: 0.9553\n",
      "Epoch 125/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9559\n",
      "Epoch 125: loss improved from 0.12257 to 0.12205, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1220 - accuracy: 0.9560\n",
      "Epoch 126/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9554\n",
      "Epoch 126: loss did not improve from 0.12205\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1255 - accuracy: 0.9554\n",
      "Epoch 127/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9554\n",
      "Epoch 127: loss did not improve from 0.12205\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1241 - accuracy: 0.9554\n",
      "Epoch 128/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9563\n",
      "Epoch 128: loss did not improve from 0.12205\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1229 - accuracy: 0.9562\n",
      "Epoch 129/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9559\n",
      "Epoch 129: loss did not improve from 0.12205\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1244 - accuracy: 0.9558\n",
      "Epoch 130/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9557\n",
      "Epoch 130: loss did not improve from 0.12205\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1242 - accuracy: 0.9558\n",
      "Epoch 131/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9554\n",
      "Epoch 131: loss did not improve from 0.12205\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1234 - accuracy: 0.9554\n",
      "Epoch 132/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9557\n",
      "Epoch 132: loss did not improve from 0.12205\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1240 - accuracy: 0.9557\n",
      "Epoch 133/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9554\n",
      "Epoch 133: loss did not improve from 0.12205\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1232 - accuracy: 0.9555\n",
      "Epoch 134/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9563\n",
      "Epoch 134: loss improved from 0.12205 to 0.12187, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1219 - accuracy: 0.9563\n",
      "Epoch 135/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9564\n",
      "Epoch 135: loss improved from 0.12187 to 0.12130, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1213 - accuracy: 0.9564\n",
      "Epoch 136/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9559\n",
      "Epoch 136: loss did not improve from 0.12130\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1221 - accuracy: 0.9559\n",
      "Epoch 137/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9559\n",
      "Epoch 137: loss did not improve from 0.12130\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1236 - accuracy: 0.9558\n",
      "Epoch 138/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9557\n",
      "Epoch 138: loss did not improve from 0.12130\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1231 - accuracy: 0.9557\n",
      "Epoch 139/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9555\n",
      "Epoch 139: loss did not improve from 0.12130\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1236 - accuracy: 0.9554\n",
      "Epoch 140/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9563\n",
      "Epoch 140: loss improved from 0.12130 to 0.12036, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1204 - accuracy: 0.9564\n",
      "Epoch 141/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9563\n",
      "Epoch 141: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1215 - accuracy: 0.9562\n",
      "Epoch 142/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9561\n",
      "Epoch 142: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1226 - accuracy: 0.9561\n",
      "Epoch 143/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9558\n",
      "Epoch 143: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1225 - accuracy: 0.9558\n",
      "Epoch 144/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9558\n",
      "Epoch 144: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1217 - accuracy: 0.9560\n",
      "Epoch 145/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9558\n",
      "Epoch 145: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1231 - accuracy: 0.9557\n",
      "Epoch 146/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9563\n",
      "Epoch 146: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1209 - accuracy: 0.9564\n",
      "Epoch 147/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9553\n",
      "Epoch 147: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1228 - accuracy: 0.9553\n",
      "Epoch 148/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9552\n",
      "Epoch 148: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1230 - accuracy: 0.9553\n",
      "Epoch 149/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1213 - accuracy: 0.9559\n",
      "Epoch 149: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1213 - accuracy: 0.9559\n",
      "Epoch 150/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9565\n",
      "Epoch 150: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1207 - accuracy: 0.9564\n",
      "Epoch 151/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9564\n",
      "Epoch 151: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1205 - accuracy: 0.9564\n",
      "Epoch 152/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9560\n",
      "Epoch 152: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1212 - accuracy: 0.9560\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9568\n",
      "Epoch 153: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1210 - accuracy: 0.9567\n",
      "Epoch 154/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9564\n",
      "Epoch 154: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1205 - accuracy: 0.9565\n",
      "Epoch 155/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9560\n",
      "Epoch 155: loss did not improve from 0.12036\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1227 - accuracy: 0.9558\n",
      "Epoch 156/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9566\n",
      "Epoch 156: loss improved from 0.12036 to 0.12034, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1203 - accuracy: 0.9564\n",
      "Epoch 157/200\n",
      "1064/1067 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9549\n",
      "Epoch 157: loss did not improve from 0.12034\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1232 - accuracy: 0.9549\n",
      "Epoch 158/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9569\n",
      "Epoch 158: loss did not improve from 0.12034\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1204 - accuracy: 0.9569\n",
      "Epoch 159/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9562\n",
      "Epoch 159: loss did not improve from 0.12034\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1205 - accuracy: 0.9562\n",
      "Epoch 160/200\n",
      "1065/1067 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9568\n",
      "Epoch 160: loss improved from 0.12034 to 0.11994, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1199 - accuracy: 0.9568\n",
      "Epoch 161/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9560\n",
      "Epoch 161: loss did not improve from 0.11994\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1212 - accuracy: 0.9561\n",
      "Epoch 162/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9564\n",
      "Epoch 162: loss did not improve from 0.11994\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1216 - accuracy: 0.9564\n",
      "Epoch 163/200\n",
      "1060/1067 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9561\n",
      "Epoch 163: loss did not improve from 0.11994\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1225 - accuracy: 0.9562\n",
      "Epoch 164/200\n",
      "1051/1067 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9567\n",
      "Epoch 164: loss did not improve from 0.11994\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1202 - accuracy: 0.9566\n",
      "Epoch 165/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9555\n",
      "Epoch 165: loss did not improve from 0.11994\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1231 - accuracy: 0.9555\n",
      "Epoch 166/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9574\n",
      "Epoch 166: loss improved from 0.11994 to 0.11972, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1197 - accuracy: 0.9573\n",
      "Epoch 167/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9561\n",
      "Epoch 167: loss did not improve from 0.11972\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1205 - accuracy: 0.9561\n",
      "Epoch 168/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9566\n",
      "Epoch 168: loss did not improve from 0.11972\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1200 - accuracy: 0.9566\n",
      "Epoch 169/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9560\n",
      "Epoch 169: loss did not improve from 0.11972\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1202 - accuracy: 0.9561\n",
      "Epoch 170/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9560\n",
      "Epoch 170: loss did not improve from 0.11972\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1206 - accuracy: 0.9563\n",
      "Epoch 171/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9565\n",
      "Epoch 171: loss did not improve from 0.11972\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1205 - accuracy: 0.9565\n",
      "Epoch 172/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.9567\n",
      "Epoch 172: loss improved from 0.11972 to 0.11879, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1188 - accuracy: 0.9567\n",
      "Epoch 173/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9561\n",
      "Epoch 173: loss did not improve from 0.11879\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1205 - accuracy: 0.9562\n",
      "Epoch 174/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9560\n",
      "Epoch 174: loss did not improve from 0.11879\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1224 - accuracy: 0.9559\n",
      "Epoch 175/200\n",
      "1050/1067 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9574\n",
      "Epoch 175: loss improved from 0.11879 to 0.11799, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1180 - accuracy: 0.9572\n",
      "Epoch 176/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9562\n",
      "Epoch 176: loss did not improve from 0.11799\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1197 - accuracy: 0.9564\n",
      "Epoch 177/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9559\n",
      "Epoch 177: loss did not improve from 0.11799\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1214 - accuracy: 0.9561\n",
      "Epoch 178/200\n",
      "1054/1067 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9568\n",
      "Epoch 178: loss improved from 0.11799 to 0.11772, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1177 - accuracy: 0.9571\n",
      "Epoch 179/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9571\n",
      "Epoch 179: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1188 - accuracy: 0.9571\n",
      "Epoch 180/200\n",
      "1066/1067 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9571\n",
      "Epoch 180: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1191 - accuracy: 0.9571\n",
      "Epoch 181/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9565\n",
      "Epoch 181: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1202 - accuracy: 0.9565\n",
      "Epoch 182/200\n",
      "1053/1067 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9571\n",
      "Epoch 182: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1182 - accuracy: 0.9572\n",
      "Epoch 183/200\n",
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9559\n",
      "Epoch 183: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1195 - accuracy: 0.9561\n",
      "Epoch 184/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9563\n",
      "Epoch 184: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1199 - accuracy: 0.9562\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049/1067 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9566\n",
      "Epoch 185: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1203 - accuracy: 0.9566\n",
      "Epoch 186/200\n",
      "1067/1067 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9559\n",
      "Epoch 186: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1212 - accuracy: 0.9559\n",
      "Epoch 187/200\n",
      "1058/1067 [============================>.] - ETA: 0s - loss: 0.1189 - accuracy: 0.9571\n",
      "Epoch 187: loss did not improve from 0.11772\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1191 - accuracy: 0.9570\n",
      "Epoch 188/200\n",
      "1052/1067 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9579\n",
      "Epoch 188: loss improved from 0.11772 to 0.11485, saving model to C:\\Users\\Admin\\IEEE_14_BusSystem\\_nn_model\\eigs_classification.hdf5\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1149 - accuracy: 0.9579\n",
      "Epoch 189/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9564\n",
      "Epoch 189: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1212 - accuracy: 0.9564\n",
      "Epoch 190/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9568\n",
      "Epoch 190: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1208 - accuracy: 0.9568\n",
      "Epoch 191/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9568\n",
      "Epoch 191: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1193 - accuracy: 0.9568\n",
      "Epoch 192/200\n",
      "1057/1067 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9570\n",
      "Epoch 192: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1182 - accuracy: 0.9571\n",
      "Epoch 193/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9567\n",
      "Epoch 193: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1190 - accuracy: 0.9567\n",
      "Epoch 194/200\n",
      "1059/1067 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9565\n",
      "Epoch 194: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1200 - accuracy: 0.9565\n",
      "Epoch 195/200\n",
      "1056/1067 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9566\n",
      "Epoch 195: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1197 - accuracy: 0.9566\n",
      "Epoch 196/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9573\n",
      "Epoch 196: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1179 - accuracy: 0.9572\n",
      "Epoch 197/200\n",
      "1061/1067 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9572\n",
      "Epoch 197: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1182 - accuracy: 0.9572\n",
      "Epoch 198/200\n",
      "1063/1067 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9568\n",
      "Epoch 198: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 4s 3ms/step - loss: 0.1212 - accuracy: 0.9568\n",
      "Epoch 199/200\n",
      "1062/1067 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9574\n",
      "Epoch 199: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1170 - accuracy: 0.9574\n",
      "Epoch 200/200\n",
      "1055/1067 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9569\n",
      "Epoch 200: loss did not improve from 0.11485\n",
      "1067/1067 [==============================] - 3s 3ms/step - loss: 0.1197 - accuracy: 0.9568\n"
     ]
    }
   ],
   "source": [
    "filepath = os.path.abspath(os.path.join(os.getcwd(), '_nn_model', 'eigs_classification.hdf5'))\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(filepath = filepath, \n",
    "                                             monitor = 'loss', \n",
    "                                             mode = 'min', save_best_only = True, verbose = 1)]\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", \n",
    "              optimizer = \"Adam\", \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train_red, T_train_red, epochs = 200, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Prediction with the NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7586/7586 [==============================] - 15s 2ms/step - loss: 0.0393 - accuracy: 0.9842\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "\n",
    "scores = model.evaluate(X_test, keras.utils.to_categorical(Y_test - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_logits = model.predict(X_test) # Probability of the input to belong to each of the classes\n",
    "Y_pred = np.argmax(Y_logits, axis = 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prediction_plotting(X_test, Y_test, Y_pred, model_name, set_name = 'Testing', fontname = \"liberation sans\"):\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function\n",
    "\n",
    "    # Creating a plot with two subplots: one for the ground truth and another for the predictions\n",
    "    fig, axes = plt.subplots(figsize = (14, 7), nrows = 1, ncols = 2)\n",
    "\n",
    "#    # fig.suptitle(\"{} - Ground Truth and Prediction ({} set)\".format(model_name, set_name), fontsize = 20, fontname = fontname)\n",
    "\n",
    "    # Colors for the different eigenvalue categories\n",
    "    color = ['red', 'orange', 'blue', 'darkblue', 'green', 'violet']\n",
    "\n",
    "    # Container for eigenvalues of each category\n",
    "    eigs_labeled = {'1 - Stable' : [],'2 - Critical' : [], '3 - Acceptable' : [], '4 - Good' : [], '5 - Satisfactory' : [], '6 - Irrelevant' : []}\n",
    "    eigs_label_pred = {'1 - Stable' : [],'2 - Critical' : [], '3 - Acceptable' : [], '4 - Good' : [], '5 - Satisfactory' : [], '6 - Irrelevant' : []}\n",
    "\n",
    "    for n_ev, eigenvalue_label in enumerate(Y_test):\n",
    "        if (eigenvalue_label == 1):\n",
    "            eigs_labeled['1 - Stable'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label == 2):\n",
    "            eigs_labeled['2 - Critical'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label == 3):\n",
    "            eigs_labeled['3 - Acceptable'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label == 4):\n",
    "            eigs_labeled['4 - Good'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label == 5):\n",
    "            eigs_labeled['5 - Satisfactory'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label == 6):\n",
    "            eigs_labeled['6 - Irrelevant'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "\n",
    "    for n_ev, eigenvalue_label_pred in enumerate(Y_pred):\n",
    "        if (eigenvalue_label_pred == 1):\n",
    "            eigs_label_pred['1 - Stable'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label_pred == 2):\n",
    "            eigs_label_pred['2 - Critical'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label_pred == 3):\n",
    "            eigs_label_pred['3 - Acceptable'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label_pred == 4):\n",
    "            eigs_label_pred['4 - Good'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label_pred == 5):\n",
    "            eigs_label_pred['5 - Satisfactory'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "        elif (eigenvalue_label_pred == 6):\n",
    "             eigs_label_pred['6 - Irrelevant'].append(X_test[n_ev,0] + 1j * X_test[n_ev,1])\n",
    "\n",
    "    # Converting lists to numpy arrays\n",
    "    for label in eigs_labeled:\n",
    "        eigs_labeled[label] = np.array(eigs_labeled[label])\n",
    "    for label in eigs_label_pred:\n",
    "        eigs_label_pred[label] = np.array(eigs_label_pred[label])\n",
    "\n",
    "    # Plotting the eigenvalues (ground truth)\n",
    "    for n_label, label in enumerate(eigs_labeled):\n",
    "        real_part = [eig.real for eig in eigs_labeled[label]]\n",
    "        imag_part = [eig.imag for eig in eigs_labeled[label]]\n",
    "        axes[0].scatter(real_part, imag_part, color = color[n_label], marker = 'x')\n",
    "\n",
    "    axes[0].set_xlabel(\"Real Axis\", fontsize = 12, fontname = fontname)\n",
    "    axes[0].set_ylabel(\"Imaginary Axis\", fontsize = 12, fontname = fontname)\n",
    "    axes[0].set_title(\"Ground Truth (hard-coded label)\", fontsize = 14, fontname = fontname)\n",
    "    axes[0].legend([\"Unstable\", \"Critical\", \"Acceptable\", \"Good\", \"Satisfactory\"], loc = 'upper left', prop = {'family' : fontname, 'size' : 12}) \n",
    "    axes[0].axvline(x = 0, color = 'black', linestyle = '--') # Drawing stability boundary\n",
    "\n",
    "    # Formatting ticks\n",
    "    for tick in axes[0].get_xticklabels():\n",
    "        tick.set_fontname(fontname)\n",
    "        tick.set_fontsize(13)\n",
    "    for tick in axes[0].get_yticklabels():\n",
    "        tick.set_fontname(fontname)\n",
    "        tick.set_fontsize(13)\n",
    "\n",
    "    # Plotting the eigenvalues (predictions)\n",
    "    for n_label, label in enumerate(eigs_label_pred):\n",
    "        real_part = [eig.real for eig in eigs_label_pred[label]]\n",
    "        imag_part = [eig.imag for eig in eigs_label_pred[label]]\n",
    "        axes[1].scatter(real_part, imag_part, color = color[n_label], marker = 'x')\n",
    "\n",
    "    axes[1].set_xlabel(\"Real Axis\", fontsize = 12, fontname = fontname)\n",
    "    axes[1].set_ylabel(\"Imaginary Axis\", fontsize = 12, fontname = fontname)\n",
    "    axes[1].set_title(\"Stability Predict Using NN (2-Hidden Layers)\", fontsize = 14, fontname = fontname)\n",
    "    axes[1].legend([\"Unstable\", \"Critical\", \"Acceptable\", \"Good\", \"Satisfactory\"], loc = 'upper left', prop = {'family' : fontname, 'size' : 12}) \n",
    "    axes[1].axvline(x = 0, color = 'black', linestyle = '--') # Drawing stability boundary\n",
    "\n",
    "    # Formatting ticks\n",
    "    for tick in axes[1].get_xticklabels():\n",
    "        tick.set_fontname(fontname)\n",
    "        tick.set_fontsize(10)\n",
    "    for tick in axes[1].get_yticklabels():\n",
    "        tick.set_fontname(fontname)\n",
    "        tick.set_fontsize(10)\n",
    "\n",
    "    fig.savefig(\"Fig_{}_{}_GroundTruth_Prediction.png\".format(model_name, set_name), dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_plotting_nn4(X_test, Y_test, Y_pred, model_name = \"NN - 2 Hidden Layers\", fontname = \"Times\")\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('Accuracy:',accuracy_score(Y_test, Y_pred))\n",
    "print('Precision:',precision_score(Y_test, Y_pred, average = 'macro'))\n",
    "print('Recall:',recall_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score:',f1_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "import time    # this is an inbuilt library in Python\n",
    "start_time = time.time()\n",
    "model.evaluate(X_test, keras.utils.to_categorical(Y_test - 1))\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time,\"secs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the NN architecture and then use Netron for visualizaiton (paper purpose)\n",
    "# Find Netron at: https://netron.app/\n",
    "# Download Netron for windows: https://sourceforge.net/projects/netron.mirror/files/latest/download\n",
    "model.save(\"model-100-2HD.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the NN Model - 5 layer (3 hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "# First layer (input layer)\n",
    "model.add(keras.layers.Dense(100, \n",
    "                             activation = \"selu\", \n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "model.add(keras.layers.Dropout(rate = 0.25)) # Dropout layer (rate = 0.25)\n",
    "\n",
    "# Second layer\n",
    "model.add(keras.layers.Dense(100, \n",
    "                             activation = \"selu\", \n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "model.add(keras.layers.Dropout(rate = 0.25)) # Dropout layer (rate = 0.25)\n",
    "\n",
    "# Third layer\n",
    "model.add(keras.layers.Dense(100, \n",
    "                             activation = \"selu\", \n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "model.add(keras.layers.Dropout(rate = 0.25)) # Dropout layer (rate = 0.25)\n",
    "\n",
    "\n",
    "# Fourth layer\n",
    "model.add(keras.layers.Dense(100, \n",
    "                             activation = \"selu\", \n",
    "                             kernel_initializer = 'lecun_normal', \n",
    "                             bias_initializer = 'zeros'))\n",
    "\n",
    "# Fifth layer (output layer)\n",
    "model.add(keras.layers.Dense(6, activation = \"softmax\"))\n",
    "\n",
    "model.build(input_shape = [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.abspath(os.path.join(os.getcwd(), '_nn_model', 'eigs_classification.hdf5'))\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(filepath = filepath, \n",
    "                                             monitor = 'loss', \n",
    "                                             mode = 'min', save_best_only = True, verbose = 1)]\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", \n",
    "              optimizer = \"Adam\", \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train_red, T_train_red, epochs = 200, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Prediction with the NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath)\n",
    "\n",
    "scores = model.evaluate(X_test, keras.utils.to_categorical(Y_test - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_plotting_nn4(X_test, Y_test, Y_pred, model_name = \"NN - 5 Layer\", fontname = \"Times\")\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('Accuracy:',accuracy_score(Y_test, Y_pred))\n",
    "print('Precision:',precision_score(Y_test, Y_pred, average = 'macro'))\n",
    "print('Recall:',recall_score(Y_test, Y_pred, average = 'macro'))\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score:',f1_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "import time    # this is an inbuilt library in Python\n",
    "start_time = time.time()\n",
    "model.evaluate(X_test, keras.utils.to_categorical(Y_test - 1))\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time,\"secs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5_traditional_ML_solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Importing Algorithms and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_preproc_data/normalized_testing_training_data_red_100%.pkl', 'rb') as f:\n",
    "    normalized_testing_training_data_red = pickle.load(f)\n",
    "    \n",
    "X_train_red, X_train, X_test = normalized_testing_training_data_red['X_train_red'], \\\n",
    "normalized_testing_training_data_red['X_train'], \\\n",
    "normalized_testing_training_data_red['X_test']\n",
    "\n",
    "Y_train_red, Y_train, Y_test = normalized_testing_training_data_red['Y_train_red'], \\\n",
    "normalized_testing_training_data_red['Y_train'], \\\n",
    "normalized_testing_training_data_red['Y_test']\n",
    "\n",
    "# Additional preprocessing\n",
    "print(f\"Y_train_red (shape) = {Y_train_red.shape}\")\n",
    "Y_train_red = np.reshape(Y_train_red, -1)\n",
    "print(f\"Y_train_red (shape) = {Y_train_red.shape}\")\n",
    "\n",
    "print(f\"Y_test (shape) = {Y_test.shape}\")\n",
    "Y_test = np.reshape(Y_test, -1)\n",
    "print(f\"Y_test (shape) = {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression algorithm\n",
    "log_reg = LogisticRegression(solver = 'lbfgs', C = 10)\n",
    "\n",
    "log_reg.fit(X_train_red, Y_train_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression algorithm\n",
    "log_reg = LogisticRegression(solver = 'lbfgs', C = 10)\n",
    "\n",
    "log_reg.fit(X_train_red, Y_train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions\n",
    "Y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Computing precision\n",
    "precision_score(Y_test, Y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_plotting(X_test, Y_test, Y_pred, model_name = \"Softmax Regression\", fontname = \"Times\")\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('Accuracy:',accuracy_score(Y_test, Y_pred))\n",
    "print('Precision:',precision_score(Y_test, Y_pred, average = 'macro'))\n",
    "print('Recall:',recall_score(Y_test, Y_pred, average = 'macro'))\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score:',f1_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "import time    # this is an inbuilt library in Python\n",
    "start_time = time.time()\n",
    "Y_pred = log_reg.predict(X_test)\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time,\"secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_clf = LinearSVC(C = 10, loss = 'hinge', max_iter = 200000)\n",
    "\n",
    "svm_clf.fit(X_train_red, Y_train_red)\n",
    "\n",
    "svm_clf = LinearSVC(C = 10, loss = 'hinge', max_iter = 200000)\n",
    "\n",
    "svm_clf.fit(X_train_red, Y_train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "precision_score(Y_test, Y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_plotting(X_test, Y_test, Y_pred, model_name = \"SVM\", fontname = \"Times\")\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('Accuracy:',accuracy_score(Y_test, Y_pred))\n",
    "print('Precision:',precision_score(Y_test, Y_pred, average = 'macro'))\n",
    "print('Recall:',recall_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score:',f1_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "import time    # this is an inbuilt library in Python\n",
    "start_time = time.time()\n",
    "Y_pred = svm_clf.predict(X_test)\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time,\"secs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Loop to select the `n_neighbors` hyperparameter\n",
    "for n in range(1, 6):\n",
    "    clf = KNeighborsClassifier(n_neighbors = n)\n",
    "    clf.fit(X_train_red, Y_train_red)\n",
    "    print(f\"n_neighbors = {n}\")\n",
    "    # Training accuracy\n",
    "    print(f\"Training accuracy: {clf.score(X_train_red, Y_train_red)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 2)\n",
    "clf.fit(X_train_red, Y_train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_0 = time.time()\n",
    "Y_pred = clf.predict(X_test)\n",
    "t_f = time.time() - t_0\n",
    "print(t_f)\n",
    "\n",
    "precision_score(Y_test, Y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_plotting(X_test, Y_test, Y_pred, model_name = \"k-NN\", fontname = \"Times\")\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('Accuracy:',accuracy_score(Y_test, Y_pred))\n",
    "print('Precision:',precision_score(Y_test, Y_pred, average = 'macro'))\n",
    "print('Recall:',recall_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score:',f1_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "import time    # this is an inbuilt library in Python\n",
    "start_time = time.time()\n",
    "Y_pred = clf.predict(X_test)\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time,\"secs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Loop for selecting the `max_depth` hyperparameter\n",
    "for max_d in range(1, 10):\n",
    "    tree = DecisionTreeClassifier(max_depth = max_d, random_state = 0)\n",
    "    tree.fit(X_train_red, Y_train_red)\n",
    "    print(f\"Depth: {max_d}\")\n",
    "    print(f\"Training accuracy: {tree.score(X_train_red, Y_train_red)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth = 7, random_state = 0)\n",
    "tree.fit(X_train_red, Y_train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_0 = time.time()\n",
    "Y_pred = tree.predict(X_test)\n",
    "t_f = time.time() - t_0\n",
    "\n",
    "print(t_f)\n",
    "\n",
    "precision_score(Y_test, Y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_plotting(X_test, Y_test, Y_pred, model_name = \"Decision Trees\", fontname = \"Times\")\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('Accuracy:',accuracy_score(Y_test, Y_pred))\n",
    "print('Precision:',precision_score(Y_test, Y_pred, average = 'macro'))\n",
    "print('Recall:',recall_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score:',f1_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "import time    # this is an inbuilt library in Python\n",
    "start_time = time.time()\n",
    "Y_pred = tree.predict(X_test)\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time,\"secs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaÃ¯ve-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train_red, Y_train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_0 = time.time()\n",
    "Y_pred = gnb.predict(X_test)\n",
    "t_f = time.time() - t_0\n",
    "\n",
    "print(t_f)\n",
    "\n",
    "precision_score(Y_test, Y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_plotting(X_test, Y_test, Y_pred, model_name = \"NaÃ¯ve-Bayes\", fontname = \"Times\")\n",
    "# the prediction_plotting.py under 'utilis' stores the detailed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('Accuracy:',accuracy_score(Y_test, Y_pred))\n",
    "print('Precision:',precision_score(Y_test, Y_pred, average = 'macro'))\n",
    "print('Recall:',recall_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1-score:',f1_score(Y_test, Y_pred, average = 'macro'))\n",
    "\n",
    "import time    # this is an inbuilt library in Python\n",
    "start_time = time.time()\n",
    "Y_pred = gnb.predict(X_test)\n",
    "end_time = time.time()\n",
    "print(\"Execution time: \", end_time - start_time,\"secs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
